# GPU ç§‘å­¦è®¡ç®— Tekton Pipeline è¿ç§»æŒ‡å—

å°† GitHub Actions GPU å·¥ä½œæµè¿ç§»åˆ° Tekton çš„å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [æ¶æ„å¯¹æ¯”](#æ¶æ„å¯¹æ¯”)
3. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
4. [å¿«é€Ÿéƒ¨ç½²](#å¿«é€Ÿéƒ¨ç½²)
5. [è¯¦ç»†é…ç½®](#è¯¦ç»†é…ç½®)
6. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
7. [ç›‘æ§ä¸è°ƒè¯•](#ç›‘æ§ä¸è°ƒè¯•)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
9. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
10. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—å¸®åŠ©æ‚¨å°†ç°æœ‰çš„ GitHub Actions GPU ç§‘å­¦è®¡ç®—å·¥ä½œæµå®Œæ•´è¿ç§»åˆ° Tektonï¼Œå®ç°ï¼š

### åŸ GitHub Actions å·¥ä½œæµç¨‹

```mermaid
graph TD
    A[GitHub æ¨é€äº‹ä»¶] --> B[GitHub Actions Runner GPU]
    B --> C[å¯åŠ¨ Docker Compose å®¹å™¨]
    C --> D[Papermill æ‰§è¡Œ Notebook]
    D --> E[ç”Ÿæˆ .ipynb æ–‡ä»¶]
    E --> F[Jupyter nbconvert è½¬ HTML]
    F --> G[ä¸‹è½½æµ‹è¯•ä»“åº“]
    G --> H[æ‰§è¡Œ PyTest]
    H --> I[ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶]
    I --> J[ä¸Šä¼  Artifacts]
```

### æ–° Tekton Pipeline æ¶æ„

```mermaid
graph TD
    A[GitHub Webhook] --> B[Tekton EventListener]
    B --> C[è§¦å‘ GPU Pipeline]
    C --> D[Task: ç¯å¢ƒå‡†å¤‡]
    D --> E[Task: GPU Notebook æ‰§è¡Œ]
    E --> F[Task: HTML è½¬æ¢]
    F --> G[Task: æµ‹è¯•æ‰§è¡Œ]
    G --> H[Task: ç»“æœå‘å¸ƒ]
    
    subgraph "GPU èŠ‚ç‚¹æ± "
        I[GPU Node 1]
        J[GPU Node 2]
        K[GPU Node N]
    end
    
    subgraph "å­˜å‚¨å±‚"
        L[æºä»£ç å·¥ä½œç©ºé—´]
        M[å…±äº« Artifacts å·¥ä½œç©ºé—´]
        N[GPU ç¼“å­˜å·¥ä½œç©ºé—´]
        O[æµ‹è¯•æ‰§è¡Œå·¥ä½œç©ºé—´]
    end
```

### è¿ç§»ä¼˜åŠ¿

- âœ… **å¼¹æ€§æ‰©å±•**: è‡ªåŠ¨ GPU èµ„æºè°ƒåº¦å’Œè´Ÿè½½å‡è¡¡
- âœ… **æˆæœ¬ä¼˜åŒ–**: æŒ‰éœ€ä½¿ç”¨ GPU èµ„æºï¼Œé¿å…èµ„æºæµªè´¹
- âœ… **é«˜å¯ç”¨æ€§**: å®¹å™¨åŒ–è¿è¡Œï¼Œè‡ªåŠ¨æ•…éšœæ¢å¤
- âœ… **æ ‡å‡†åŒ–**: åŸºäº Kubernetes åŸç”Ÿå·¥å…·é“¾
- âœ… **å¯è§‚æµ‹æ€§**: å®Œæ•´çš„æ—¥å¿—ã€ç›‘æ§å’Œå‘Šè­¦

## ğŸ”„ æ¶æ„å¯¹æ¯”

### GitHub Actions vs Tekton

| ç‰¹æ€§ | GitHub Actions | Tekton |
|------|---------------|--------|
| **è¿è¡Œç¯å¢ƒ** | VM-based Runners | Kubernetes Pods |
| **GPU è°ƒåº¦** | å›ºå®š Runner | K8s GPU è°ƒåº¦å™¨ |
| **èµ„æºåˆ©ç”¨** | ç‹¬å å¼ | å…±äº«å¼ï¼Œé«˜æ•ˆåˆ©ç”¨ |
| **æ‰©å±•æ€§** | æ‰‹åŠ¨æ‰©å±• | è‡ªåŠ¨å¼¹æ€§æ‰©å±• |
| **å­˜å‚¨** | Runner æœ¬åœ°å­˜å‚¨ | PV/PVC æŒä¹…åŒ–å­˜å‚¨ |
| **ç›‘æ§** | Actions å†…ç½® | Prometheus + Grafana |
| **æˆæœ¬** | æŒ‰ Runner æ—¶é—´è®¡è´¹ | æŒ‰å®é™…èµ„æºä½¿ç”¨è®¡è´¹ |

### å·¥ä½œæµå¯¹åº”å…³ç³»

| GitHub Actions æ­¥éª¤ | Tekton Task | è¯´æ˜ |
|-------------------|-------------|-----|
| ä»£ç æ£€å‡º | `gpu-env-preparation` | Git clone + ç¯å¢ƒéªŒè¯ |
| Docker Compose å¯åŠ¨ | `gpu-papermill-execution` | GPU å®¹å™¨ä¸­æ‰§è¡Œ Papermill |
| Papermill æ‰§è¡Œ | `gpu-papermill-execution` | ä½¿ç”¨ GPU æ‰§è¡Œ Notebook |
| Jupyter nbconvert | `jupyter-nbconvert` | è½¬æ¢ä¸º HTML æ ¼å¼ |
| ä¸‹è½½æµ‹è¯•ä»“åº“ + PyTest | `pytest-execution` | æµ‹è¯•æ¡†æ¶ä¸‹è½½å’Œæ‰§è¡Œ |
| ä¸Šä¼  Artifacts | `publish-results` | ç»“æœæ”¶é›†å’Œå‘å¸ƒ |

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### å‰ç½®è¦æ±‚

#### 1. Kubernetes é›†ç¾¤è¦æ±‚

```bash
# æ£€æŸ¥é›†ç¾¤ç‰ˆæœ¬ï¼ˆæ¨è 1.24+ï¼‰
kubectl version --short

# æ£€æŸ¥ GPU æ”¯æŒ
kubectl get nodes -o wide
kubectl describe node <gpu-node-name>
```

#### 2. GPU èŠ‚ç‚¹é…ç½®

```bash
# å®‰è£… NVIDIA GPU Operatorï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/gpu-operator/main/deployments/gpu-operator.yaml

# æ ‡è®° GPU èŠ‚ç‚¹
kubectl label nodes <gpu-node-name> accelerator=nvidia-tesla-gpu

# éªŒè¯ GPU èµ„æº
kubectl get nodes -o jsonpath='{.items[*].status.allocatable.nvidia\.com/gpu}'
```

#### 3. å­˜å‚¨é…ç½®

```yaml
# é«˜æ€§èƒ½å­˜å‚¨ç±»ç¤ºä¾‹
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
volumeBindingMode: WaitForFirstConsumer
```

#### 4. Tekton å®‰è£…

```bash
# å®‰è£… Tekton Pipelines
kubectl apply -f https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml

# å®‰è£… Tekton Triggers
kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/latest/release.yaml
kubectl apply -f https://storage.googleapis.com/tekton-releases/triggers/latest/interceptors.yaml

# éªŒè¯å®‰è£…
kubectl get pods -n tekton-pipelines
kubectl get crd | grep tekton
```

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### ä¸€é”®éƒ¨ç½²è„šæœ¬

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/Real-world_Tekton_Installation_Guide.git
cd Real-world_Tekton_Installation_Guide

# æ‰§è¡Œä¸­æ–‡éƒ¨ç½²è„šæœ¬
chmod +x scripts/zh/deploy-gpu-pipeline.sh
./scripts/zh/deploy-gpu-pipeline.sh

# æˆ–è€…æ‰§è¡Œè‹±æ–‡éƒ¨ç½²è„šæœ¬
chmod +x scripts/en/deploy-gpu-pipeline.sh
./scripts/en/deploy-gpu-pipeline.sh
```

### éƒ¨ç½²éªŒè¯

```bash
# æ£€æŸ¥æ‰€æœ‰ç»„ä»¶çŠ¶æ€
kubectl get tasks,pipelines,eventlisteners -n tekton-pipelines

# æŸ¥çœ‹ EventListener æœåŠ¡
kubectl get svc -n tekton-pipelines
kubectl describe eventlistener gpu-scientific-computing-eventlistener -n tekton-pipelines
```

## âš™ï¸ è¯¦ç»†é…ç½®

### 1. Tasks é…ç½®è¯¦è§£

#### GPU ç¯å¢ƒå‡†å¤‡ Task

```yaml
# examples/tasks/gpu-env-preparation-task.yaml
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: gpu-env-preparation
spec:
  description: |
    Environment preparation for GPU scientific computing workflow
  params:
  - name: git-repo-url
    description: Git repository URL to clone
  - name: git-revision
    description: Git revision to checkout
    default: "main"
  workspaces:
  - name: source-code
    description: Workspace for source code checkout
  - name: shared-storage
    description: Shared storage for artifacts
  # ... è¯¦ç»†é…ç½®è§å®é™…æ–‡ä»¶
```

#### GPU Papermill æ‰§è¡Œ Task

```yaml
# examples/tasks/gpu-papermill-execution-task.yaml
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: gpu-papermill-execution
  labels:
    tekton.dev/gpu-required: "true"
spec:
  description: |
    GPU-accelerated Papermill execution task
  params:
  - name: gpu-count
    description: Number of GPUs required
    default: "1"
  - name: memory-limit
    description: Memory limit for the container
    default: "32Gi"
  - name: container-image
    description: GPU-enabled container image
    default: "nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12"
  # ... GPU èµ„æºé…ç½®
  resources:
    limits:
      nvidia.com/gpu: $(params.gpu-count)
      memory: $(params.memory-limit)
      cpu: "8"
    requests:
      nvidia.com/gpu: $(params.gpu-count)
      memory: "16Gi"
      cpu: "4"
```

### 2. Pipeline é…ç½®

```yaml
# examples/pipelines/gpu-scientific-computing-pipeline.yaml
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: gpu-scientific-computing-pipeline
spec:
  description: |
    GPU-accelerated scientific computing pipeline
  params:
  - name: git-repo-url
    description: Git repository URL
  - name: gpu-count
    description: Number of GPUs required
    default: "1"
  workspaces:
  - name: source-code-workspace
  - name: shared-artifacts-workspace
  - name: gpu-cache-workspace
  - name: test-execution-workspace
  
  tasks:
  - name: prepare-environment
    taskRef:
      name: gpu-env-preparation
  - name: execute-notebook-gpu
    taskRef:
      name: gpu-papermill-execution
    runAfter: ["prepare-environment"]
  - name: convert-to-html
    taskRef:
      name: jupyter-nbconvert
    runAfter: ["execute-notebook-gpu"]
  - name: run-tests
    taskRef:
      name: pytest-execution
    runAfter: ["convert-to-html"]
  - name: publish-results
    runAfter: ["run-tests"]
    # ... å†…è” Task å®šä¹‰
```

### 3. Triggers é…ç½®

#### EventListener

```yaml
# examples/triggers/gpu-pipeline-trigger-template.yaml
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: gpu-scientific-computing-eventlistener
spec:
  triggers:
  - name: gpu-scientific-computing-trigger
    interceptors:
    # GitHub webhook æ‹¦æˆªå™¨
    - ref:
        name: "github"
      params:
      - name: "secretRef"
        value:
          secretName: github-webhook-secret
          secretKey: webhook-secret
      - name: "eventTypes"
        value: ["push", "pull_request"]
    
    # CEL è¿‡æ»¤å™¨
    - ref:
        name: "cel"
      params:
      - name: "filter"
        value: >
          (body.ref == 'refs/heads/main' || 
           body.ref == 'refs/heads/develop') &&
          (body.head_commit.message.contains('[gpu]') ||
           body.head_commit.message.contains('[notebook]') ||
           body.head_commit.modified.exists(f, f.contains('notebooks/')))
```

### 4. GitHub Webhook é…ç½®

#### è®¾ç½®æ­¥éª¤

1. è¿›å…¥ GitHub ä»“åº“è®¾ç½®
2. é€‰æ‹© "Webhooks" > "Add webhook"
3. é…ç½®å‚æ•°ï¼š

```
Payload URL: http://<EXTERNAL_IP>:8080
Content type: application/json
Secret: <WEBHOOK_SECRET>  # éƒ¨ç½²è„šæœ¬ç”Ÿæˆçš„å¯†é’¥
SSL verification: Enable SSL verification (å¦‚æœä½¿ç”¨ HTTPS)
Events: Just the push event æˆ– Send me everything
```

#### è§¦å‘æ¡ä»¶

Pipeline ä¼šåœ¨ä»¥ä¸‹æƒ…å†µè‡ªåŠ¨è§¦å‘ï¼š

- æ¨é€åˆ° `main` æˆ– `develop` åˆ†æ”¯
- æäº¤æ¶ˆæ¯åŒ…å« `[gpu]` æˆ– `[notebook]` æ ‡ç­¾
- ä¿®æ”¹ `notebooks/` ç›®å½•ä¸‹çš„æ–‡ä»¶

## ğŸ“– ä½¿ç”¨æŒ‡å—

### æ‰‹åŠ¨æ‰§è¡Œ Pipeline

```bash
# åˆ›å»ºæ‰‹åŠ¨æ‰§è¡Œçš„ PipelineRun
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: gpu-manual-run-
  namespace: tekton-pipelines
spec:
  pipelineRef:
    name: gpu-scientific-computing-pipeline
  params:
  - name: git-repo-url
    value: "https://github.com/your-org/your-repo.git"
  - name: git-revision
    value: "main"
  - name: notebook-path
    value: "notebooks/01_scRNA_analysis_preprocessing.ipynb"
  - name: gpu-count
    value: "1"
  workspaces:
  - name: source-code-workspace
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi
        storageClassName: fast-ssd
  - name: shared-artifacts-workspace
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 50Gi
        storageClassName: fast-ssd
  - name: gpu-cache-workspace
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 100Gi
        storageClassName: fast-nvme
  - name: test-execution-workspace
    volumeClaimTemplate:
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 5Gi
        storageClassName: fast-ssd
  timeout: "2h"
EOF
```

### æŸ¥çœ‹æ‰§è¡ŒçŠ¶æ€

```bash
# æŸ¥çœ‹ PipelineRun åˆ—è¡¨
kubectl get pipelineruns -n tekton-pipelines

# æŸ¥çœ‹ç‰¹å®š PipelineRun è¯¦æƒ…
kubectl describe pipelinerun <pipelinerun-name> -n tekton-pipelines

# æŸ¥çœ‹ TaskRun çŠ¶æ€
kubectl get taskruns -n tekton-pipelines

# æŸ¥çœ‹ Pod çŠ¶æ€
kubectl get pods -n tekton-pipelines
```

### æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹ PipelineRun æ—¥å¿—
kubectl logs -f -n tekton-pipelines <pipelinerun-pod-name>

# æŸ¥çœ‹ç‰¹å®š Task æ—¥å¿—
kubectl logs -f -n tekton-pipelines <taskrun-pod-name> -c step-<step-name>

# æŸ¥çœ‹ GPU æ‰§è¡Œ Task æ—¥å¿—
kubectl logs -f -n tekton-pipelines <gpu-papermill-pod-name> -c step-gpu-papermill-execute
```

## ğŸ“Š ç›‘æ§ä¸è°ƒè¯•

### ç›‘æ§æŒ‡æ ‡

#### 1. é›†ç¾¤çº§åˆ«ç›‘æ§

```bash
# GPU èµ„æºä½¿ç”¨æƒ…å†µ
kubectl top nodes
kubectl get nodes -o jsonpath='{.items[*].status.allocatable.nvidia\.com/gpu}'

# å­˜å‚¨ä½¿ç”¨æƒ…å†µ
kubectl get pv
kubectl top pods -n tekton-pipelines
```

#### 2. Pipeline ç›‘æ§

```bash
# Pipeline æ‰§è¡Œå†å²
kubectl get pipelineruns -n tekton-pipelines -o wide

# æˆåŠŸç‡ç»Ÿè®¡
kubectl get pipelineruns -n tekton-pipelines -o jsonpath='{.items[*].status.conditions[0].reason}' | tr ' ' '\n' | sort | uniq -c

# æ‰§è¡Œæ—¶é—´åˆ†æ
kubectl get pipelineruns -n tekton-pipelines -o custom-columns=NAME:.metadata.name,START:.status.startTime,COMPLETION:.status.completionTime
```

#### 3. èµ„æºç›‘æ§

```yaml
# Prometheus ç›‘æ§é…ç½®ç¤ºä¾‹
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: tekton-pipelines-monitor
spec:
  selector:
    matchLabels:
      app: tekton-pipelines
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

### è°ƒè¯•æŠ€å·§

#### 1. å¸¸è§é—®é¢˜è¯Šæ–­

```bash
# æ£€æŸ¥ GPU èµ„æºå¯ç”¨æ€§
kubectl describe node <gpu-node-name> | grep -A 10 "Allocated resources"

# æ£€æŸ¥å­˜å‚¨ç±»å’Œ PVC
kubectl get storageclass
kubectl get pvc -n tekton-pipelines

# æ£€æŸ¥ Pod è°ƒåº¦æƒ…å†µ
kubectl get pods -n tekton-pipelines -o wide
kubectl describe pod <stuck-pod-name> -n tekton-pipelines
```

#### 2. æ—¥å¿—åˆ†æ

```bash
# æ£€æŸ¥ EventListener æ—¥å¿—
kubectl logs -f deployment/el-gpu-scientific-computing-eventlistener -n tekton-pipelines

# æ£€æŸ¥ Webhook è§¦å‘æ—¥å¿—
kubectl logs -f -l app=tekton-triggers-controller -n tekton-pipelines

# æ£€æŸ¥ GPU Task æ‰§è¡Œæ—¥å¿—
kubectl logs -f <gpu-task-pod> -n tekton-pipelines -c step-gpu-papermill-execute
```

#### 3. æ€§èƒ½åˆ†æ

```bash
# åˆ†æ GPU åˆ©ç”¨ç‡
kubectl exec -it <gpu-pod-name> -n tekton-pipelines -- nvidia-smi

# åˆ†æå­˜å‚¨ I/O
kubectl exec -it <task-pod> -n tekton-pipelines -- iostat -x 1

# åˆ†æå†…å­˜ä½¿ç”¨
kubectl top pod <pod-name> -n tekton-pipelines --containers
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. GPU èµ„æºä¼˜åŒ–

#### GPU èŠ‚ç‚¹æ± é…ç½®

```yaml
# GPU èŠ‚ç‚¹æ± äº²å’Œæ€§
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpu-workload
spec:
  template:
    spec:
      nodeSelector:
        accelerator: nvidia-tesla-gpu
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            preference:
              matchExpressions:
              - key: gpu-memory
                operator: Gt
                values: ["16000"]
```

#### GPU å¤šå®ä¾‹é…ç½®

```yaml
# GPU Multi-Instance GPU (MIG) é…ç½®
resources:
  limits:
    nvidia.com/gpu: 1  # å®Œæ•´ GPU
    # æˆ–è€…ä½¿ç”¨ MIG å®ä¾‹
    nvidia.com/mig-1g.5gb: 1  # 1/7 GPU å®ä¾‹
```

### 2. å­˜å‚¨ä¼˜åŒ–

#### ç¼“å­˜ç­–ç•¥

```yaml
# GPU ç¼“å­˜å·é…ç½®
- name: gpu-cache-workspace
  persistentVolumeClaim:
    claimName: gpu-cache-pvc
    # ä½¿ç”¨æœ¬åœ° NVMe å­˜å‚¨
    storageClass: local-nvme
    # å¯ç”¨ç¼“å­˜é¢„çƒ­
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 500Gi
```

#### æ•°æ®é¢„åŠ è½½

```bash
# é¢„åŠ è½½å¸¸ç”¨æ•°æ®é›†
kubectl create configmap common-datasets \
  --from-file=dataset1.csv \
  --from-file=dataset2.parquet \
  -n tekton-pipelines
```

### 3. å¹¶è¡ŒåŒ–ä¼˜åŒ–

#### Task å¹¶è¡Œæ‰§è¡Œ

```yaml
# å¹¶è¡Œ Task é…ç½®
tasks:
- name: download-test-framework
  taskRef:
    name: test-framework-setup
  runAfter: ["prepare-environment"]  # ä¸ GPU Task å¹¶è¡Œ

- name: execute-notebook-gpu
  taskRef:
    name: gpu-papermill-execution
  runAfter: ["prepare-environment"]  # å¹¶è¡Œæ‰§è¡Œ
```

#### å·¥ä½œç©ºé—´å…±äº«ä¼˜åŒ–

```yaml
# ä½¿ç”¨ ReadWriteMany å·æé«˜å¹¶å‘æ€§èƒ½
workspaces:
- name: shared-data
  persistentVolumeClaim:
    claimName: shared-data-pvc
    accessModes: ["ReadWriteMany"]  # æ”¯æŒå¤š Pod åŒæ—¶è®¿é—®
    storageClassName: efs-storage-class
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1. GPU è°ƒåº¦å¤±è´¥

**é—®é¢˜**: Pod æ— æ³•è°ƒåº¦åˆ° GPU èŠ‚ç‚¹

```bash
# è¯Šæ–­æ­¥éª¤
kubectl describe pod <pod-name> -n tekton-pipelines
# æŸ¥çœ‹ Events éƒ¨åˆ†çš„é”™è¯¯ä¿¡æ¯

# å¸¸è§åŸå› å’Œè§£å†³æ–¹æ¡ˆ
# 1. GPU èµ„æºä¸è¶³
kubectl get nodes -o jsonpath='{.items[*].status.allocatable.nvidia\.com/gpu}'
kubectl describe node <gpu-node> | grep -A 10 "Allocated resources"

# 2. èŠ‚ç‚¹æ ‡ç­¾ç¼ºå¤±
kubectl label nodes <node-name> accelerator=nvidia-tesla-gpu

# 3. GPU Operator æœªå®‰è£…
kubectl get pods -n gpu-operator-resources
```

#### 2. å­˜å‚¨é—®é¢˜

**é—®é¢˜**: PVC æ— æ³•ç»‘å®šæˆ–å­˜å‚¨ç©ºé—´ä¸è¶³

```bash
# æ£€æŸ¥å­˜å‚¨ç±»
kubectl get storageclass
kubectl describe storageclass fast-ssd

# æ£€æŸ¥ PVC çŠ¶æ€
kubectl get pvc -n tekton-pipelines
kubectl describe pvc <pvc-name> -n tekton-pipelines

# æ£€æŸ¥å¯ç”¨å­˜å‚¨
kubectl get pv
```

#### 3. ç½‘ç»œè¿æ¥é—®é¢˜

**é—®é¢˜**: GitHub Webhook æ— æ³•è§¦å‘ Pipeline

```bash
# æ£€æŸ¥ EventListener æœåŠ¡
kubectl get svc -n tekton-pipelines
kubectl describe svc el-gpu-scientific-computing-eventlistener -n tekton-pipelines

# æ£€æŸ¥ Webhook é…ç½®
kubectl get eventlistener gpu-scientific-computing-eventlistener -n tekton-pipelines -o yaml

# æµ‹è¯• Webhook è¿æ¥
curl -X POST http://<external-ip>:8080 \
  -H "Content-Type: application/json" \
  -d '{"test": "webhook"}'
```

#### 4. å®¹å™¨é•œåƒé—®é¢˜

**é—®é¢˜**: GPU å®¹å™¨é•œåƒæ‹‰å–å¤±è´¥æˆ–è¿è¡Œå¼‚å¸¸

```bash
# æ£€æŸ¥é•œåƒæ˜¯å¦å¯ç”¨
docker pull nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12

# æ£€æŸ¥é•œåƒæ‹‰å–ç­–ç•¥
kubectl describe pod <pod-name> -n tekton-pipelines | grep -A 5 "Image"

# æ£€æŸ¥é•œåƒä»“åº“è®¿é—®æƒé™
kubectl get secrets -n tekton-pipelines
```

### æ—¥å¿—æ”¶é›†è„šæœ¬

```bash
#!/bin/bash
# æ•…éšœæ’é™¤æ—¥å¿—æ”¶é›†è„šæœ¬

LOG_DIR="tekton-gpu-logs-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$LOG_DIR"

# æ”¶é›†é›†ç¾¤ä¿¡æ¯
kubectl cluster-info > "$LOG_DIR/cluster-info.txt"
kubectl get nodes -o wide > "$LOG_DIR/nodes.txt"
kubectl get storageclass > "$LOG_DIR/storage-classes.txt"

# æ”¶é›† Tekton ç»„ä»¶çŠ¶æ€
kubectl get all -n tekton-pipelines > "$LOG_DIR/tekton-resources.txt"
kubectl get pipelineruns -n tekton-pipelines -o yaml > "$LOG_DIR/pipelineruns.yaml"
kubectl get taskruns -n tekton-pipelines -o yaml > "$LOG_DIR/taskruns.yaml"

# æ”¶é›† Pod æ—¥å¿—
for pod in $(kubectl get pods -n tekton-pipelines -o name); do
  kubectl logs "$pod" -n tekton-pipelines > "$LOG_DIR/${pod#pod/}.log" 2>&1
done

# æ”¶é›† Events
kubectl get events -n tekton-pipelines --sort-by='.lastTimestamp' > "$LOG_DIR/events.txt"

echo "æ—¥å¿—å·²æ”¶é›†åˆ° $LOG_DIR ç›®å½•"
tar -czf "$LOG_DIR.tar.gz" "$LOG_DIR"
echo "å‹ç¼©åŒ…: $LOG_DIR.tar.gz"
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. èµ„æºç®¡ç†

#### GPU èµ„æºé…é¢

```yaml
# GPU å‘½åç©ºé—´èµ„æºé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: tekton-pipelines
spec:
  hard:
    requests.nvidia.com/gpu: "10"
    limits.nvidia.com/gpu: "10"
    requests.memory: "100Gi"
    limits.memory: "200Gi"
```

#### èµ„æºé™åˆ¶å’Œè¯·æ±‚

```yaml
# åˆç†çš„èµ„æºé…ç½®
resources:
  requests:
    nvidia.com/gpu: "1"
    memory: "16Gi"
    cpu: "4"
  limits:
    nvidia.com/gpu: "1"
    memory: "32Gi"
    cpu: "8"
```

### 2. å®‰å…¨é…ç½®

#### Pod å®‰å…¨ç­–ç•¥

```yaml
# GPU Pod å®‰å…¨ä¸Šä¸‹æ–‡
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false  # GPU é©±åŠ¨éœ€è¦å†™æƒé™
```

#### å¯†é’¥ç®¡ç†

```bash
# å®‰å…¨åœ°ç®¡ç† GitHub Webhook å¯†é’¥
kubectl create secret generic github-webhook-secret \
  --from-literal=webhook-secret="$(openssl rand -base64 32)" \
  -n tekton-pipelines

# å®šæœŸè½®æ¢å¯†é’¥
kubectl patch secret github-webhook-secret \
  -p='{"data":{"webhook-secret":"'$(openssl rand -base64 32 | base64 -w 0)'"}}' \
  -n tekton-pipelines
```

### 3. ç›‘æ§å‘Šè­¦

#### Prometheus è§„åˆ™

```yaml
# GPU Pipeline ç›‘æ§è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: tekton-gpu-alerts
spec:
  groups:
  - name: tekton-gpu
    rules:
    - alert: GPUPipelineFailure
      expr: increase(tekton_pipelinerun_failed_total[5m]) > 0
      labels:
        severity: warning
      annotations:
        summary: "GPU Pipeline æ‰§è¡Œå¤±è´¥"
        
    - alert: GPUResourceExhausted
      expr: nvidia_gpu_memory_used_percent > 90
      labels:
        severity: critical
      annotations:
        summary: "GPU å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
```

### 4. æ€§èƒ½ç›‘æ§

#### å…³é”®æŒ‡æ ‡

- **Pipeline æˆåŠŸç‡**: ç›‘æ§ Pipeline æ‰§è¡ŒæˆåŠŸç‡
- **GPU åˆ©ç”¨ç‡**: ç›‘æ§ GPU è®¡ç®—èµ„æºä½¿ç”¨æ•ˆç‡
- **æ‰§è¡Œæ—¶é—´**: è·Ÿè¸ª Notebook æ‰§è¡Œæ—¶é—´è¶‹åŠ¿
- **èµ„æºæ¶ˆè€—**: ç›‘æ§å†…å­˜ã€CPUã€å­˜å‚¨ä½¿ç”¨æƒ…å†µ
- **é˜Ÿåˆ—ç­‰å¾…æ—¶é—´**: ç›‘æ§ GPU èµ„æºç­‰å¾…æ—¶é—´

#### æ€§èƒ½åŸºå‡†

```bash
# å»ºç«‹æ€§èƒ½åŸºå‡†
# 1. Notebook æ‰§è¡Œæ—¶é—´åŸºå‡†
echo "Notebook æ‰§è¡Œæ—¶é—´åº”åœ¨ 30 åˆ†é’Ÿå†…å®Œæˆ"

# 2. GPU åˆ©ç”¨ç‡åŸºå‡†
echo "GPU åˆ©ç”¨ç‡åº”ä¿æŒåœ¨ 80% ä»¥ä¸Š"

# 3. å†…å­˜ä½¿ç”¨åŸºå‡†
echo "å†…å­˜ä½¿ç”¨ä¸åº”è¶…è¿‡ 85%"

# 4. å­˜å‚¨ I/O åŸºå‡†
echo "å­˜å‚¨è¯»å†™é€Ÿåº¦åº”æ»¡è¶³ 1GB/s"
```

## ğŸ“ æ€»ç»“

é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨å·²ç»å®Œæˆäº†ä» GitHub Actions åˆ° Tekton çš„ GPU ç§‘å­¦è®¡ç®—å·¥ä½œæµè¿ç§»ã€‚ä¸»è¦æ”¶ç›ŠåŒ…æ‹¬ï¼š

1. **æå‡èµ„æºåˆ©ç”¨ç‡**: GPU èµ„æºæŒ‰éœ€åˆ†é…ï¼Œé¿å…æµªè´¹
2. **å¢å¼ºå¯æ‰©å±•æ€§**: æ”¯æŒå¤šèŠ‚ç‚¹å¹¶è¡Œå¤„ç†
3. **æ”¹å–„å¯è§‚æµ‹æ€§**: å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—ä½“ç³»
4. **é™ä½è¿ç»´æˆæœ¬**: è‡ªåŠ¨åŒ–è¿ç»´å’Œæ•…éšœæ¢å¤
5. **æ ‡å‡†åŒ–æµç¨‹**: åŸºäº Kubernetes ç”Ÿæ€çš„æ ‡å‡†åŒ–å·¥å…·é“¾

### åç»­ä¼˜åŒ–å»ºè®®

1. **å®æ–½åˆ†å±‚ç¼“å­˜ç­–ç•¥** æå‡æ•°æ®è®¿é—®æ•ˆç‡
2. **é…ç½®è‡ªåŠ¨æ‰©ç¼©å®¹** æ ¹æ®è´Ÿè½½åŠ¨æ€è°ƒæ•´èµ„æº
3. **å»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»** å®ç°ä¸»åŠ¨è¿ç»´
4. **å®šæœŸæ€§èƒ½è°ƒä¼˜** ç¡®ä¿æœ€ä½³æ‰§è¡Œæ•ˆæœ
5. **åˆ¶å®šç¾å¤‡è®¡åˆ’** ä¿éšœä¸šåŠ¡è¿ç»­æ€§

å¦‚éœ€æŠ€æœ¯æ”¯æŒï¼Œè¯·å‚è€ƒé¡¹ç›® Issues æˆ–è”ç³»ç»´æŠ¤å›¢é˜Ÿã€‚ 