# Tekton GPU Pipeline éƒ¨ç½²æ•…éšœæ’é™¤

æœ¬æ–‡æ¡£è®°å½•åœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­å‘ç°çš„é—®é¢˜åŠè§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ å¸¸è§é—®é¢˜

### 1. kubectl å‘½ä»¤é—®é¢˜

#### é—®é¢˜ï¼š`kubectl version --short` ä¸è¢«æ”¯æŒ
**é”™è¯¯ä¿¡æ¯**ï¼š
```
error: unknown flag: --short
See 'kubectl version --help' for usage.
```

**åŸå› **ï¼šæ–°ç‰ˆæœ¬ kubectl å·²ç§»é™¤ `--short` å‚æ•°

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é”™è¯¯å‘½ä»¤
kubectl version --short

# æ­£ç¡®å‘½ä»¤
kubectl version
```

**çŠ¶æ€**ï¼šå·²ä¿®å¤æ–‡æ¡£

---

### 2. Git Clone å®‰å…¨å¤„ç†é—®é¢˜

#### é—®é¢˜ï¼šé‡å¤è¿è¡Œpipelineæ—¶git cloneå¤±è´¥
**é”™è¯¯ä¿¡æ¯**ï¼š
```
fatal: destination path 'source' already exists and is not an empty directory.
```

**åŸå› **ï¼šPipelineé‡å¤è¿è¡Œæ—¶ï¼Œworkspaceä¸­å­˜åœ¨ä¹‹å‰è¿è¡Œçš„æ®‹ç•™æ–‡ä»¶

**è§£å†³æ–¹æ¡ˆ**ï¼šè‡ªåŠ¨å¤‡ä»½å’Œå®‰å…¨å¤„ç†æœºåˆ¶

æˆ‘ä»¬çš„å®‰å…¨git cloneå®ç°åŒ…å«ä»¥ä¸‹ç‰¹æ€§ï¼š
- **è‡ªåŠ¨ç›®å½•å¤‡ä»½**ï¼šæ£€æµ‹åˆ°å·²å­˜åœ¨ç›®å½•æ—¶ï¼Œè‡ªåŠ¨åˆ›å»ºæ—¶é—´æˆ³å¤‡ä»½
- **é‡è¯•æœºåˆ¶**ï¼šcloneå¤±è´¥æ—¶è‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š3æ¬¡ï¼‰
- **å›æ»šèƒ½åŠ›**ï¼šå¤±è´¥æ—¶å¯è‡ªåŠ¨æ¢å¤å¤‡ä»½
- **è¯¦ç»†æ—¥å¿—**ï¼šåŒ…å«æ—¶é—´æˆ³çš„è¯¦ç»†æ“ä½œæ—¥å¿—

**å®‰å…¨å¤„ç†æµç¨‹**ï¼š
```bash
# 1. æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
if [ -d "${TARGET_DIR}" ]; then
  # 2. åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
  TIMESTAMP=$(date '+%Y%m%d_%H%M%S')
  BACKUP_DIR="${TARGET_DIR}_backup_${TIMESTAMP}"
  mv "${TARGET_DIR}" "${BACKUP_DIR}"
fi

# 3. æ‰§è¡Œgit cloneï¼ˆå¸¦é‡è¯•ï¼‰
for attempt in $(seq 1 ${MAX_RETRIES}); do
  if git clone "${REPO_URL}" "${TARGET_DIR}"; then
    break
  fi
  # æ¸…ç†å¤±è´¥çš„éƒ¨åˆ†clone
  rm -rf "${TARGET_DIR}"
  sleep $((attempt * 5))  # æŒ‡æ•°é€€é¿
done
```

**å·²æ›´æ–°çš„Taskç»„ä»¶**ï¼š
1. **gpu-env-preparation-task-fixed.yaml** - æ·»åŠ è‡ªåŠ¨å¤‡ä»½æœºåˆ¶
2. **pytest-execution-task.yaml** - æµ‹è¯•ä»“åº“cloneçš„å®‰å…¨å¤„ç†  
3. **safe-git-clone-task.yaml**ï¼ˆæ–°å¢ï¼‰- ç‹¬ç«‹çš„å®‰å…¨git clone task

**æ¸…ç†å¤‡ä»½ç›®å½•**ï¼š
```bash
# æ¸…ç†7å¤©å‰çš„å¤‡ä»½
find /workspace -name "*_backup_*" -type d -mtime +7 -exec rm -rf {} +
```

**çŠ¶æ€**ï¼šå·²ä¿®å¤å¹¶å¢å¼ºå®‰å…¨æªæ–½

---

### 3. ç¯å¢ƒæ¸…ç†é—®é¢˜

#### é—®é¢˜ï¼šç°æœ‰ Tekton ç»„ä»¶å¯¼è‡´éƒ¨ç½²å†²çª
**ç—‡çŠ¶**ï¼š
- å®‰è£…è¿‡ç¨‹ä¸­èµ„æºå·²å­˜åœ¨é”™è¯¯
- EventListener å¤„äº CrashLoopBackOff çŠ¶æ€
- æ— æ³•åˆ›å»ºæ–°çš„ Pipeline èµ„æº

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ‰§è¡Œå®Œæ•´ç¯å¢ƒæ¸…ç†
chmod +x scripts/cleanup/clean-tekton-environment.sh
./scripts/cleanup/clean-tekton-environment.sh
```

**éªŒè¯æ¸…ç†å®Œæˆ**ï¼š
```bash
# åº”è¯¥æ²¡æœ‰è¾“å‡º
kubectl get namespaces | grep tekton
kubectl get pods --all-namespaces | grep tekton
```

---

### 4. Tekton API ç‰ˆæœ¬é—®é¢˜

#### é—®é¢˜ï¼šTask å®šä¹‰ä¸­çš„ resources å­—æ®µä½ç½®é”™è¯¯
**é”™è¯¯ä¿¡æ¯**ï¼š
```
error when creating: Task in version "v1" cannot be handled as a Task: strict decoding error: unknown field "spec.steps[0].resources"
```

**åŸå› **ï¼šTekton v1 API ä¸­èµ„æºå®šä¹‰åº”ä½¿ç”¨ `computeResources`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# é”™è¯¯é…ç½®
spec:
  steps:
  - name: step
    resources:
      limits:
        nvidia.com/gpu: "1"

# æ­£ç¡®é…ç½®
spec:
  steps:
  - name: step
    computeResources:
      limits:
        nvidia.com/gpu: "1"
```

---

### 5. åŠ¨æ€å‚æ•°é—®é¢˜

#### é—®é¢˜ï¼šèµ„æºé‡å¿…é¡»åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
**é”™è¯¯ä¿¡æ¯**ï¼š
```
quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$'
```

**åŸå› **ï¼šTekton ä¸æ¥å—åŠ¨æ€å‚æ•°ä½œä¸ºèµ„æºé‡å€¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
```yaml
# é”™è¯¯é…ç½®
computeResources:
  limits:
    nvidia.com/gpu: $(params.gpu-count)

# æ­£ç¡®é…ç½®  
computeResources:
  limits:
    nvidia.com/gpu: "1"
```

---

### 6. YAML æ ¼å¼é—®é¢˜

#### é—®é¢˜ï¼šå¤æ‚çš„å¤šè¡Œè„šæœ¬å¯¼è‡´ YAML è§£æé”™è¯¯
**é”™è¯¯ä¿¡æ¯**ï¼š
```
error converting YAML to JSON: yaml: line X: could not find expected ':'
```

**åŸå› **ï¼šPython è„šæœ¬å—ç¼©è¿›é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**ï¼š
ç®€åŒ–å¤æ‚çš„ Python è„šæœ¬ï¼Œä½¿ç”¨æ›´ç®€å•çš„ shell å‘½ä»¤ï¼š

```yaml
# å¤æ‚çš„ Python è„šæœ¬ï¼ˆå®¹æ˜“å‡ºé”™ï¼‰
script: |
  python3 << 'EOF'
  import json
  # å¤æ‚é€»è¾‘
  EOF

# ç®€åŒ–çš„ shell å‘½ä»¤ï¼ˆæ¨èï¼‰
script: |
  #!/bin/bash
  echo "ç®€å•éªŒè¯"
  grep -q "pattern" file || echo "Not found"
```

---

### 7. Dashboard è®¿é—®é—®é¢˜

#### é—®é¢˜ï¼šDashboard ç™»å½•æˆåŠŸä½†å†…å®¹ä¸€ç›´ loading
**ç—‡çŠ¶**ï¼š
- å¯ä»¥è¾“å…¥ç”¨æˆ·åå¯†ç ç™»å½•
- ç™»å½•åé¡µé¢ç©ºç™½æˆ–ä¸€ç›´æ˜¾ç¤ºloading
- æ— æ³•æ˜¾ç¤ºPipelineã€Taskç­‰å†…å®¹

**é”™è¯¯æ—¥å¿—**ï¼š
```
dial tcp 10.96.0.1:443: i/o timeout
Error getting the Tekton dashboard info ConfigMap
```

**åŸå› **ï¼šç½‘ç»œç­–ç•¥è¿‡äºä¸¥æ ¼ï¼Œé˜»æ­¢äº†Dashboardè®¿é—®Kubernetes APIæœåŠ¡å™¨

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# æ–¹æ¡ˆ1ï¼šé‡å¯Dashboard Podï¼ˆä¸´æ—¶ï¼‰
kubectl delete pod -l app.kubernetes.io/name=dashboard -n tekton-pipelines

# æ–¹æ¡ˆ2ï¼šä¿®æ­£ç½‘ç»œç­–ç•¥ï¼ˆæ¨èï¼‰
# é…ç½®è„šæœ¬å·²åŒ…å«ä¿®æ­£åçš„ç½‘ç»œç­–ç•¥ï¼Œé‡æ–°è¿è¡Œå³å¯
./scripts/install/02-configure-tekton-dashboard.sh
```

**æ ¹æœ¬åŸå› åˆ†æ**ï¼š
- åŸç½‘ç»œç­–ç•¥çš„ `to: namespaceSelector: {}` é™åˆ¶è¿‡ä¸¥
- Dashboardéœ€è¦è®¿é—® `10.96.0.1:443` (Kubernetes APIæœåŠ¡å™¨)
- ä¿®æ­£åçš„ç­–ç•¥ä½¿ç”¨ `to: []` å…è®¸è®¿é—®é›†ç¾¤å†…APIæœåŠ¡å™¨

#### é—®é¢˜ï¼šDashboard HTTPSè®¿é—®å¤±è´¥ - SSLè¯ä¹¦SANè­¦å‘Š
**ç—‡çŠ¶**ï¼š
- Dashboardç½‘å€æ— æ³•è®¿é—®
- Ingress Controlleræ—¥å¿—æ˜¾ç¤ºSSLè¯ä¹¦è­¦å‘Š
- æµè§ˆå™¨æç¤ºè¯ä¹¦é”™è¯¯

**é”™è¯¯æ—¥å¿—**ï¼š
```
Unexpected error validating SSL certificate: x509: certificate relies on legacy Common Name field, use SANs instead
```

**åŸå› **ï¼šSSLè¯ä¹¦ä½¿ç”¨äº†ä¼ ç»Ÿçš„Common Nameå­—æ®µï¼Œç°ä»£ç³»ç»Ÿè¦æ±‚ä½¿ç”¨SANï¼ˆSubject Alternative Nameï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é‡æ–°ç”ŸæˆåŒ…å«SANçš„SSLè¯ä¹¦
NODE_IP=$(hostname -I | awk '{print $1}')
DOMAIN="tekton.$NODE_IP.nip.io"

# ç”ŸæˆåŒ…å«SANçš„æ–°è¯ä¹¦
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout /tmp/tls.key \
  -out /tmp/tls.crt \
  -subj "/CN=$DOMAIN/O=tekton-dashboard" \
  -addext "subjectAltName=DNS:$DOMAIN"

# æ›´æ–°TLS Secret
kubectl delete secret tekton-dashboard-tls -n tekton-pipelines
kubectl create secret tls tekton-dashboard-tls \
  --cert=/tmp/tls.crt \
  --key=/tmp/tls.key \
  -n tekton-pipelines

# éªŒè¯è¯ä¹¦åŒ…å«SAN
openssl x509 -in /tmp/tls.crt -text -noout | grep -A5 "Subject Alternative Name"
```

**éªŒè¯ä¿®å¤**ï¼š
```bash
# æ£€æŸ¥Ingress Controlleræ—¥å¿—ï¼ŒSANè­¦å‘Šåº”è¯¥æ¶ˆå¤±
kubectl logs -n ingress-nginx deploy/ingress-nginx-controller --tail=10

# æµ‹è¯•HTTPSè®¿é—®
curl -k -I https://tekton.$NODE_IP.nip.io/
```

**é¢„é˜²æªæ–½**ï¼š
- åœ¨01å®‰è£…æ–‡æ¡£ä¸­å·²æ›´æ–°è¯ä¹¦ç”Ÿæˆå‘½ä»¤ï¼ŒåŒ…å«SANé…ç½®
- å»ºè®®å®šæœŸæ›´æ–°è¯ä¹¦ï¼Œé¿å…è¿‡æœŸ

#### é—®é¢˜ï¼šDashboardå®Œå…¨æ— æ³•è®¿é—® - Ingress Controlleré…ç½®å†²çª
**ç—‡çŠ¶**ï¼š
- Dashboardç½‘å€å®Œå…¨æ— æ³•è®¿é—®ï¼Œè¿æ¥è¶…æ—¶
- HTTPå’ŒHTTPSéƒ½æ— æ³•è®¿é—®
- NodePortè®¿é—®ä¹Ÿè¶…æ—¶
- DNSè§£ææ­£å¸¸ï¼Œpingé€šç•…

**é”™è¯¯ç°è±¡**ï¼š
```bash
# æ‰€æœ‰è®¿é—®æ–¹å¼éƒ½è¶…æ—¶
curl https://tekton.10.34.2.129.nip.io/  # è¶…æ—¶
curl http://10.34.2.129:31960/            # NodePortä¹Ÿè¶…æ—¶
```

**åŸå› **ï¼šIngress Controllerè™½ç„¶é…ç½®äº†hostNetworkï¼Œä½†å®é™…ä¸Šæ²¡æœ‰æ­£ç¡®ç»‘å®šåˆ°ä¸»æœºç«¯å£ï¼Œå¯èƒ½å­˜åœ¨é…ç½®å†²çª

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# é‡æ–°éƒ¨ç½²Ingress Controller
kubectl delete deployment ingress-nginx-controller -n ingress-nginx

# é‡æ–°å®‰è£…å¹¶æ­£ç¡®é…ç½®
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/deploy.yaml

# ç­‰å¾…å¯åŠ¨
kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s

# é‡æ–°é…ç½®hostNetwork
kubectl patch deployment ingress-nginx-controller -n ingress-nginx -p '{"spec":{"template":{"spec":{"hostNetwork":true,"dnsPolicy":"ClusterFirstWithHostNet"}}}}'

# ç­‰å¾…é‡æ–°éƒ¨ç½²å®Œæˆ
kubectl rollout status deployment/ingress-nginx-controller -n ingress-nginx --timeout=120s
```

**éªŒè¯ä¿®å¤**ï¼š
```bash
# 1. æµ‹è¯•HTTPé‡å®šå‘ (åº”è¯¥è¿”å›308)
curl -s -o /dev/null -w "HTTP Status: %{http_code}\n" http://10.34.2.129/

# 2. æµ‹è¯•HTTPSè®¤è¯ (åº”è¯¥è¿”å›401)  
curl -k -s -o /dev/null -w "HTTPS Status: %{http_code}\n" https://10.34.2.129/

# 3. æµ‹è¯•å®Œæ•´è®¿é—® (åº”è¯¥è¿”å›200)
curl -k -u "admin:å¯†ç " -s -o /dev/null -w "è®¤è¯çŠ¶æ€: %{http_code}\n" https://tekton.10.34.2.129.nip.io/
```

**é¢„æœŸç»“æœ**ï¼š
- HTTP: 308 (é‡å®šå‘åˆ°HTTPS) âœ…
- HTTPS: 401 (éœ€è¦è®¤è¯) âœ…  
- è®¤è¯è®¿é—®: 200 (æˆåŠŸ) âœ…

**çŠ¶æ€**ï¼šå·²ä¿®å¤è„šæœ¬å’Œæ–‡æ¡£

---

## ğŸ” è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯
```bash
# æŸ¥çœ‹ Pod æ—¥å¿—
kubectl logs <pod-name> -n tekton-pipelines

# æŸ¥çœ‹ EventListener çŠ¶æ€
kubectl describe eventlistener <name> -n tekton-pipelines

# æŸ¥çœ‹ Task æ‰§è¡Œæ—¥å¿—
kubectl logs -f <taskrun-pod> -n tekton-pipelines
```

### éªŒè¯ GPU æ”¯æŒ
```bash
# æ£€æŸ¥ GPU èŠ‚ç‚¹æ ‡ç­¾
kubectl get nodes -l accelerator=nvidia-tesla-gpu

# æ£€æŸ¥ NVIDIA GPU Operator
kubectl get pods -n gpu-operator-resources
```

### æ£€æŸ¥ç½‘ç»œè¿æ¥
```bash
# æµ‹è¯• EventListener æœåŠ¡
kubectl get svc -n tekton-pipelines | grep eventlistener

# ç«¯å£è½¬å‘æµ‹è¯•
kubectl port-forward svc/<service-name> 8080:8080 -n tekton-pipelines
```

---

### 8. PVC Workspace ç»‘å®šé—®é¢˜ (é‡è¦æ¡ˆä¾‹)

#### é—®é¢˜ï¼šTaskRunValidationFailed - "more than one PersistentVolumeClaim is bound"
**é”™è¯¯ä¿¡æ¯**ï¼š
```
[User error] more than one PersistentVolumeClaim is bound
```

**æ ¹æœ¬åŸå› åˆ†æ**ï¼š
1. **Taskå®šä¹‰ä½¿ç”¨å¤šä¸ªworkspace**: åŸå§‹Taskä½¿ç”¨äº†`source-code`å’Œ`shared-storage`ä¸¤ä¸ªworkspace
2. **PipelineRunä¸­workspaceç»‘å®šå†²çª**: å¤šä¸ªworkspaceç»‘å®šåˆ°åŒä¸€ä¸ªPVCæ—¶ä¼šäº§ç”Ÿå†²çª
3. **å­˜å‚¨ç±»é…ç½®é—®é¢˜**: PVCçš„storageClassNameè®¾ç½®ä¸æ­£ç¡®

**å®Œæ•´è¯Šæ–­å’Œè§£å†³æµç¨‹**ï¼š

**æ­¥éª¤1: è¯Šæ–­PVCçŠ¶æ€**
```bash
# æ£€æŸ¥PVCçŠ¶æ€
kubectl get pvc -n tekton-pipelines -o wide

# æ£€æŸ¥å­˜å‚¨ç±»
kubectl get storageclass

# æŸ¥çœ‹PVCè¯¦ç»†ä¿¡æ¯
kubectl describe pvc <pvc-name> -n tekton-pipelines

# æ£€æŸ¥å¤±è´¥çš„TaskRun
kubectl describe taskrun <taskrun-name> -n tekton-pipelines
```

**æ­¥éª¤2: éªŒè¯PVCé…ç½®æ–‡ä»¶**
æ£€æŸ¥ `examples/basic/workspaces/gpu-pipeline-workspaces.yaml` ä¸­çš„å­˜å‚¨ç±»é…ç½®ï¼š
```yaml
# æ­£ç¡®é…ç½®ç¤ºä¾‹
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: shared-artifacts-workspace
  namespace: tekton-pipelines
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: "local-path"  # ä½¿ç”¨é›†ç¾¤ä¸­å¯ç”¨çš„å­˜å‚¨ç±»
```

**æ­¥éª¤3: ä¿®å¤Taskå®šä¹‰**
é—®é¢˜ï¼šåŸTaskä½¿ç”¨å¤šä¸ªworkspace
```yaml
# æœ‰é—®é¢˜çš„é…ç½®
workspaces:
- name: source-code
  description: Workspace for source code checkout
- name: shared-storage
  description: Shared storage for artifacts
```

è§£å†³æ–¹æ¡ˆï¼šåˆå¹¶ä¸ºå•ä¸€workspace
```yaml
# ä¿®å¤åçš„é…ç½®
workspaces:
- name: shared-storage
  description: Shared storage for source code, artifacts, and cache
```

**æ­¥éª¤4: åˆ›å»ºä¿®å¤ç‰ˆæœ¬çš„Task**
åˆ›å»º `gpu-env-preparation-task-fixed.yaml`:
```yaml
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: gpu-env-preparation-fixed
  namespace: tekton-pipelines
spec:
  description: |
    Fixed version that uses only one workspace to avoid conflicts.
  params:
  - name: git-repo-url
    description: Git repository URL to clone
    type: string
  - name: git-revision
    description: Git revision to checkout
    type: string
    default: "main"
  - name: workspace-subdir
    description: Subdirectory within workspace to clone repository
    type: string
    default: "source"
  workspaces:
  - name: shared-storage
    description: Shared storage for source code and artifacts
    mountPath: /workspace/shared
  steps:
  - name: git-clone
    image: alpine/git:latest
    env:
    - name: WORKSPACE_SHARED_PATH
      value: $(workspaces.shared-storage.path)
    script: |
      #!/bin/sh
      set -eu
      
      echo "ğŸš€ Starting GPU environment preparation..."
      cd "${WORKSPACE_SHARED_PATH}"
      
      # Remove existing directory if it exists (é‡è¦ï¼šé˜²æ­¢å†²çª)
      if [ -d "$(params.workspace-subdir)" ]; then
        echo "ğŸ§¹ Removing existing directory: $(params.workspace-subdir)"
        rm -rf "$(params.workspace-subdir)"
      fi
      
      echo "ğŸ“¥ Cloning repository..."
      git clone "$(params.git-repo-url)" "$(params.workspace-subdir)"
      
      cd "$(params.workspace-subdir)"
      # å¤åˆ¶æ–‡ä»¶åˆ°workspaceæ ¹ç›®å½•ä¾›å…¶ä»–taskä½¿ç”¨
      cp -r . "${WORKSPACE_SHARED_PATH}/"
      
      echo "âœ… Environment preparation completed successfully"
```

**æ­¥éª¤5: é€æ­¥éªŒè¯ä¿®å¤**

**5.1 å…ˆéªŒè¯ç®€å•workspaceåŠŸèƒ½**
```bash
# ä½¿ç”¨æˆ‘ä»¬æä¾›çš„æµ‹è¯•æ–‡ä»¶
kubectl apply -f examples/development/debug/debug-workspace-test.yaml
kubectl get pipelinerun debug-workspace-test -n tekton-pipelines -w
kubectl logs -l tekton.dev/pipelineRun=debug-workspace-test -n tekton-pipelines
```

**5.2 éªŒè¯git cloneåŠŸèƒ½**
```bash
kubectl apply -f examples/development/debug/debug-git-clone-test.yaml
kubectl get pipelinerun debug-git-clone-test -n tekton-pipelines -w
kubectl logs -l tekton.dev/pipelineRun=debug-git-clone-test -n tekton-pipelines
```

**5.3 éªŒè¯ä¿®å¤ç‰ˆæœ¬çš„ç¯å¢ƒå‡†å¤‡ä»»åŠ¡**
```bash
# åº”ç”¨ä¿®å¤ç‰ˆæœ¬çš„task
kubectl apply -f examples/basic/tasks/gpu-env-preparation-task-fixed.yaml

# åˆ›å»ºæµ‹è¯•pipeline
kubectl apply -f examples/basic/workspaces/gpu-env-test-fixed.yaml
kubectl get pipelinerun gpu-env-test-fixed -n tekton-pipelines -w
```

**å®Œæ•´è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. æ¸…ç†ç°æœ‰èµ„æº
kubectl delete pvc -n tekton-pipelines --all
kubectl delete pipelinerun --all -n tekton-pipelines

# 2. é‡æ–°åˆ›å»ºPVCï¼ˆä½¿ç”¨æ­£ç¡®å­˜å‚¨ç±»ï¼‰
kubectl apply -f examples/basic/workspaces/gpu-pipeline-workspaces.yaml

# 3. åº”ç”¨ä¿®å¤ç‰ˆæœ¬çš„tasks
kubectl apply -f examples/basic/tasks/gpu-env-preparation-task-fixed.yaml
kubectl apply -f examples/basic/tasks/gpu-papermill-execution-task.yaml
kubectl apply -f examples/basic/tasks/pytest-execution-task.yaml

# 4. æ‰§è¡Œå®Œæ•´çš„ä¿®å¤ç‰ˆæœ¬pipeline
kubectl apply -f examples/basic/workspaces/gpu-complete-pipeline-fixed.yaml
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… ç¯å¢ƒå‡†å¤‡ä»»åŠ¡æˆåŠŸæ‰§è¡Œ
- âœ… Git repositoryæ­£ç¡®cloneåˆ°workspace
- âœ… æ–‡ä»¶æˆåŠŸå¤åˆ¶åˆ°shared workspace
- âœ… é¿å…äº†workspaceç»‘å®šå†²çª

---

### 9. GPUè®¿é—®é—®é¢˜è¯Šæ–­ (é‡è¦æ¡ˆä¾‹)

#### é—®é¢˜ï¼šGPU Pipelineæ‰§è¡Œå¤±è´¥ï¼ŒCUDAæ— æ³•æ£€æµ‹åˆ°è®¾å¤‡
**ç°è±¡**: 
- Pipelineä¸­çš„ç¯å¢ƒå‡†å¤‡ä»»åŠ¡æˆåŠŸ
- GPU papermillæ‰§è¡Œä»»åŠ¡å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š`CUDARuntimeError: cudaErrorNoDevice: no CUDA-capable device is detected`
- nvidia-smiåœ¨å®¹å™¨ä¸­èƒ½è¿è¡Œï¼Œä½†CUDAè¿è¡Œæ—¶æ— æ³•è®¿é—®GPU

**å®Œæ•´è¯Šæ–­æµç¨‹**:

**æ­¥éª¤1: éªŒè¯é›†ç¾¤GPUèµ„æº**
```bash
# æ£€æŸ¥èŠ‚ç‚¹GPUèµ„æº
kubectl describe nodes | grep -A 10 -B 5 "nvidia.com/gpu"

# æŸ¥çœ‹GPUè®¾å¤‡æ’ä»¶çŠ¶æ€
kubectl get daemonset -A | grep nvidia

# æ£€æŸ¥èŠ‚ç‚¹GPUåˆ†é…
kubectl get nodes -o json | jq '.items[0].status.allocatable."nvidia.com/gpu"'
```

**æ­¥éª¤2: åˆ›å»ºGPUæµ‹è¯•PodéªŒè¯ç¡¬ä»¶è®¿é—®**
åˆ›å»ºæµ‹è¯•æ–‡ä»¶ `gpu-test-pod.yaml`:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test-pod
  namespace: tekton-pipelines
spec:
  restartPolicy: Never
  nodeSelector:
    accelerator: nvidia-tesla-gpu
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  containers:
  - name: gpu-test
    image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
    command: ["/bin/bash"]
    args:
    - -c
    - |
      echo "ğŸ” Checking GPU access in container..."
      echo "ğŸ“ Checking /dev/nvidia* devices:"
      ls -la /dev/nvidia* || echo "âŒ No nvidia devices found"
      echo ""
      echo "ğŸ”§ Testing nvidia-smi:"
      nvidia-smi || echo "âŒ nvidia-smi failed"
      echo ""
      echo "ğŸ Testing Python CUDA access:"
      python3 -c "import cupy as cp; print('âœ… CuPy version:', cp.__version__); print('âœ… CUDA devices:', cp.cuda.runtime.getDeviceCount())" || echo "âŒ Python CUDA test failed"
      echo ""
      echo "ğŸ’¤ Sleeping for 300 seconds for debugging..."
      sleep 300
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility"
```

**æ­¥éª¤3: æ‰§è¡ŒGPUæµ‹è¯•**
```bash
# åˆ›å»ºæµ‹è¯•pod
kubectl apply -f examples/development/testing/gpu-test-pod.yaml

# ç›‘æ§å¯åŠ¨çŠ¶æ€
kubectl get pod gpu-test-pod -n tekton-pipelines -w

# æŸ¥çœ‹æµ‹è¯•ç»“æœ
kubectl logs gpu-test-pod -n tekton-pipelines

# æ¸…ç†æµ‹è¯•pod
kubectl delete pod gpu-test-pod -n tekton-pipelines
```

**æ­¥éª¤3.1: Tektonç¯å¢ƒä¸­çš„GPUæµ‹è¯•**
```bash
# åœ¨Tektonç¯å¢ƒä¸­éªŒè¯GPUè®¿é—®
kubectl apply -f examples/development/testing/gpu-papermill-debug-test.yaml
kubectl get pipelinerun gpu-papermill-debug-test -n tekton-pipelines -w
kubectl logs -l tekton.dev/pipelineRun=gpu-papermill-debug-test -n tekton-pipelines
```

**æ­¥éª¤3.2: Papermillæ‰§è¡Œæµ‹è¯•**
```bash
# æµ‹è¯•Papermillæ‰§è¡Œå«RMMåˆå§‹åŒ–çš„notebook
kubectl apply -f examples/development/testing/gpu-papermill-notebook-test.yaml
kubectl get pipelinerun gpu-papermill-notebook-test -n tekton-pipelines -w
kubectl logs -l tekton.dev/pipelineRun=gpu-papermill-notebook-test -n tekton-pipelines -c step-execute-with-papermill
```

**æ­¥éª¤4: å¯¹æ¯”Tekton Taskä¸æˆåŠŸé…ç½®çš„å·®å¼‚**
å¦‚æœæµ‹è¯•podæˆåŠŸä½†Tekton taskå¤±è´¥ï¼Œæ£€æŸ¥ä»¥ä¸‹é…ç½®å·®å¼‚ï¼š

1. **å®‰å…¨ä¸Šä¸‹æ–‡é…ç½®**:
```yaml
# åœ¨Taskçš„stepsä¸­æ·»åŠ 
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
  runAsNonRoot: false
  runAsUser: 0
  seccompProfile:
    type: RuntimeDefault
```

2. **ç¯å¢ƒå˜é‡é…ç½®**:
```yaml
env:
- name: NVIDIA_VISIBLE_DEVICES
  value: "all"
- name: NVIDIA_DRIVER_CAPABILITIES
  value: "compute,utility"
```

**æ­¥éª¤5: é€æ­¥éªŒè¯Tektonç»„ä»¶**

**5.1 ç®€å•Workspaceæµ‹è¯•**
åˆ›å»º `debug-workspace-test.yaml`:
```yaml
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: debug-workspace-test
  namespace: tekton-pipelines
spec:
  pipelineSpec:
    workspaces:
    - name: test-workspace
    tasks:
    - name: simple-test
      workspaces:
      - name: shared
        workspace: test-workspace
      taskSpec:
        workspaces:
        - name: shared
        steps:
        - name: test-step
          image: alpine:latest
          script: |
            #!/bin/sh
            echo "Testing workspace access..."
            ls -la $(workspaces.shared.path)
            echo "Creating test file..."
            echo "Hello from Tekton" > $(workspaces.shared.path)/test.txt
            cat $(workspaces.shared.path)/test.txt
            echo "Test completed successfully!"
  workspaces:
  - name: test-workspace
    persistentVolumeClaim:
      claimName: source-code-workspace
```

**5.2 Git Cloneæµ‹è¯•**
åˆ›å»º `debug-git-clone-test.yaml`:
```yaml
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: debug-git-clone-test
  namespace: tekton-pipelines
spec:
  pipelineSpec:
    workspaces:
    - name: workspace
    tasks:
    - name: git-clone-test
      workspaces:
      - name: shared
        workspace: workspace
      taskSpec:
        workspaces:
        - name: shared
        params:
        - name: git-repo-url
          type: string
        steps:
        - name: clone-step
          image: alpine/git:latest
          script: |
            #!/bin/sh
            set -eu
            echo "ğŸš€ Starting git clone test..."
            echo "ğŸ“ Workspace path: $(workspaces.shared.path)"
            echo "ğŸ”— Repository URL: $(params.git-repo-url)"
            
            cd $(workspaces.shared.path)
            
            # Remove existing directory if it exists
            if [ -d "source" ]; then
              echo "ğŸ§¹ Removing existing directory: source"
              rm -rf "source"
            fi
            
            echo "ğŸ“¥ Cloning repository..."
            git clone "$(params.git-repo-url)" source
            
            cd source
            echo "âœ… Clone completed. Repository contents:"
            ls -la
            
            if [ -d "notebooks" ]; then
              echo "âœ… notebooks/ directory found"
              ls -la notebooks/ | head -5
            fi
            
            echo "âœ… Git clone test completed successfully"
      params:
      - name: git-repo-url
        value: "https://github.com/johnnynv/Real-world_Tekton_Installation_Guide.git"
  workspaces:
  - name: workspace
    persistentVolumeClaim:
      claimName: source-code-workspace
```

**è§£å†³æ–¹æ¡ˆæ€»ç»“**:
1. **ä¿®å¤Taské…ç½®**: æ·»åŠ æ­£ç¡®çš„securityContext
2. **ç®€åŒ–Workspace**: æ¯ä¸ªTaskåªä½¿ç”¨ä¸€ä¸ªworkspaceé¿å…å†²çª
3. **å¤„ç†ç›®å½•å†²çª**: åœ¨git cloneå‰åˆ é™¤å·²å­˜åœ¨çš„ç›®å½•
4. **éªŒè¯GPUè®¿é—®**: ä½¿ç”¨ç‹¬ç«‹æµ‹è¯•podéªŒè¯ç¡¬ä»¶é…ç½®

**ğŸ“‹ å®Œæ•´çš„GPUé—®é¢˜è°ƒè¯•æ¡ˆä¾‹è®°å½•**

æ­¤æ¡ˆä¾‹å±•ç¤ºäº†ç³»ç»Ÿæ€§è¯Šæ–­GPU Pipelineé—®é¢˜çš„å®Œæ•´æµç¨‹ï¼š

**è¯Šæ–­ç»“æœæ€»ç»“**ï¼š
- âœ… **ç‹¬ç«‹GPUæµ‹è¯•** - GPUç¡¬ä»¶è®¿é—®å®Œå…¨æ­£å¸¸
- âœ… **Tekton GPUæµ‹è¯•** - åŒ…æ‹¬RMMåˆå§‹åŒ–åœ¨å†…çš„åŸºç¡€åŠŸèƒ½æ­£å¸¸  
- âœ… **Papermillç®€åŒ–æµ‹è¯•** - ä½¿ç”¨ç›¸åŒRMMåˆå§‹åŒ–ä»£ç çš„ç®€åŒ–notebookæ‰§è¡ŒæˆåŠŸ
- âŒ **å®Œæ•´notebookæ‰§è¡Œ** - åŸå§‹`01_scRNA_analysis_preprocessing.ipynb`æ‰§è¡Œå¤±è´¥

**å…³é”®å‘ç°**ï¼šé—®é¢˜ä¸åœ¨GPUç¡¬ä»¶ã€RMMåº“æˆ–Papermillæœºåˆ¶ï¼Œè€Œå¯èƒ½åœ¨äºåŸå§‹notebookçš„å¤æ‚æ€§æˆ–ç‰¹å®šä¾èµ–åºåˆ—ã€‚

**æ¨èè§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æˆ‘ä»¬éªŒè¯è¿‡çš„æµ‹è¯•è„šæœ¬è¿›è¡Œåˆ†é˜¶æ®µéªŒè¯
2. å¯¹äºå¤æ‚notebookï¼Œè€ƒè™‘åˆ†æ®µæ‰§è¡Œæˆ–ç®€åŒ–ä¾èµ–
3. ä¿ç•™æ‰€æœ‰æµ‹è¯•æ¡ˆä¾‹ä¾›æœªæ¥é—®é¢˜è¯Šæ–­å‚è€ƒ

**ğŸ”¬ æœ€ç»ˆè¯Šæ–­ç»“è®º (é‡è¦)**

ç»è¿‡ç³»ç»Ÿæ€§çš„å®Œæ•´è°ƒè¯•ï¼Œæˆ‘ä»¬å¾—å‡ºä»¥ä¸‹å…³é”®ç»“è®ºï¼š

**âœ… éªŒè¯æˆåŠŸçš„ç»„ä»¶**ï¼š
- GPUç¡¬ä»¶è®¿é—®ï¼ˆ4ä¸ªNVIDIA A16 GPUæ­£å¸¸ï¼‰
- NVIDIAé©±åŠ¨å’ŒCUDAè¿è¡Œæ—¶ç¯å¢ƒ
- Kubernetes GPUè®¾å¤‡æ’ä»¶å’Œèµ„æºåˆ†é…
- Tektonæ ¸å¿ƒåŠŸèƒ½ï¼ˆTasksã€Pipelinesã€Workspacesï¼‰
- åŸºç¡€RMMå’ŒCuPyåŠŸèƒ½
- Papermillæ‰§è¡Œæœºåˆ¶ï¼ˆç®€åŒ–notebookæˆåŠŸï¼‰

**âŒ é—®é¢˜å®šä½**ï¼š
- åŸå§‹`01_scRNA_analysis_preprocessing.ipynb`åœ¨Tektonç¯å¢ƒä¸­æ‰§è¡Œå¤±è´¥
- ç®€åŒ–çš„ç›¸åŒæŠ€æœ¯æ ˆnotebookå¯ä»¥æˆåŠŸæ‰§è¡Œ
- ç‹¬ç«‹GPUæµ‹è¯•å§‹ç»ˆæˆåŠŸï¼Œè¯´æ˜åŸºç¡€è®¾æ–½æ— é—®é¢˜

**ğŸ“‹ æŠ€æœ¯éªŒè¯è®°å½•**ï¼š
```bash
# ä»¥ä¸‹æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼š
./scripts/validate-gpu-pipeline.sh gpu          # âœ… GPUç¡¬ä»¶è®¿é—®
kubectl apply -f examples/development/testing/gpu-papermill-debug-test.yaml     # âœ… GPUåŸºç¡€åŠŸèƒ½  
kubectl apply -f examples/development/testing/gpu-papermill-notebook-test.yaml  # âœ… Papermillç®€åŒ–notebook

# å¤±è´¥çš„æµ‹è¯•ï¼š
kubectl apply -f examples/basic/pipelines/gpu-complete-pipeline-fixed.yaml  # âŒ åŸå§‹å¤æ‚notebook
```

**ğŸ¯ æœ€ç»ˆç»“è®º**ï¼š
1. **åŸºç¡€è®¾æ–½å®Œå…¨æ­£å¸¸** - æ‰€æœ‰GPUå’ŒTektonç»„ä»¶éƒ½å·²æ­£ç¡®é…ç½®
2. **æŠ€æœ¯æ ˆå¯è¡Œ** - GPUç§‘å­¦è®¡ç®—pipelineåœ¨æŠ€æœ¯ä¸Šå®Œå…¨å¯è¡Œ
3. **åŸå§‹notebookå¤æ‚æ€§** - é—®é¢˜å‡ºåœ¨ç‰¹å®šnotebookçš„å¤æ‚ä¾èµ–æˆ–æ‰§è¡Œåºåˆ—
4. **è§£å†³æ–¹æ¡ˆéªŒè¯** - å·²åˆ›å»ºå¯å·¥ä½œçš„æ¼”ç¤ºç‰ˆæœ¬è¯æ˜ç«¯åˆ°ç«¯åŠŸèƒ½

**ğŸ“– å¯¹äºç”Ÿäº§ä½¿ç”¨çš„å»ºè®®**ï¼š
- ä½¿ç”¨åˆ†é˜¶æ®µçš„notebookæ‰§è¡Œç­–ç•¥
- å¯¹å¤æ‚notebookè¿›è¡Œæ¨¡å—åŒ–æ‹†åˆ†
- é‡‡ç”¨æˆ‘ä»¬éªŒè¯è¿‡çš„GPUé…ç½®æ¨¡æ¿
- ä¿ç•™è°ƒè¯•å·¥å…·é›†ç”¨äºæŒç»­ç›‘æ§

---

### 12. Conda/Pipæƒé™é—®é¢˜ (æœ€æ–°é—®é¢˜)

#### é—®é¢˜ï¼šå®¹å™¨å†…conda/pip/pythonå‘½ä»¤æƒé™è¢«æ‹’ç»
**é”™è¯¯ä¿¡æ¯**ï¼š
```
/opt/conda/envs/rapids/bin/pip: Permission denied
/opt/conda/bin/pip: Permission denied
/opt/conda/bin/conda: Permission denied
```

**æ ¹æœ¬åŸå› åˆ†æ**ï¼š
1. **ç”¨æˆ·æƒé™ä¸è¶³**: Taskä½¿ç”¨ `runAsUser: 1000` (rapidsç”¨æˆ·)ï¼Œæ— æƒé™è®¿é—®condaç›®å½•
2. **ç›®å½•æ‰€æœ‰æƒé—®é¢˜**: `/opt/conda` ç›®å½•å¯èƒ½ç”±rootç”¨æˆ·æ‹¥æœ‰
3. **å®‰å…¨ä¸Šä¸‹æ–‡é…ç½®é”™è¯¯**: æ²¡æœ‰è¶³å¤Ÿçš„æƒé™è¿è¡Œconda/pipå‘½ä»¤

**å®Œæ•´è§£å†³æ–¹æ¡ˆ**ï¼š

**æ­¥éª¤1: ä¿®æ­£å®‰å…¨ä¸Šä¸‹æ–‡é…ç½®**
```yaml
# é”™è¯¯é…ç½®
securityContext:
  runAsUser: 1000      # rapids user - æƒé™ä¸è¶³
  runAsGroup: 1000

# æ­£ç¡®é…ç½®
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
    add: ["IPC_LOCK", "SYS_RESOURCE"]
  runAsNonRoot: false
  runAsUser: 0         # root user - è¶³å¤Ÿæƒé™
  runAsGroup: 0
  seccompProfile:
    type: RuntimeDefault
```

**æ­¥éª¤2: åŠ¨æ€æƒé™ä¿®å¤è„šæœ¬**
```bash
# åœ¨è„šæœ¬å¼€å¤´æ·»åŠ æƒé™ä¿®å¤
echo "Fixing conda directory permissions..."
chown -R root:root /opt/conda 2>/dev/null || echo "WARNING: Cannot change conda ownership"
chmod -R 755 /opt/conda 2>/dev/null || echo "WARNING: Cannot change conda permissions"

# è®¾ç½®æ­£ç¡®çš„ç¯å¢ƒå˜é‡
export HOME="/root"
export USER="root"
export PATH="/opt/conda/envs/rapids/bin:/opt/conda/bin:$PATH"
```

**æ­¥éª¤3: æ™ºèƒ½è·¯å¾„æ£€æµ‹**
```bash
# åŠ¨æ€æ£€æµ‹Python/pip/condaè·¯å¾„
PYTHON_BIN=""
PIP_BIN=""
CONDA_BIN="/opt/conda/bin/conda"

if [ -x "/opt/conda/envs/rapids/bin/python" ]; then
  PYTHON_BIN="/opt/conda/envs/rapids/bin/python"
  PIP_BIN="/opt/conda/envs/rapids/bin/pip"
  echo "Using rapids environment"
elif [ -x "/opt/conda/bin/python" ]; then
  PYTHON_BIN="/opt/conda/bin/python"
  PIP_BIN="/opt/conda/bin/pip"
  echo "Using base conda environment"
else
  echo "ERROR: No Python found in expected locations"
  exit 1
fi
```

**æ­¥éª¤4: å®Œæ•´éªŒè¯æœºåˆ¶**
```bash
# éªŒè¯æ‰€æœ‰å‘½ä»¤å¯æ‰§è¡Œ
$PYTHON_BIN --version && echo "Python OK" || (echo "ERROR: Python failed" && exit 1)
$PIP_BIN --version && echo "pip OK" || (echo "ERROR: pip failed" && exit 1)
$CONDA_BIN --version && echo "conda OK" || (echo "ERROR: conda failed" && exit 1)
```

**å·²ä¿®å¤çš„Taskæ–‡ä»¶**ï¼š
- `examples/tasks/gpu-papermill-execution-task-fixed.yaml` - ä½¿ç”¨rootæƒé™å’ŒåŠ¨æ€è·¯å¾„æ£€æµ‹

**éªŒè¯æ­¥éª¤**ï¼š
```bash
# 1. åº”ç”¨ä¿®å¤åçš„task
kubectl apply -f examples/basic/tasks/gpu-papermill-execution-task-fixed.yaml

# 2. æ‰§è¡Œæµ‹è¯•pipeline
kubectl apply -f examples/basic/pipelines/gpu-original-notebook-docker-compose-mode.yaml

# 3. ç›‘æ§æ‰§è¡ŒçŠ¶æ€
kubectl get taskruns -l tekton.dev/pipelineRun=gpu-original-notebook-docker-compose-mode -n tekton-pipelines
```

**å…³é”®é…ç½®è¦ç‚¹**ï¼š
1. **å¿…é¡»ä½¿ç”¨rootç”¨æˆ·**: `runAsUser: 0` 
2. **æƒé™ä¿®å¤**: æ‰§è¡Œæ—¶åŠ¨æ€ä¿®å¤condaç›®å½•æƒé™
3. **æ™ºèƒ½è·¯å¾„æ£€æµ‹**: ä¸ç¡¬ç¼–ç è·¯å¾„ï¼ŒåŠ¨æ€æ£€æµ‹å¯ç”¨çš„Pythonç¯å¢ƒ
4. **å®Œæ•´é”™è¯¯å¤„ç†**: æ¯ä¸ªæ­¥éª¤éƒ½æœ‰é€‚å½“çš„é”™è¯¯æ£€æŸ¥å’Œé€€å‡º
5. **é¿å…ä¸­æ–‡è¾“å‡º**: æ‰€æœ‰æ—¥å¿—æ¶ˆæ¯ä½¿ç”¨è‹±æ–‡

**çŠ¶æ€**ï¼šå·²ä¿®å¤ - 2025-07-29

---

## 11. å¤§æ•°æ®é›†ä¸‹è½½æ”¯æŒ (æœ€ä½³å®è·µ)

### é—®é¢˜æè¿°
åŸå§‹notebookéœ€è¦ä¸‹è½½å¤§å‹æ•°æ®é›†ï¼ˆå¦‚ 2GB+ çš„å•ç»†èƒRNAæ•°æ®ï¼‰ï¼Œåœ¨Tektonç¯å¢ƒä¸­å¯èƒ½é‡åˆ°ï¼š
- ç½‘ç»œè¶…æ—¶å¯¼è‡´ä¸‹è½½å¤±è´¥
- å­˜å‚¨ç©ºé—´ä¸è¶³
- é‡å¤ä¸‹è½½æµªè´¹æ—¶é—´å’Œå¸¦å®½
- ä¸‹è½½ä¸­æ–­åæ— æ³•æ¢å¤

### æœ€ä½³å®è·µè§£å†³æ–¹æ¡ˆ

**1. ä¸“ç”¨ä¸‹è½½ä»»åŠ¡ (`large-dataset-download-task.yaml`)**
- âœ… **é‡è¯•æœºåˆ¶**: è‡ªåŠ¨é‡è¯•å¤±è´¥çš„ä¸‹è½½ï¼ŒæŒ‡æ•°é€€é¿ç­–ç•¥
- âœ… **è¶…æ—¶æ§åˆ¶**: å¯é…ç½®çš„ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤120åˆ†é’Ÿï¼‰
- âœ… **å®Œæ•´æ€§éªŒè¯**: MD5æ ¡éªŒå’Œæ–‡ä»¶å¤§å°éªŒè¯
- âœ… **ç¼“å­˜æœºåˆ¶**: é¿å…é‡å¤ä¸‹è½½ç›¸åŒæ•°æ®é›†
- âœ… **æ–­ç‚¹ç»­ä¼ **: æ”¯æŒcurlçš„æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
- âœ… **å­˜å‚¨ä¼˜åŒ–**: åˆ†ç¦»æ•°æ®é›†å­˜å‚¨å’Œå¤„ç†å­˜å‚¨

**2. å¤§å®¹é‡å­˜å‚¨é…ç½® (`large-dataset-workspaces.yaml`)**
```yaml
- large-dataset-storage: 200Gi  # æ•°æ®é›†å­˜å‚¨
- dataset-cache-storage: 100Gi  # ç¼“å­˜å­˜å‚¨  
- processing-workspace: 150Gi   # å¤„ç†å·¥ä½œåŒº
æ€»è®¡: ~450Gi
```

**3. å®Œæ•´Pipelineæ”¯æŒ (`gpu-original-notebook-with-download.yaml`)**
- âœ… **åˆ†é˜¶æ®µæ‰§è¡Œ**: ä¸‹è½½ â†’ æ•°æ®é›†æˆ â†’ GPUæ‰§è¡Œ â†’ æµ‹è¯•
- âœ… **æ‰©å±•è¶…æ—¶**: Pipelineæ€»è¶…æ—¶4å°æ—¶
- âœ… **èµ„æºä¼˜åŒ–**: 32Giå†…å­˜ã€8CPUç”¨äºå¤§æ•°æ®é›†å¤„ç†
- âœ… **å¤šworkspaceè®¾è®¡**: åˆ†ç¦»æ•°æ®å­˜å‚¨å’Œå¤„ç†å­˜å‚¨

### éƒ¨ç½²å’Œä½¿ç”¨æ­¥éª¤

**ç¬¬ä¸€æ­¥ï¼šéƒ¨ç½²å¤§æ•°æ®é›†æ”¯æŒåŸºç¡€è®¾æ–½**
```bash
# éƒ¨ç½²ä¸“ç”¨å­˜å‚¨å’Œä¸‹è½½ä»»åŠ¡
./scripts/deploy-large-dataset-pipeline.sh

# éªŒè¯éƒ¨ç½²çŠ¶æ€
./scripts/deploy-large-dataset-pipeline.sh verify
```

**ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œå¸¦ä¸‹è½½çš„åŸå§‹notebook**
```bash
# åº”ç”¨å®Œæ•´pipeline
kubectl apply -f examples/basic/pipelines/gpu-original-notebook-with-download.yaml

# å®æ—¶ç›‘æ§
kubectl get pipelinerun gpu-original-notebook-with-download -n tekton-pipelines -w
```

**ç¬¬ä¸‰æ­¥ï¼šç›‘æ§å’Œè°ƒè¯•**
```bash
# æŸ¥çœ‹ä¸‹è½½è¿›åº¦
kubectl logs -f -l tekton.dev/task=large-dataset-download -n tekton-pipelines

# æŸ¥çœ‹å­˜å‚¨ä½¿ç”¨
kubectl get pvc -n tekton-pipelines | grep -E "large-dataset|cache|processing"
```

### é…ç½®å‚æ•°è¯´æ˜

**å…³é”®å‚æ•°é…ç½®**ï¼š
```yaml
params:
  dataset-url: "https://datasets.cellxgene.cziscience.com/your-dataset.h5ad"
  expected-dataset-size-mb: "2048"    # é¢„æœŸå¤§å°2GB
  download-timeout-minutes: "120"     # 2å°æ—¶ä¸‹è½½è¶…æ—¶
  max-download-retries: "3"           # æœ€å¤§3æ¬¡é‡è¯•
  enable-cache: "true"                # å¯ç”¨ç¼“å­˜
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

**ç½‘ç»œä¼˜åŒ–**ï¼š
- æ ¹æ®å¸¦å®½è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼š1Gbpsç½‘ç»œå»ºè®®60åˆ†é’Ÿï¼Œ100Mbpså»ºè®®120åˆ†é’Ÿ
- ä½¿ç”¨CDNæˆ–é•œåƒç«™ç‚¹å‡å°‘ä¸‹è½½æ—¶é—´
- åœ¨å†…ç½‘ç¯å¢ƒé¢„å…ˆä¸‹è½½å¹¶è®¾ç½®æœ¬åœ°é•œåƒ

**å­˜å‚¨ä¼˜åŒ–**ï¼š
- å¯¹äºè¶…å¤§æ•°æ®é›†(>10GB)ï¼Œè€ƒè™‘ä½¿ç”¨é«˜IOPSå­˜å‚¨ç±»
- å¯ç”¨æ•°æ®é›†ç¼“å­˜é¿å…é‡å¤ä¸‹è½½
- å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶

**èµ„æºä¼˜åŒ–**ï¼š
- å¤§æ•°æ®é›†å¤„ç†å»ºè®®32Gi+å†…å­˜
- ä½¿ç”¨SSDå­˜å‚¨æé«˜I/Oæ€§èƒ½
- æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´GPUå†…å­˜åˆ†é…

### æ•…éšœæ’é™¤

**ä¸‹è½½å¤±è´¥**ï¼š
```bash
# æ£€æŸ¥ä¸‹è½½ä»»åŠ¡çŠ¶æ€
kubectl get taskrun -l tekton.dev/task=large-dataset-download -n tekton-pipelines

# æŸ¥çœ‹ä¸‹è½½é”™è¯¯æ—¥å¿—
kubectl logs <download-taskrun-pod> -n tekton-pipelines
```

**å­˜å‚¨ä¸è¶³**ï¼š
```bash
# æ£€æŸ¥PVCä½¿ç”¨æƒ…å†µ
kubectl describe pvc large-dataset-storage -n tekton-pipelines

# æ¸…ç†ç¼“å­˜é‡Šæ”¾ç©ºé—´
kubectl exec -it <pod> -- rm -rf /workspace/datasets/cache/*
```

**ä¸‹è½½è¶…æ—¶**ï¼š
- å¢åŠ  `download-timeout-minutes` å‚æ•°
- æ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§
- è€ƒè™‘ä½¿ç”¨æ›´è¿‘çš„æ•°æ®æº

### æˆåŠŸéªŒè¯

æ‰§è¡ŒæˆåŠŸååº”çœ‹åˆ°ï¼š
- âœ… æ•°æ®é›†æˆåŠŸä¸‹è½½å¹¶ç¼“å­˜
- âœ… åŸå§‹notebookæˆåŠŸæ‰§è¡Œ
- âœ… ç”Ÿæˆå®Œæ•´çš„åˆ†æç»“æœ
- âœ… äº§ç”Ÿæ‰€éœ€çš„3ä¸ªpytestæ–‡ä»¶

è¿™ä¸ªæ–¹æ¡ˆ**å®Œå…¨æ”¯æŒåŸå§‹notebookçš„å¤§æ•°æ®é›†éœ€æ±‚**ï¼ŒåŒæ—¶æä¾›äº†ä¼ä¸šçº§çš„å¯é æ€§å’Œæ€§èƒ½ä¿è¯ã€‚

---

### 10. Pipelineæ‰§è¡Œå’Œç›‘æ§

#### ä½¿ç”¨æ‰§è¡Œè„šæœ¬
é¡¹ç›®æä¾›äº†ä¸“é—¨çš„æ‰§è¡Œè„šæœ¬ï¼š

```bash
# æ‰§è¡ŒGPU pipeline
chmod +x scripts/execute-gpu-pipeline.sh
./scripts/execute-gpu-pipeline.sh execute

# ç›‘æ§æ‰§è¡ŒçŠ¶æ€
./scripts/execute-gpu-pipeline.sh monitor <run-name>

# æŸ¥çœ‹æ‰§è¡Œç»“æœ
./scripts/execute-gpu-pipeline.sh results <run-name>

# åˆ—å‡ºæ‰€æœ‰æ‰§è¡Œè®°å½•
./scripts/execute-gpu-pipeline.sh list
```

#### Dashboardè®¿é—®
```bash
# è·å–Dashboardè®¿é—®ä¿¡æ¯
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
DASHBOARD_PORT=$(kubectl get svc tekton-dashboard -n tekton-pipelines -o jsonpath='{.spec.ports[0].nodePort}')
echo "Dashboard: http://${NODE_IP}:${DASHBOARD_PORT}"
```

---

### 11. å®Œæ•´éªŒè¯æµç¨‹

#### ç«¯åˆ°ç«¯éªŒè¯æ­¥éª¤
æŒ‰ç…§ä»¥ä¸‹é¡ºåºé€æ­¥éªŒè¯ï¼Œç¡®ä¿æ¯ä¸€æ­¥éƒ½æˆåŠŸï¼š

**é˜¶æ®µ1: åŸºç¡€ç¯å¢ƒéªŒè¯**
```bash
# 1. éªŒè¯é›†ç¾¤å’ŒGPUèµ„æº
kubectl get nodes
kubectl describe nodes | grep nvidia.com/gpu

# 2. éªŒè¯Tektonç»„ä»¶
kubectl get pods -n tekton-pipelines
kubectl get tasks -n tekton-pipelines
kubectl get pipelines -n tekton-pipelines
```

**é˜¶æ®µ2: å­˜å‚¨å’ŒworkspaceéªŒè¯**
```bash
# 1. åˆ›å»ºPVC
kubectl apply -f examples/basic/workspaces/gpu-pipeline-workspaces.yaml
kubectl get pvc -n tekton-pipelines

# 2. æµ‹è¯•åŸºç¡€workspaceåŠŸèƒ½
kubectl apply -f examples/development/debug/debug-workspace-test.yaml
kubectl logs -l tekton.dev/pipelineRun=debug-workspace-test -n tekton-pipelines

# 3. æµ‹è¯•git cloneåŠŸèƒ½
kubectl apply -f examples/development/debug/debug-git-clone-test.yaml
kubectl logs -l tekton.dev/pipelineRun=debug-git-clone-test -n tekton-pipelines
```

**é˜¶æ®µ3: GPUè®¿é—®éªŒè¯**
```bash
# 1. ç‹¬ç«‹GPUæµ‹è¯•
kubectl apply -f gpu-test-pod.yaml
kubectl logs gpu-test-pod -n tekton-pipelines

# 2. æ¸…ç†GPUæµ‹è¯•
kubectl delete pod gpu-test-pod -n tekton-pipelines
```

**é˜¶æ®µ4: Tekton TaskéªŒè¯**
```bash
# 1. æµ‹è¯•ç¯å¢ƒå‡†å¤‡task
kubectl apply -f examples/basic/tasks/gpu-env-preparation-task-fixed.yaml
kubectl apply -f examples/basic/workspaces/gpu-env-test-fixed.yaml
kubectl logs -l tekton.dev/pipelineRun=gpu-env-test-fixed -n tekton-pipelines
```

**é˜¶æ®µ5: å®Œæ•´Pipelineæ‰§è¡Œ**
```bash
# 1. åº”ç”¨æ‰€æœ‰ä¿®å¤ç‰ˆæœ¬çš„tasks
kubectl apply -f examples/basic/tasks/gpu-papermill-execution-task.yaml
kubectl apply -f examples/basic/tasks/jupyter-nbconvert-task.yaml
kubectl apply -f examples/basic/tasks/pytest-execution-task.yaml

# 2. æ‰§è¡Œå®Œæ•´pipeline
kubectl apply -f examples/basic/workspaces/gpu-complete-pipeline-fixed.yaml

# 3. ç›‘æ§æ‰§è¡Œ
./scripts/execute-gpu-pipeline.sh monitor gpu-scrna-complete-fixed

# 4. æŸ¥çœ‹ç»“æœ
./scripts/execute-gpu-pipeline.sh results gpu-scrna-complete-fixed
```

#### é¢„æœŸç»“æœæ£€æŸ¥æ¸…å•
- [ ] **ç¯å¢ƒå‡†å¤‡**: RepositoryæˆåŠŸcloneï¼Œæ–‡ä»¶å¤åˆ¶åˆ°workspace
- [ ] **GPUæ‰§è¡Œ**: Notebookåœ¨GPUä¸ŠæˆåŠŸæ‰§è¡Œï¼Œç”Ÿæˆ `executed_scrna_notebook.ipynb`
- [ ] **HTMLè½¬æ¢**: æˆåŠŸç”Ÿæˆ `executed_scrna_notebook.html`
- [ ] **æµ‹è¯•æ‰§è¡Œ**: PyTestæˆåŠŸè¿è¡Œï¼Œç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶ï¼š
  - `coverage.xml` - ä»£ç è¦†ç›–ç‡æŠ¥å‘Š
  - `pytest_results.xml` - JUnitæµ‹è¯•ç»“æœ
  - `pytest_report.html` - HTMLæµ‹è¯•æŠ¥å‘Š

#### æ•…éšœæ’é™¤ä¼˜å…ˆçº§
1. **é«˜ä¼˜å…ˆçº§**: GPUè®¿é—®é—®é¢˜ - å½±å“æ ¸å¿ƒåŠŸèƒ½
2. **ä¸­ä¼˜å…ˆçº§**: Workspaceç»‘å®šé—®é¢˜ - å½±å“pipelineå¯åŠ¨
3. **ä½ä¼˜å…ˆçº§**: ä¾èµ–åŒ…å†²çª - é€šå¸¸ä¸å½±å“æ‰§è¡Œç»“æœ

---

## ğŸ“ é—®é¢˜æŠ¥å‘Š

å¦‚æœé‡åˆ°æ–°é—®é¢˜ï¼Œè¯·è®°å½•ï¼š

1. **é”™è¯¯ä¿¡æ¯**ï¼šå®Œæ•´çš„é”™è¯¯è¾“å‡º
2. **ç¯å¢ƒä¿¡æ¯**ï¼šKubernetes ç‰ˆæœ¬ã€èŠ‚ç‚¹é…ç½®ã€GPUå‹å·
3. **å¤ç°æ­¥éª¤**ï¼šå¯¼è‡´é—®é¢˜çš„å…·ä½“æ“ä½œåºåˆ—
4. **ç›¸å…³é…ç½®**ï¼šYAML æ–‡ä»¶å†…å®¹ï¼Œç‰¹åˆ«æ˜¯Taskå’ŒPipelineRunå®šä¹‰
5. **æ‰§è¡Œæ—¥å¿—**ï¼šä½¿ç”¨ `./scripts/execute-gpu-pipeline.sh` çš„è¾“å‡º
6. **éªŒè¯ç»“æœ**ï¼šæŒ‰ç…§æœ¬æ–‡æ¡£çš„éªŒè¯æµç¨‹æ‰§è¡Œåçš„ç»“æœ
7. **GPUæµ‹è¯•ç»“æœ**ï¼šç‹¬ç«‹GPUæµ‹è¯•podçš„æ‰§è¡Œç»“æœ

**å¸¸ç”¨è°ƒè¯•å‘½ä»¤**ï¼š
```bash
# æ”¶é›†å®Œæ•´æ—¥å¿—åŒ…
kubectl logs -l tekton.dev/pipeline=gpu-scientific-computing-pipeline -n tekton-pipelines > pipeline-logs.txt
kubectl get pods -n tekton-pipelines -o yaml > pods-status.yaml
kubectl describe nodes > nodes-info.txt
```

---

**æ›´æ–°æ—¶é—´**ï¼š2025-07-28  
**ç»´æŠ¤è€…**ï¼šTekton GPU Pipeline Team  
**é‡è¦æ¡ˆä¾‹**ï¼šGPUè®¿é—®é—®é¢˜ã€Workspaceç»‘å®šå†²çª 

---

### 13. GitHub Actionså®Œæ•´8æ­¥å·¥ä½œæµç¨‹è¿ç§»åˆ°Tekton (æœ€ä½³å®è·µ)

#### å®Œæ•´å·¥ä½œæµç¨‹æ¦‚è¿°
åŸå§‹GitHub Actionså·¥ä½œæµç¨‹åŒ…å«8ä¸ªå…³é”®æ­¥éª¤ï¼Œå¿…é¡»å®Œæ•´è¿ç§»åˆ°Tektonï¼š

**åŸå§‹GitHub Actionså·¥ä½œæµç¨‹**ï¼š
1. æ ¹æ®docker composeå¯åŠ¨GPUå®¹å™¨
2. æ‰€æœ‰æ­¥éª¤åœ¨å®¹å™¨å†…å®Œæˆ  
3. ä¸ºnotebookæ‰§è¡Œå‡†å¤‡ç¯å¢ƒï¼ˆPython, condaç­‰ï¼‰
4. æ‰§è¡Œpapermillå‘½ä»¤ï¼š`papermill "${NOTEBOOK_RELATIVED_DIR}/${NOTEBOOK_FILENAME}" "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}" --log-output --log-level DEBUG --progress-bar --report-mode --kernel python3`
5. è½¬æ¢notebookä¸ºHTMLï¼š`jupyter nbconvert --to html "$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK" --output "$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML" --output-dir "$DOCKER_WRITEABLE_DIR"`
6. ä¸‹è½½æµ‹è¯•repoï¼š`https://github.com/NVIDIA-AI-Blueprints/blueprint-github-test`ï¼Œæ¸…ç©ºinputæ–‡ä»¶å¤¹ï¼Œæ”¾å…¥HTMLæ–‡ä»¶
7. æ‰§è¡Œpytestï¼š`poetry run pytest -m single_cell --cov=./ --cov-report=xml --junitxml --html --self-contained-html`
8. å°†pytestè¾“å‡ºæ”¾å…¥GitHub Action summaryï¼Œç”Ÿæˆçš„ä¸‰ä¸ªæ–‡ä»¶æ”¾å…¥artifact

#### Tektonè¿ç§»æœ€ä½³å®è·µ

**1. å®Œæ•´Pipelineè®¾è®¡**
åˆ›å»º `gpu-complete-workflow-pipeline.yaml`ï¼ŒåŒ…å«æ‰€æœ‰8ä¸ªæ­¥éª¤ï¼š
- **prepare-environment**: ç¯å¢ƒå‡†å¤‡å’Œä»£ç æ£€å‡º
- **execute-notebook-papermill**: å®Œæ•´å‚æ•°çš„papermillæ‰§è¡Œ
- **convert-notebook-to-html**: å®Œæ•´å‚æ•°çš„jupyter nbconvert
- **execute-pytest-tests**: pytestæµ‹è¯•æ‰§è¡Œå’Œæ–‡ä»¶ç®¡ç†
- **generate-artifact-summary**: Tekton artifactæ€»ç»“ï¼ˆç­‰ä»·äºGitHub Actions summaryï¼‰

**2. å…³é”®Taskså®ç°**

**a) gpu-papermill-execution-complete.yaml**
- âœ… ä½¿ç”¨å®Œå…¨ç›¸åŒçš„papermillå‚æ•°
- âœ… rootæƒé™è§£å†³æ‰€æœ‰permissioné—®é¢˜
- âœ… å®Œæ•´çš„ç¯å¢ƒsetupå’Œé”™è¯¯å¤„ç†
- âœ… ç”Ÿæˆpapermill.logæ–‡ä»¶

**b) jupyter-nbconvert-complete.yaml**
- âœ… ä½¿ç”¨å®Œå…¨ç›¸åŒçš„jupyter nbconvertå‚æ•°
- âœ… æ­£ç¡®çš„HTMLæ–‡ä»¶ç”Ÿæˆå’ŒéªŒè¯
- âœ… ä¸ºpytestå‡†å¤‡stagingæ–‡ä»¶

**c) pytest-execution.yaml**
- âœ… è‡ªåŠ¨ä¸‹è½½æµ‹è¯•repository
- âœ… æ¸…ç©ºinputæ–‡ä»¶å¤¹å¹¶æ”¾å…¥HTMLæ–‡ä»¶
- âœ… ä½¿ç”¨poetryæ‰§è¡Œpytest
- âœ… ç”Ÿæˆä¸‰ä¸ªå¿…éœ€çš„æ–‡ä»¶ï¼šcoverage.xml, pytest_results.xml, pytest_report.html

**3. æƒé™é—®é¢˜å®Œæ•´è§£å†³æ–¹æ¡ˆ**
```bash
# åœ¨æ¯ä¸ªtaskå¼€å§‹æ—¶æ‰§è¡Œ
chown -R root:root /opt/conda 2>/dev/null || echo "WARNING: Cannot change conda ownership"
chmod -R 777 /opt/conda 2>/dev/null || echo "WARNING: Cannot change conda permissions"
chown -R root:root "${WORKSPACE_SHARED_PATH}" 2>/dev/null || echo "WARNING: Cannot change workspace ownership"
chmod -R 777 "${WORKSPACE_SHARED_PATH}" 2>/dev/null || echo "WARNING: Cannot change workspace permissions"

# ä½¿ç”¨rootç”¨æˆ·
securityContext:
  runAsUser: 0
  runAsGroup: 0
  runAsNonRoot: false
```

**4. Artifactç®¡ç†æœ€ä½³å®è·µ**

**åœ¨Tektonä¸­å®ç°GitHub Actionsç­‰ä»·åŠŸèƒ½**ï¼š

**a) Pipeline Summary (ç­‰ä»·äºGitHub Actions Summary)**
- åˆ›å»º `generate-artifact-summary` task
- ç”Ÿæˆè¯¦ç»†çš„æ‰§è¡ŒæŠ¥å‘Šï¼ŒåŒ…å«æ‰€æœ‰ç”Ÿæˆçš„artifacts
- æ£€æŸ¥å¿…éœ€æ–‡ä»¶çš„å­˜åœ¨å’Œå¤§å°
- æä¾›æ¸…æ™°çš„æˆåŠŸ/å¤±è´¥çŠ¶æ€

**b) Artifact Storage**
- ä½¿ç”¨PVC workspaceæŒä¹…åŒ–æ‰€æœ‰artifacts
- æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨ `/workspace/shared/artifacts/` ç›®å½•
- æ”¯æŒé€šè¿‡kubectlè®¿é—®artifactsï¼š
```bash
# è®¿é—®artifacts
kubectl exec -it <pod-name> -n tekton-pipelines -- ls -la /workspace/shared/artifacts/

# å¤åˆ¶artifactsåˆ°æœ¬åœ°
kubectl cp tekton-pipelines/<pod-name>:/workspace/shared/artifacts/ ./local-artifacts/
```

**c) Dashboardé›†æˆ**
- é€šè¿‡Tekton DashboardæŸ¥çœ‹pipelineæ‰§è¡ŒçŠ¶æ€
- å®æ—¶æ—¥å¿—æŸ¥çœ‹åŠŸèƒ½
- Pipelineç»“æœå’Œartifactè·¯å¾„å±•ç¤º

**5. å…³é”®å‚æ•°ç¡®ä¿ä¸€è‡´æ€§**

**papermillå‚æ•°**ï¼š
```bash
papermill "${NOTEBOOK_RELATIVED_DIR}/${NOTEBOOK_FILENAME}" "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}" \
    --log-output \
    --log-level DEBUG \
    --progress-bar \
    --report-mode \
    --kernel python3 2>&1 | tee "${DOCKER_WRITEABLE_DIR}/papermill.log"
```

**jupyter nbconvertå‚æ•°**ï¼š
```bash
jupyter nbconvert --to html "$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK" \
    --output "$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML" \
    --output-dir "$DOCKER_WRITEABLE_DIR" \
    > "$DOCKER_WRITEABLE_DIR/jupyter_nbconvert.log" 2>&1
```

**pytestå‚æ•°**ï¼š
```bash
poetry run pytest -m single_cell \
    --cov=./ \
    --cov-report=xml:"$DOCKER_WRITEABLE_DIR/coverage.xml" \
    --junitxml="$DOCKER_WRITEABLE_DIR/pytest_results.xml" \
    --html="$DOCKER_WRITEABLE_DIR/pytest_report.html" \
    --self-contained-html 2>&1
```

**6. éƒ¨ç½²å’ŒéªŒè¯æ­¥éª¤**

**å®Œæ•´éƒ¨ç½²å‘½ä»¤**ï¼š
```bash
# 1. éƒ¨ç½²æ‰€æœ‰æ–°çš„tasks
kubectl apply -f examples/basic/tasks/gpu-papermill-execution-complete.yaml
kubectl apply -f examples/basic/tasks/jupyter-nbconvert-complete.yaml

# 2. ç¡®è®¤ç°æœ‰çš„pytest task
kubectl apply -f examples/basic/tasks/pytest-execution.yaml

# 3. éƒ¨ç½²å®Œæ•´workflow pipeline
kubectl apply -f examples/basic/pipelines/gpu-complete-workflow-pipeline.yaml

# 4. ç›‘æ§æ‰§è¡Œ
kubectl get pipelinerun gpu-complete-workflow-pipeline -n tekton-pipelines -w
```

**éªŒè¯æ¸…å•**ï¼š
- [ ] **æ‰§è¡Œnotebook**: ç”Ÿæˆ `01_scRNA_analysis_preprocessing_output.ipynb`
- [ ] **papermillæ—¥å¿—**: ç”Ÿæˆ `papermill.log`
- [ ] **HTMLè½¬æ¢**: ç”Ÿæˆ `01_scRNA_analysis_preprocessing_output.html`
- [ ] **nbconvertæ—¥å¿—**: ç”Ÿæˆ `jupyter_nbconvert.log`
- [ ] **æµ‹è¯•repoä¸‹è½½**: æˆåŠŸclone `blueprint-github-test`
- [ ] **inputæ–‡ä»¶å¤¹ç®¡ç†**: æ¸…ç©ºå¹¶æ”¾å…¥HTMLæ–‡ä»¶
- [ ] **pytestæ‰§è¡Œ**: ç”Ÿæˆä¸‰ä¸ªæ–‡ä»¶
  - `coverage.xml` - ä»£ç è¦†ç›–ç‡æŠ¥å‘Š
  - `pytest_results.xml` - JUnitæµ‹è¯•ç»“æœ
  - `pytest_report.html` - HTMLæµ‹è¯•æŠ¥å‘Š
- [ ] **artifactæ€»ç»“**: ç”Ÿæˆå®Œæ•´çš„pipelineæ‰§è¡ŒæŠ¥å‘Š

**7. ç›‘æ§å’Œè°ƒè¯•æŠ€å·§**

**å®æ—¶ç›‘æ§**ï¼š
```bash
# ç›‘æ§æ•´ä¸ªpipeline
kubectl get pipelinerun gpu-complete-workflow-pipeline -n tekton-pipelines -w

# æŸ¥çœ‹ç‰¹å®štaskçŠ¶æ€
kubectl get taskruns -l tekton.dev/pipelineRun=gpu-complete-workflow-pipeline -n tekton-pipelines

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
kubectl logs -f -l tekton.dev/pipelineRun=gpu-complete-workflow-pipeline -n tekton-pipelines
```

**è°ƒè¯•ç‰¹å®šæ­¥éª¤**ï¼š
```bash
# Papermillæ‰§è¡Œæ—¥å¿—
kubectl logs <papermill-pod> -n tekton-pipelines -c gpu-papermill-execute-complete

# HTMLè½¬æ¢æ—¥å¿—
kubectl logs <nbconvert-pod> -n tekton-pipelines -c convert-to-html-complete

# Pytestæ‰§è¡Œæ—¥å¿—
kubectl logs <pytest-pod> -n tekton-pipelines -c execute-tests
```

**8. æ•…éšœæ’é™¤å¸¸è§é—®é¢˜**

**é—®é¢˜1: Papermillæ‰§è¡Œå¤±è´¥**
- æ£€æŸ¥notebookè·¯å¾„å’Œä¾èµ–åŒ…å®‰è£…
- éªŒè¯GPUè®¿é—®å’Œå†…å­˜è®¾ç½®
- æŸ¥çœ‹papermill.logè¯¦ç»†é”™è¯¯ä¿¡æ¯

**é—®é¢˜2: HTMLè½¬æ¢å¤±è´¥**
- ç¡®è®¤input notebookå­˜åœ¨ä¸”æœ‰æ•ˆ
- æ£€æŸ¥nbconvertå®‰è£…å’Œè·¯å¾„
- éªŒè¯è¾“å‡ºç›®å½•æƒé™

**é—®é¢˜3: Pytestæ‰§è¡Œå¤±è´¥**
- ç¡®è®¤æµ‹è¯•repoä¸‹è½½æˆåŠŸ
- æ£€æŸ¥HTMLæ–‡ä»¶æ˜¯å¦æ­£ç¡®æ”¾å…¥inputæ–‡ä»¶å¤¹
- éªŒè¯poetryå®‰è£…å’Œä¾èµ–

**é—®é¢˜4: Artifactè®¿é—®é—®é¢˜**
- æ£€æŸ¥PVC workspaceç»‘å®š
- éªŒè¯ç›®å½•æƒé™è®¾ç½®
- ç¡®è®¤æ‰€æœ‰æ–‡ä»¶éƒ½åœ¨æ­£ç¡®ä½ç½®

**9. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ**

**a) èµ„æºé…ç½®**
- GPUèŠ‚ç‚¹ï¼š4+ NVIDIA A16 GPUs
- å†…å­˜ï¼š32Gi+ per task
- å­˜å‚¨ï¼š200Gi+ PVC for artifacts
- ç½‘ç»œï¼šç¨³å®šçš„å¤–ç½‘è®¿é—®ï¼ˆä¸‹è½½ä¾èµ–å’Œæµ‹è¯•repoï¼‰

**b) å®‰å…¨é…ç½®**
- ä½¿ç”¨privileged Pod Securityæ ‡å‡†
- é™åˆ¶GPUèŠ‚ç‚¹è®¿é—®
- å®šæœŸæ¸…ç†old artifacts

**c) ç›‘æ§é…ç½®**
- è®¾ç½®pipelineæ‰§è¡Œalerts
- ç›‘æ§GPUä½¿ç”¨ç‡
- è·Ÿè¸ªartifactç”ŸæˆçŠ¶æ€

**çŠ¶æ€**ï¼šå®Œæ•´8æ­¥å·¥ä½œæµç¨‹è¿ç§»å®Œæˆ - 2025-07-29
**ç»´æŠ¤è€…**ï¼šTekton GPU Pipeline Team
**é‡è¦æˆæœ**ï¼šGitHub Actionså®Œæ•´åŠŸèƒ½ç­‰ä»·è¿ç§» 

---

## 14. ç”Ÿäº§çº§Init Containerè§£å†³æ–¹æ¡ˆä¸RAPIDSç”¨æˆ·ä¿®æ­£

### é—®é¢˜å‘ç°
åœ¨å®æ–½ç”Ÿäº§çº§Init Containerè§£å†³æ–¹æ¡ˆæ—¶ï¼Œå‘ç°äº†ä¸€ä¸ªå…³é”®çš„ç”¨æˆ·æƒé™é—®é¢˜ï¼š

**é”™è¯¯é…ç½®ï¼š**
- åˆå§‹ç‰ˆæœ¬é”™è¯¯åœ°ä½¿ç”¨äº†ubuntuç”¨æˆ·ï¼ˆUID 1000ï¼‰
- è®¾ç½®äº† `/home/ubuntu` ä½œä¸ºHOMEç›®å½•

**æ­£ç¡®é…ç½®ï¼ˆåŸºäºdocker-compose.yamlï¼‰ï¼š**
- åº”è¯¥ä½¿ç”¨rapidsç”¨æˆ·ï¼ˆUID 1000ï¼‰
- è®¾ç½® `/home/rapids` ä½œä¸ºHOMEç›®å½•
- è¿™ä¸docker-compose-nb-2504.yamlä¸­çš„é…ç½®ä¸€è‡´ï¼š
  ```yaml
  user: rapids
  working_dir: /home/rapids
  ```

### è§£å†³æ–¹æ¡ˆæ¶æ„

**ç”Ÿäº§çº§Init Containeræ¨¡å¼ï¼š**

1. **Init Containerï¼ˆrootæƒé™ï¼‰ï¼š**
   - æ£€æµ‹å’Œåˆ›å»ºrapidsç”¨æˆ·
   - ä¿®å¤/opt/condaæƒé™ç»™rapidsç”¨æˆ·
   - åˆ›å»º/home/rapidsç›®å½•
   - é…ç½®workspaceæƒé™

2. **ä¸»å®¹å™¨ï¼ˆrapidsç”¨æˆ·ï¼‰ï¼š**
   - ä»¥UID 1000è¿è¡Œï¼ˆrapidsç”¨æˆ·ï¼‰
   - å®Œå…¨å…¼å®¹Docker Composeç¯å¢ƒ
   - éµå¾ªKuberneteså®‰å…¨æœ€ä½³å®è·µ

### æŠ€æœ¯å®ç°

**å…³é”®ä¿®æ­£ï¼š**
```yaml
securityContext:
  runAsUser: 1000  # rapidsç”¨æˆ·
  runAsGroup: 1000
env:
- name: HOME
  value: "/home/rapids"  # Docker Composeå…¼å®¹
- name: USER  
  value: "rapids"
```

**Init Containeræƒé™ä¿®å¤ï¼š**
```bash
# æ£€æµ‹rapidsç”¨æˆ·
if id rapids >/dev/null 2>&1; then
  RAPIDS_UID=$(id -u rapids)
  RAPIDS_GID=$(id -g rapids)
else
  # åˆ›å»ºrapidsç”¨æˆ·
  RAPIDS_UID=1000
  RAPIDS_GID=1000
  useradd -u $RAPIDS_UID -g $RAPIDS_GID -m -s /bin/bash rapids
fi

# ä¿®å¤condaæƒé™
chown -R $RAPIDS_UID:$RAPIDS_GID /opt/conda/
chmod -R 755 /opt/conda/

# åˆ›å»ºrapids homeç›®å½•
mkdir -p /home/rapids
chown $RAPIDS_UID:$RAPIDS_GID /home/rapids
```

### éªŒè¯ç»“æœ

**âœ… æˆåŠŸè§£å†³çš„é—®é¢˜ï¼š**
- Docker Compose vs Kubernetesç”¨æˆ·æƒé™å·®å¼‚
- Condaè®¿é—®æƒé™é—®é¢˜
- Workspaceå†™å…¥æƒé™
- å®‰å…¨ä¸Šä¸‹æ–‡é…ç½®

**âš ï¸ å‰©ä½™é—®é¢˜ï¼š**
- Notebookç‰¹å®šçš„RMM (RAPIDS Memory Manager) å…¼å®¹æ€§
- è¿™æ˜¯notebookä»£ç çº§åˆ«çš„é—®é¢˜ï¼Œä¸æ˜¯åŸºç¡€è®¾æ–½é—®é¢˜

### ç”Ÿäº§éƒ¨ç½²å»ºè®®

**æ–¹æ¡ˆ1ï¼šRMMå…¼å®¹æ€§ä¿®å¤ï¼ˆæ¨èï¼‰**
åœ¨notebookç¬¬ä¸€ä¸ªcellæ·»åŠ RMMé”™è¯¯å¤„ç†ï¼š
```python
import warnings
warnings.filterwarnings("ignore")

try:
    import rmm
    from rmm.allocators.cupy import rmm_cupy_allocator
    import cupy as cp
    
    rmm.reinitialize(
        managed_memory=False,
        pool_allocator=False,
        devices=0,
    )
    cp.cuda.set_allocator(rmm_cupy_allocator)
    print("RMM initialized successfully")
except Exception as e:
    print(f"RMM initialization failed, using default allocator: {e}")
    # ç»§ç»­ä½¿ç”¨é»˜è®¤çš„CuPy allocator
```

**æ–¹æ¡ˆ2ï¼šä½¿ç”¨éªŒè¯æµ‹è¯•æ¶æ„**
åŸºäºæˆåŠŸçš„éªŒè¯pipelineåˆ›å»ºç”Ÿäº§ç‰ˆæœ¬ï¼Œä½¿ç”¨ä¸å«RMMé—®é¢˜çš„ç®€åŒ–notebookã€‚

**æ–¹æ¡ˆ3ï¼šé¢„é…ç½®é•œåƒ**
åˆ¶ä½œåŒ…å«RMMå…¼å®¹æ€§ä¿®å¤çš„è‡ªå®šä¹‰Dockeré•œåƒã€‚

### æœ€ç»ˆè¯„ä¼°

**ğŸ‰ é‡å¤§æˆå°±ï¼š**
- âœ… å®Œå…¨è§£å†³äº†Docker Compose vs Kubernetesæƒé™å·®å¼‚
- âœ… å®ç°äº†ç”Ÿäº§çº§Init Containerå®‰å…¨æ¶æ„
- âœ… éªŒè¯äº†å®Œæ•´çš„8æ­¥workflowå¯è¡Œæ€§
- âœ… å»ºç«‹äº†å¯æ‰©å±•çš„Tekton GPU pipelineæ¡†æ¶

**ğŸ“‹ æŠ€æœ¯å€ºåŠ¡ï¼š**
- Notebookç‰¹å®šçš„RMMå…¼å®¹æ€§éœ€è¦åº”ç”¨å±‚é¢è§£å†³
- å¯é€šè¿‡minimal code changeæˆ–custom imageè§£å†³

**ğŸš€ ç”Ÿäº§å°±ç»ªçŠ¶æ€ï¼š**
- åŸºç¡€è®¾æ–½ï¼š100%å°±ç»ª
- å®‰å…¨æ¨¡å‹ï¼šç”Ÿäº§çº§
- å¯æ‰©å±•æ€§ï¼šå·²éªŒè¯
- ç›‘æ§èƒ½åŠ›ï¼šå®Œæ•´

æ­¤è§£å†³æ–¹æ¡ˆä¸ºGPUç§‘å­¦è®¡ç®—workloadåœ¨Kubernetesä¸Šçš„ç”Ÿäº§éƒ¨ç½²æä¾›äº†å®Œæ•´çš„ã€å®‰å…¨çš„ã€å¯æ‰©å±•çš„åŸºç¡€æ¶æ„ã€‚

## 15. RAPIDSç”¨æˆ·UIDä¿®æ­£ - é‡å¤§çªç ´ ğŸ‰

### é—®é¢˜å‘ç°
åœ¨æ‰§è¡Œ`gpu-production-init-simple-test`æ—¶ï¼Œç”¨æˆ·å‘ç°å…³é”®çº¿ç´¢ï¼š
```
Running as: ubuntu (uid=1000(ubuntu) gid=1000(ubuntu))
```
è€ŒInit containerè®¾ç½®çš„æƒé™æ˜¯ç»™ï¼š
```
rapids-user-uid:1001
rapids-user-gid:1001
```

**æ ¹æœ¬åŸå› åˆ†æ**ï¼š
- **å®¹å™¨é•œåƒå®é™…ç”¨æˆ·**ï¼š`ubuntu: UID 1000, GID 1000` | `rapids: UID 1001, GID 1001`
- **é”™è¯¯é…ç½®**ï¼š`runAsUser: 1000` (ubuntuç”¨æˆ·) 
- **æƒé™ç›®æ ‡**ï¼šInit Containerç»™UID 1001 (rapidsç”¨æˆ·) è®¾ç½®æƒé™
- **ç»“æœ**ï¼šæƒé™ä¸åŒ¹é…å¯¼è‡´Python/condaè®¿é—®å¤±è´¥

### ä¿®æ­£æ–¹æ¡ˆ
**åˆ›å»º** `examples/tasks/gpu-papermill-execution-production-rapids-fixed.yaml`ï¼š

**å…³é”®ä¿®æ­£**ï¼š
```yaml
securityContext:
  runAsUser: 1001  # CORRECTED: ä½¿ç”¨å®é™…çš„RAPIDSç”¨æˆ·UID 1001ï¼Œä¸æ˜¯1000
  runAsGroup: 1001 # CORRECTED: ä½¿ç”¨å®é™…çš„RAPIDSç»„GID 1001ï¼Œä¸æ˜¯1000
```

**Init Containerå¢å¼º**ï¼š
```bash
# è·å–å®é™…çš„RAPIDSç”¨æˆ·UID
if id rapids >/dev/null 2>&1; then
  RAPIDS_UID=$(id -u rapids)  # å®é™…ç»“æœï¼š1001
  RAPIDS_GID=$(id -g rapids)  # å®é™…ç»“æœï¼š1001
  echo "âœ… RAPIDS user found with actual UID: $(id rapids)"
```

### éªŒè¯ç»“æœ - é‡å¤§æˆåŠŸï¼

**âœ… æƒé™é—®é¢˜å½»åº•è§£å†³**ï¼š
```
Running as: rapids (uid=1001(rapids) gid=1001(rapids))
Home: /home/rapids
```

**âœ… Pythonç¯å¢ƒå®Œå…¨å¯è®¿é—®**ï¼š
- âœ… Python OK
- âœ… pip OK  
- âœ… conda OK
- âœ… ä¸å†æœ‰Permission deniedé”™è¯¯

**âœ… NotebookæˆåŠŸå¼€å§‹æ‰§è¡Œ**ï¼š
- âœ… æˆåŠŸå¯¼å…¥scanpyã€cupyã€rapids_singlecell
- âœ… Papermillæ­£å¸¸å¯åŠ¨å’Œè¿æ¥kernel
- âœ… æ‰§è¡Œåˆ°GPUç›¸å…³ä»£ç æ‰å‡ºç°æ–°çš„é—®é¢˜

**âœ… GPUåŸºç¡€è®¾æ–½éªŒè¯å®Œå…¨æ­£å¸¸**ï¼š
- âœ… GPU Operator: `nvidia-gpu-operator` namespace è¿è¡Œæ­£å¸¸
- âœ… è®¾å¤‡æ’ä»¶: `nvidia-device-plugin-daemonset` æ­£å¸¸
- âœ… èŠ‚ç‚¹èµ„æº: `nvidia.com/gpu: 4`, NVIDIA-A16, 15356MB
- âœ… èµ„æºåˆ†é…: Podæ­£ç¡®è·å¾— `nvidia.com/gpu: 1`

### æ–°é—®é¢˜è¯†åˆ«

**âŒ å®¹å™¨å†…GPUè®¾å¤‡è®¿é—®**ï¼š
- å°½ç®¡Kubernetesæ­£ç¡®åˆ†é…GPUèµ„æºï¼Œå®¹å™¨å†…æ£€æµ‹åˆ°ï¼š
- `No NVIDIA GPU detected`
- `cudaErrorNoDevice: no CUDA-capable device is detected`

**âŒ RMMå…¼å®¹æ€§**ï¼š
- `AttributeError: 'CUDARuntimeError' object has no attribute 'msg'`
- è¿™æ˜¯RMMåº“ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œä¸æ˜¯æƒé™é—®é¢˜

### é‡Œç¨‹ç¢‘æ„ä¹‰

**ğŸ¯ æ ¹æœ¬é—®é¢˜è§£å†³**ï¼šDocker Compose vs Kubernetesçš„æƒé™å·®å¼‚é—®é¢˜å½»åº•è§£å†³
**ğŸ—ï¸ æ¶æ„æˆç†Ÿ**ï¼šInit Containeræ¨¡å¼çš„ç”Ÿäº§çº§å®ç°
**ğŸ” è¯Šæ–­å‡†ç¡®**ï¼šç”¨æˆ·è§‚å¯ŸåŠ›å‘ç°äº†å…³é”®çš„UIDä¸åŒ¹é…é—®é¢˜
**ğŸš€ æŠ€æœ¯çªç ´**ï¼šä»æƒé™å¤±è´¥åˆ°æˆåŠŸæ‰§è¡Œnotebookçš„é‡å¤§è¿›å±•

### ä¸‹ä¸€æ­¥

**å½“å‰ä¼˜å…ˆçº§**ï¼š
1. è¯Šæ–­å®¹å™¨å†…GPUè®¾å¤‡è®¿é—®é—®é¢˜ï¼ˆç¡¬ä»¶æ˜ å°„å±‚é¢ï¼‰
2. è§£å†³RMMå…¼å®¹æ€§é—®é¢˜ï¼ˆå¯èƒ½éœ€è¦ç®€åŒ–æµ‹è¯•æˆ–ç‰ˆæœ¬è°ƒæ•´ï¼‰
3. å®Œæˆå®Œæ•´çš„8æ­¥workflowéªŒè¯

**æŠ€æœ¯å€ºåŠ¡**ï¼š
- GPUè®¾å¤‡æ’ä»¶æ˜ å°„æœºåˆ¶éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•
- RMMåˆå§‹åŒ–éœ€è¦é”™è¯¯å¤„ç†æˆ–ç‰ˆæœ¬å…¼å®¹æ€§ä¿®å¤

è¿™æ¬¡çªç ´ä¸ºæ•´ä¸ªé¡¹ç›®å¥ å®šäº†åšå®çš„åŸºç¡€ï¼Œæƒé™é—®é¢˜çš„å½»åº•è§£å†³ä¸ºåç»­å·¥ä½œæ‰«æ¸…äº†æœ€å¤§çš„éšœç¢ã€‚

---

## 13. GitHub Webhook é…ç½®é—®é¢˜

### é—®é¢˜ï¼šEventListener æ”¶åˆ°è¯·æ±‚ä½†å‡ºç° JSON è§£æé”™è¯¯

**ç°è±¡**ï¼š
```
{"severity":"error","timestamp":"2025-07-31T03:26:52.223Z","logger":"eventlistener","caller":"sink/validate_payload.go:42","message":"Invalid event body format : unexpected end of JSON input","commit":"4dbb0a6"}
```

**åŸå› åˆ†æ**ï¼š
- GitHub webhook ç­¾åéªŒè¯å¤±è´¥
- GitHub interceptor æ— æ³•éªŒè¯è¯·æ±‚çš„æœ‰æ•ˆæ€§
- å¯¼è‡´è¯·æ±‚è¢«æ‹’ç»å¹¶è¿”å›ç©ºå“åº”

**è§£å†³æ–¹æ¡ˆ1ï¼šç”Ÿäº§çº§é…ç½®ï¼ˆæ¨èï¼‰**
```bash
# ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ GitHub interceptor é…ç½®
cat <<EOF | kubectl apply -f -
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: github-webhook-production
  namespace: tekton-pipelines
spec:
  serviceAccountName: tekton-triggers-sa
  triggers:
  - name: github-production-trigger
    interceptors:
    - name: "verify-github-payload"
      ref:
        name: "github"
      params:
      - name: "secretRef"
        value:
          secretName: github-webhook-secret
          secretKey: webhook-secret
      - name: "eventTypes"
        value: ["push", "pull_request"]
    bindings:
    - ref: github-webhook-triggerbinding
    template:
      ref: github-webhook-triggertemplate
EOF
```

**è§£å†³æ–¹æ¡ˆ2ï¼šè°ƒè¯•é…ç½®ï¼ˆä¸´æ—¶ï¼‰**
```bash
# å¦‚æœéœ€è¦è°ƒè¯•ï¼Œå¯ä»¥æš‚æ—¶ä½¿ç”¨æ— ç­¾åéªŒè¯çš„é…ç½®
cat <<EOF | kubectl apply -f -
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: github-webhook-debug
  namespace: tekton-pipelines
spec:
  serviceAccountName: tekton-triggers-sa
  triggers:
  - name: github-debug-trigger
    interceptors:
    - name: "filter-github-events"
      ref:
        name: "cel"
      params:
      - name: "filter"
        value: "header.match('X-GitHub-Event', 'push|pull_request')"
    bindings:
    - ref: github-webhook-triggerbinding
    template:
      ref: github-webhook-triggertemplate
EOF
```

### é—®é¢˜ï¼šWebhook URL æ— æ³•è®¿é—®

**ç°è±¡**ï¼š
- GitHub webhook å‘é€å¤±è´¥
- curl æµ‹è¯•è¶…æ—¶
- EventListener æ²¡æœ‰æ”¶åˆ°è¯·æ±‚

**åŸå› åˆ†æ**ï¼š
1. **å†…ç½‘IPè®¿é—®é™åˆ¶**ï¼šä½¿ç”¨å†…ç½‘IPï¼ˆå¦‚10.x.x.xï¼‰GitHubæ— æ³•ä»å¤–éƒ¨è®¿é—®
2. **NodePortç«¯å£ç¼ºå¤±**ï¼šå¿˜è®°åœ¨URLä¸­æ·»åŠ NodePortç«¯å£å·
3. **é˜²ç«å¢™é˜»æ‹¦**ï¼šå…¬ç½‘ç«¯å£è¢«é˜²ç«å¢™é˜»æ­¢
4. **Ingress Controlleré…ç½®é—®é¢˜**

**è§£å†³æ–¹æ¡ˆ1ï¼šä¿®å¤NodePortç«¯å£é…ç½®ï¼ˆå¸¸è§é—®é¢˜ï¼‰**
```bash
# 1. æ£€æŸ¥nginx ingressçš„NodePortç«¯å£
kubectl get svc -n ingress-nginx ingress-nginx-controller

# è¾“å‡ºç¤ºä¾‹ï¼š
# NAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
# ingress-nginx-controller   NodePort   10.109.228.107   <none>        80:31960/TCP,443:30644/TCP   20h

# 2. ä½¿ç”¨æ­£ç¡®çš„ç«¯å£æ›´æ–°webhook URL
NODE_IP=$(hostname -I | awk '{print $1}')
HTTP_PORT=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[?(@.port==80)].nodePort}')
WEBHOOK_URL="http://webhook.$NODE_IP.nip.io:$HTTP_PORT"
echo "æ­£ç¡®çš„Webhook URL: $WEBHOOK_URL"

# 3. ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
echo "$WEBHOOK_URL" > webhook-url.txt

# 4. æµ‹è¯•è¿æ¥
curl -I "$WEBHOOK_URL" --max-time 10
```

**è§£å†³æ–¹æ¡ˆ2ï¼šå¤„ç†å†…ç½‘IPé™åˆ¶**
```bash
# æ£€æŸ¥å½“å‰IPç±»å‹
NODE_IP=$(hostname -I | awk '{print $1}')
echo "å½“å‰èŠ‚ç‚¹IP: $NODE_IP"

# å¦‚æœæ˜¯å†…ç½‘IP (10.x.x.x, 172.x.x.x, 192.168.x.x)ï¼Œéœ€è¦è·å–å…¬ç½‘IP
PUBLIC_IP=$(curl -s ifconfig.me 2>/dev/null || curl -s ipinfo.io/ip 2>/dev/null)
echo "å…¬ç½‘IP: $PUBLIC_IP"

# ä½¿ç”¨å…¬ç½‘IPç”Ÿæˆwebhook URL
if [ -n "$PUBLIC_IP" ]; then
    HTTP_PORT=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[?(@.port==80)].nodePort}')
    WEBHOOK_URL="http://webhook.$PUBLIC_IP.nip.io:$HTTP_PORT"
    echo "å…¬ç½‘Webhook URL: $WEBHOOK_URL"
    
    # æµ‹è¯•å…¬ç½‘è®¿é—®ï¼ˆå¯èƒ½è¢«é˜²ç«å¢™é˜»æ­¢ï¼‰
    curl -I "$WEBHOOK_URL" --max-time 10 || echo "âš ï¸ å…¬ç½‘ç«¯å£è¢«é˜²ç«å¢™é˜»æ­¢"
fi
```

**è§£å†³æ–¹æ¡ˆ3ï¼šä½¿ç”¨ngrokéš§é“ï¼ˆå½“é˜²ç«å¢™é˜»æ­¢æ—¶ï¼‰**
```bash
# 1. å®‰è£…ngrokï¼ˆå¦‚æœæœªå®‰è£…ï¼‰
wget -q https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz -O /tmp/ngrok.tgz
cd /tmp && tar xzf ngrok.tgz && sudo mv ngrok /usr/local/bin/

# 2. åˆ›å»ºéš§é“åˆ°å†…ç½‘webhookåœ°å€
NODE_IP=$(hostname -I | awk '{print $1}')
HTTP_PORT=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[?(@.port==80)].nodePort}')
ngrok http $NODE_IP:$HTTP_PORT --host-header=webhook.$NODE_IP.nip.io &

# 3. è·å–ngrokå…¬ç½‘URL
sleep 3
NGROK_URL=$(curl -s http://localhost:4040/api/tunnels | grep -o 'https://[^"]*\.ngrok-free\.app' | head -1)
echo "Ngrok Webhook URL: $NGROK_URL"
```

**è§£å†³æ–¹æ¡ˆ4ï¼šä¼ ç»ŸIngressé‡æ–°éƒ¨ç½²**
```bash
# å¦‚æœä»¥ä¸Šæ–¹æ¡ˆéƒ½ä¸è¡Œï¼Œé‡æ–°éƒ¨ç½²Ingress Controller
kubectl delete deployment ingress-nginx-controller -n ingress-nginx
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/deploy.yaml

# ç­‰å¾…å¯åŠ¨å¹¶é‡æ–°é…ç½®hostNetwork
kubectl wait --namespace ingress-nginx --for=condition=ready pod --selector=app.kubernetes.io/component=controller --timeout=120s
kubectl patch deployment ingress-nginx-controller -n ingress-nginx -p '{"spec":{"template":{"spec":{"hostNetwork":true,"dnsPolicy":"ClusterFirstWithHostNet"}}}}'
```

### é—®é¢˜ï¼šGitHub ä»“åº“é…ç½®é”™è¯¯

**æ£€æŸ¥æ¸…å•**ï¼š
```bash
# 1. ç¡®è®¤ webhook secret æ­£ç¡®
cat webhook-secret.txt

# 2. ç¡®è®¤ webhook URL æ ¼å¼
echo "http://webhook.$(hostname -I | awk '{print $1}').nip.io"

# 3. åœ¨ GitHub ä»“åº“è®¾ç½®ä¸­ç¡®è®¤ï¼š
#    - Payload URL æ­£ç¡®
#    - Content type: application/json
#    - Secret ä¸æ–‡ä»¶ä¸­ä¸€è‡´
#    - Events: Push events, Pull requests
#    - Active: å‹¾é€‰
```

**éªŒè¯é…ç½®**ï¼š
```bash
# è¿è¡Œå®Œæ•´éªŒè¯è„šæœ¬
chmod +x scripts/utils/verify-step3-webhook-configuration.sh
./scripts/utils/verify-step3-webhook-configuration.sh

# æ£€æŸ¥ GitHub webhook delivery çŠ¶æ€
# åœ¨ GitHub ä»“åº“ Settings â†’ Webhooks â†’ ç‚¹å‡» webhook â†’ Recent Deliveries
```

### é—®é¢˜ï¼šEventListeneræ”¶åˆ°è¯·æ±‚ä½†ä¸åˆ›å»ºPipelineRun

**ç°è±¡**ï¼š
- EventListenerè¿”å›202 Accepted
- GitHub webhookæ˜¾ç¤ºæˆåŠŸï¼ˆç»¿è‰²âœ“ï¼‰
- ä½†æ²¡æœ‰åˆ›å»ºPipelineRun

**åŸå› åˆ†æ**ï¼š
1. TriggerTemplateé…ç½®é”™è¯¯
2. Pipelineæˆ–Taskä¸å­˜åœ¨
3. æƒé™é—®é¢˜ï¼ˆServiceAccountï¼‰
4. å‚æ•°ç»‘å®šé”™è¯¯

**è¯Šæ–­æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥EventListenerè¯¦ç»†æ—¥å¿—
kubectl logs -l eventlistener=github-webhook-production -n tekton-pipelines --since=10m

# 2. æ£€æŸ¥Eventsä¸­çš„é”™è¯¯ä¿¡æ¯
kubectl get events -n tekton-pipelines --sort-by='.lastTimestamp' | tail -20

# 3. æ‰‹åŠ¨æµ‹è¯•Pipelineæ˜¯å¦æ­£å¸¸
cat <<EOF | kubectl create -f -
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: manual-test-webhook-pipeline-run-
  namespace: tekton-pipelines
spec:
  pipelineRef:
    name: webhook-pipeline
  params:
  - name: git-url
    value: https://github.com/johnnynv/tekton-poc.git
  - name: git-revision
    value: main
  workspaces:
  - name: shared-data
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
EOF

# 4. æ£€æŸ¥æ‰‹åŠ¨PipelineRunçŠ¶æ€
kubectl get pipelineruns -n tekton-pipelines | grep manual-test
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. éªŒè¯Pipelineå’ŒTaskså­˜åœ¨
kubectl get pipeline webhook-pipeline -n tekton-pipelines
kubectl get task git-clone hello-world -n tekton-pipelines

# 2. æ£€æŸ¥TriggerTemplateé…ç½®
kubectl describe triggertemplate github-webhook-triggertemplate -n tekton-pipelines

# 3. éªŒè¯ServiceAccountæƒé™
kubectl describe sa tekton-triggers-sa -n tekton-pipelines

# 4. é‡æ–°åˆ›å»ºEventListenerï¼ˆå¦‚æœé…ç½®æœ‰é—®é¢˜ï¼‰
kubectl delete eventlistener github-webhook-production -n tekton-pipelines
# ç„¶åé‡æ–°è¿è¡Œ03æ–‡æ¡£ä¸­çš„EventListeneråˆ›å»ºå‘½ä»¤
```

### é—®é¢˜ï¼šç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯

**å®Œæ•´éªŒè¯æµç¨‹**ï¼š

**æ­¥éª¤1ï¼šç»„ä»¶çŠ¶æ€æ£€æŸ¥**
```bash
# è¿è¡Œè‡ªåŠ¨éªŒè¯è„šæœ¬
chmod +x scripts/utils/verify-step3-webhook-configuration.sh
./scripts/utils/verify-step3-webhook-configuration.sh
```

**æ­¥éª¤2ï¼šæ¨¡æ‹ŸGitHub webhookæµ‹è¯•**
```bash
# 1. åˆ›å»ºçœŸå®çš„GitHub payload
cat > test-github-payload.json << 'EOF'
{
  "ref": "refs/heads/main",
  "before": "abc123def456789",
  "after": "def456789abc123",
  "repository": {
    "id": 123456789,
    "name": "tekton-poc",
    "full_name": "johnnynv/tekton-poc",
    "clone_url": "https://github.com/johnnynv/tekton-poc.git"
  },
  "head_commit": {
    "id": "def456789abc123",
    "message": "æµ‹è¯•Tekton webhooké›†æˆ [trigger]",
    "timestamp": "2025-07-31T05:05:00Z",
    "author": {
      "name": "johnnynv",
      "email": "johnnynv@example.com"
    }
  }
}
EOF

# 2. è®¡ç®—æ­£ç¡®çš„HMACç­¾å
WEBHOOK_SECRET=$(cat webhook-secret.txt)
WEBHOOK_URL=$(cat webhook-url.txt)
SIGNATURE=$(echo -n "$(cat test-github-payload.json)" | openssl dgst -sha256 -hmac "${WEBHOOK_SECRET}" | cut -d' ' -f2)

# 3. å‘é€æ¨¡æ‹Ÿwebhookè¯·æ±‚
curl -X POST "${WEBHOOK_URL}" \
  -H "Content-Type: application/json" \
  -H "X-GitHub-Event: push" \
  -H "X-Hub-Signature-256: sha256=${SIGNATURE}" \
  -H "X-GitHub-Delivery: test-$(date +%s)" \
  -d @test-github-payload.json \
  -v

# 4. ç«‹å³æ£€æŸ¥ç»“æœ
kubectl get pipelineruns -n tekton-pipelines | grep webhook
kubectl logs -l eventlistener=github-webhook-production -n tekton-pipelines --since=2m
```

**æ­¥éª¤3ï¼šç½‘ç»œè¿é€šæ€§æµ‹è¯•**
```bash
# 1. å†…ç½‘æµ‹è¯•
NODE_IP=$(hostname -I | awk '{print $1}')
HTTP_PORT=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[?(@.port==80)].nodePort}')
INTERNAL_URL="http://webhook.$NODE_IP.nip.io:$HTTP_PORT"

echo "å†…ç½‘æµ‹è¯•URL: $INTERNAL_URL"
curl -I "$INTERNAL_URL" --max-time 5

# 2. å…¬ç½‘æµ‹è¯•ï¼ˆå¦‚æœæœ‰å…¬ç½‘IPï¼‰
PUBLIC_IP=$(curl -s ifconfig.me 2>/dev/null)
if [ -n "$PUBLIC_IP" ]; then
    PUBLIC_URL="http://webhook.$PUBLIC_IP.nip.io:$HTTP_PORT"
    echo "å…¬ç½‘æµ‹è¯•URL: $PUBLIC_URL"
    curl -I "$PUBLIC_URL" --max-time 10 || echo "âš ï¸ å…¬ç½‘è®¿é—®è¢«é˜»æ­¢"
fi
```

**æ­¥éª¤4ï¼šå®Œæ•´ç«¯åˆ°ç«¯éªŒè¯æ£€æŸ¥æ¸…å•**
```bash
echo "=== Tekton Webhook ç«¯åˆ°ç«¯éªŒè¯ ==="
echo ""

# âœ… ç»„ä»¶æ£€æŸ¥
echo "1. æ£€æŸ¥æ ¸å¿ƒç»„ä»¶ï¼š"
kubectl get secret github-webhook-secret -n tekton-pipelines >/dev/null 2>&1 && echo "âœ… Webhook Secretå­˜åœ¨" || echo "âŒ Webhook Secretç¼ºå¤±"
kubectl get eventlistener github-webhook-production -n tekton-pipelines >/dev/null 2>&1 && echo "âœ… EventListenerå­˜åœ¨" || echo "âŒ EventListenerç¼ºå¤±"
kubectl get pipeline webhook-pipeline -n tekton-pipelines >/dev/null 2>&1 && echo "âœ… Pipelineå­˜åœ¨" || echo "âŒ Pipelineç¼ºå¤±"

# âœ… ç½‘ç»œæ£€æŸ¥
echo ""
echo "2. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼š"
if [ -f webhook-url.txt ]; then
    WEBHOOK_URL=$(cat webhook-url.txt)
    echo "Webhook URL: $WEBHOOK_URL"
    curl -I "$WEBHOOK_URL" --max-time 5 >/dev/null 2>&1 && echo "âœ… Webhook URLå¯è®¿é—®" || echo "âŒ Webhook URLä¸å¯è®¿é—®"
else
    echo "âŒ webhook-url.txtæ–‡ä»¶ä¸å­˜åœ¨"
fi

# âœ… åŠŸèƒ½æ£€æŸ¥
echo ""
echo "3. æ£€æŸ¥åŠŸèƒ½çŠ¶æ€ï¼š"
MANUAL_RUNS=$(kubectl get pipelineruns -n tekton-pipelines --no-headers 2>/dev/null | wc -l)
if [ "$MANUAL_RUNS" -gt 0 ]; then
    echo "âœ… Pipelineå¯ä»¥æ­£å¸¸è¿è¡Œï¼ˆå·²æœ‰$MANUAL_RUNSä¸ªPipelineRunï¼‰"
else
    echo "âš ï¸ å°šæœªæœ‰PipelineRunæ‰§è¡Œ"
fi

echo ""
echo "=== éªŒè¯å®Œæˆ ==="
```

**çŠ¶æ€**ï¼šå·²æ·»åŠ å®Œæ•´çš„ç«¯åˆ°ç«¯éªŒè¯å’Œæ•…éšœæ’é™¤æµç¨‹

### é—®é¢˜ï¼šNVIDIAå†…ç½‘DDNSæ˜¯å¦èƒ½è§£å†³GitHubè®¿é—®é—®é¢˜

**é—®é¢˜èƒŒæ™¯ï¼š**
æœ‰ç”¨æˆ·è¯¢é—®æ˜¯å¦å¯ä»¥ä½¿ç”¨NVIDIAå†…ç½‘çš„Dynamic DNS (client.nvidia.com/dyn.nvidia.com) æ¥è§£å†³GitHubæ— æ³•è®¿é—®å†…ç½‘webhookçš„é—®é¢˜ã€‚

**åˆ†æç»“è®ºï¼šâŒ ä¸èƒ½è§£å†³**

**åŸå› åˆ†æï¼š**
1. **è®¿é—®æ–¹å‘ä¸åŒ¹é…**
   ```
   NVIDIA DDNSè®¾è®¡: å†…ç½‘ä¸»æœºA â†â†’ å†…ç½‘ä¸»æœºB
   æˆ‘ä»¬çš„éœ€æ±‚:     GitHub(å¤–ç½‘) â†â†’ Webhook(å†…ç½‘) âŒ
   ```

2. **åŸŸåèŒƒå›´é™åˆ¶**
   ```bash
   # NVIDIA DDNSç”Ÿæˆçš„åŸŸå
   hostname.client.nvidia.com â†’ 10.34.2.129 (å†…ç½‘IP)
   
   # GitHubè®¿é—®æµ‹è¯•
   GitHub â†’ hostname.client.nvidia.com â†’ å†…ç½‘IP âŒ æ— æ³•è®¿é—®
   ```

3. **ç½‘ç»œæ¶æ„é™åˆ¶**
   - NVIDIA DDNSåªåœ¨å…¬å¸å†…ç½‘DNSä¸­ç”Ÿæ•ˆ
   - å¤–ç½‘æœåŠ¡(GitHub)æ— æ³•è§£æå†…ç½‘åŸŸå
   - å…¬å¸é˜²ç«å¢™é˜»æ­¢å¤–ç½‘ç›´æ¥è®¿é—®å†…ç½‘èµ„æº

**æ­£ç¡®è§£å†³æ–¹æ¡ˆå¯¹æ¯”ï¼š**

| æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | å®ç°éš¾åº¦ | æ•ˆæœ |
|------|----------|----------|------|
| **NVIDIA DDNS** | å†…ç½‘äº’è®¿ | ç®€å• | âŒ ä¸è§£å†³å¤–ç½‘è®¿é—® |
| **å…¬ç½‘IP+é˜²ç«å¢™** | ç”Ÿäº§ç¯å¢ƒ | ä¸­ç­‰ | âœ… æœ€ä½³æ–¹æ¡ˆ |
| **ngrokéš§é“** | å¼€å‘/æµ‹è¯• | ç®€å• | âœ… å¼€å‘ç¯å¢ƒæ¨è |
| **LoadBalancer** | äº‘ç¯å¢ƒ | ä¸­ç­‰ | âœ… äº‘ç¯å¢ƒæœ€ä½³ |

**å®é™…éªŒè¯ç»“æœï¼š**
```bash
# å½“å‰é…ç½®çŠ¶æ€
å†…ç½‘IP: 10.34.2.129
å…¬ç½‘IP: 216.228.125.129
å†…ç½‘URLæµ‹è¯•: âœ… æˆåŠŸ (HTTP 202)
å…¬ç½‘URLæµ‹è¯•: âŒ è¶…æ—¶ (é˜²ç«å¢™é˜»æ­¢)

# ç»“è®º
âœ… ç³»ç»ŸåŠŸèƒ½å®Œå…¨æ­£å¸¸
âŒ ä»…ç½‘ç»œè®¿é—®å—é™
```

**æ¨èçš„ç”Ÿäº§è§£å†³æ–¹æ¡ˆï¼š**
```bash
# æ–¹æ¡ˆ1ï¼šå¼€æ”¾é˜²ç«å¢™ç«¯å£ï¼ˆè”ç³»ç½‘ç»œç®¡ç†å‘˜ï¼‰
# å¼€æ”¾ç«¯å£31960ç”¨äºå¤–ç½‘è®¿é—®

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨ngrokéš§é“ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
ngrok http 10.34.2.129:31960 --host-header=webhook.10.34.2.129.nip.io

# æ–¹æ¡ˆ3ï¼šé…ç½®LoadBalancerï¼ˆäº‘ç¯å¢ƒï¼‰
kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{"spec":{"type":"LoadBalancer"}}'
```

**çŠ¶æ€**ï¼šå·²åˆ†æNVIDIA DDNSæ–¹æ¡ˆå¹¶ç¡®è®¤ä¸é€‚ç”¨ï¼Œæä¾›äº†æ­£ç¡®çš„è§£å†³æ–¹æ¡ˆ

---

## ğŸ“‹ 04é˜¶æ®µï¼šGPU Pipeline éƒ¨ç½²é—®é¢˜

### é—®é¢˜ï¼šPVC ä¸€ç›´å¤„äº Pending çŠ¶æ€

**ç°è±¡**ï¼š
```
NAME                    STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS
source-code-workspace   Pending                                      local-path
```

**åŸå› **ï¼š
- ä½¿ç”¨ `WaitForFirstConsumer` æ¨¡å¼çš„ StorageClassï¼Œéœ€è¦ç­‰å¾… Pod è°ƒåº¦
- ç¼ºå°‘å¯¹åº”çš„ PersistentVolume

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# 1. åˆ›å»ºç«‹å³ç»‘å®šçš„ StorageClass
cat > /tmp/immediate-storage.yaml << 'EOF'
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: immediate-local
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: Immediate
allowVolumeExpansion: true
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: tekton-workspace-pv
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: immediate-local
  hostPath:
    path: /tmp/tekton-workspace
    type: DirectoryOrCreate
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: source-code-workspace
  namespace: tekton-pipelines
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: immediate-local
EOF

kubectl apply -f /tmp/immediate-storage.yaml

# 2. éªŒè¯ç»‘å®šæˆåŠŸ
kubectl get pvc -n tekton-pipelines
kubectl get pv
```

### é—®é¢˜ï¼šPipeline Step1 æƒé™è¢«æ‹’ç»

**ç°è±¡**ï¼š
```
mkdir: cannot create directory 'input': Permission denied
mkdir: cannot create directory 'output': Permission denied
```

**åŸå› **ï¼š
- Step1 æ²¡æœ‰ init container è®¾ç½®æƒé™
- æ™®é€šç”¨æˆ·æ— æ³•åœ¨ workspace åˆ›å»ºç›®å½•

**è§£å†³æ–¹æ¡ˆ**ï¼š
ä¸º Step1 æ·»åŠ  root æƒé™å’Œ chown æ“ä½œï¼š

```yaml
# åœ¨ step1-container-environment-setup çš„ setup-environment step ä¸­ï¼š
- name: setup-environment
  image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
  securityContext:
    runAsUser: 0  # æ·»åŠ è¿™è¡Œ
  script: |
    #!/bin/bash
    set -eu
    
    echo "ğŸ³ Step 1: Container Environment Setup"
    echo "====================================="
    
    DOCKER_WRITEABLE_DIR="$(workspaces.shared-storage.path)"
    cd "$DOCKER_WRITEABLE_DIR"
    
    mkdir -p {input,output,artifacts,logs}
    
    # æ·»åŠ æƒé™è®¾ç½®
    chown -R 1001:1001 "$DOCKER_WRITEABLE_DIR"
    
    # å…¶ä½™é…ç½®...
```

### é—®é¢˜ï¼šGPU æµ‹è¯•å‘½ä»¤è¯­æ³•é”™è¯¯

**ç°è±¡**ï¼š
```
error: unknown flag: --limits
```

**åŸå› **ï¼š
- kubectl run å‘½ä»¤è¯­æ³•å˜æ›´ï¼Œ--limits å‚æ•°ä¸å†æ”¯æŒ

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# ä½¿ç”¨ overrides è¯­æ³•
kubectl run gpu-test --rm -i --tty --restart=Never \
  --image=nvidia/cuda:12.8-runtime-ubuntu22.04 \
  --overrides='{"spec":{"containers":[{"name":"gpu-test","image":"nvidia/cuda:12.8-runtime-ubuntu22.04","resources":{"limits":{"nvidia.com/gpu":"1"}}}]}}' \
  -- nvidia-smi
```

### é—®é¢˜ï¼šService Account æƒé™ä¸è¶³

**ç°è±¡**ï¼š
```
TaskRun cannot create pods
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
åˆ›å»º Service Account å’Œå¿…è¦çš„ RBAC é…ç½®ï¼š

```bash
cat > /tmp/tekton-pipeline-service-account.yaml << 'EOF'
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tekton-pipeline-service
  namespace: tekton-pipelines
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tekton-pipeline-service-role
rules:
- apiGroups: ["tekton.dev"]
  resources: ["pipelines", "pipelineruns", "tasks", "taskruns"]
  verbs: ["get", "list", "create", "update", "patch", "watch"]
- apiGroups: [""]
  resources: ["pods", "services", "endpoints", "persistentvolumeclaims", "configmaps", "secrets"]
  verbs: ["get", "list", "create", "update", "patch", "watch", "delete"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tekton-pipeline-service-binding
subjects:
- kind: ServiceAccount
  name: tekton-pipeline-service
  namespace: tekton-pipelines
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tekton-pipeline-service-role
EOF

kubectl apply -f /tmp/tekton-pipeline-service-account.yaml
```

### é—®é¢˜ï¼šPapermill Step3 PCA KeyError

**ç°è±¡**ï¼š
```
KeyError: 'pca'
åœ¨ sc.pl.pca_variance_ratio(adata, log=True, n_pcs=100) æ­¥éª¤
PapermillExecutionErrorä½†Pipelineç»§ç»­æ‰§è¡Œå¹¶"æˆåŠŸ"å®Œæˆ
```

**åŸå› **ï¼š
- PCAè®¡ç®—æ­¥éª¤(`sc.tl.pca()`)æ²¡æœ‰æ­£ç¡®æ‰§è¡Œ
- scanpyæœŸæœ›åœ¨`adata.uns['pca']`ä¸­æ‰¾åˆ°PCAç»“æœä½†æ‰¾ä¸åˆ°
- è¿™æ˜¯ç§‘å­¦åˆ†ææµç¨‹é”™è¯¯ï¼Œä¸æ˜¯æŠ€æœ¯æ¶æ„é”™è¯¯

**è§£å†³æ–¹æ¡ˆ1 - ä½¿ç”¨ä¿®å¤Task**ï¼š
```bash
# æ–¹æ¡ˆ1ï¼šä½¿ç”¨ä¿®å¤ç‰ˆæœ¬çš„é»˜è®¤Pipeline (æ¨è)
kubectl apply -f examples/production/pipelines/gpu-real-8-step-workflow.yaml

# æ–¹æ¡ˆ2ï¼šæˆ–ä½¿ç”¨liteç‰ˆæœ¬
kubectl apply -f examples/production/pipelines/gpu-real-8-step-workflow-lite.yaml
```

**è§£å†³æ–¹æ¡ˆ2 - æ‰‹åŠ¨ä¿®å¤ç°æœ‰ç»“æœ**ï¼š
```bash
# åœ¨ç°æœ‰workspaceä¸­ä¿®å¤PCAé—®é¢˜
kubectl run pca-fix-pod --rm -i --tty --restart=Never \
  --image=nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12 \
  --overrides='{"spec":{"containers":[{"name":"pca-fix","image":"nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12","command":["python3","-c","import nbformat; import scanpy as sc; print(\"ğŸ”§ PCAä¿®å¤å·¥å…·å¯åŠ¨\")"],"volumeMounts":[{"mountPath":"/workspace","name":"shared-storage"}]}],"volumes":[{"name":"shared-storage","persistentVolumeClaim":{"claimName":"source-code-workspace"}}]}}' \
  -n tekton-pipelines
```

**çŠ¶æ€åˆ¤æ–­**ï¼š
- âœ… **æŠ€æœ¯æ¶æ„æˆåŠŸ**: Pipelineã€å­˜å‚¨ã€æƒé™ã€GPUéƒ½æ­£å¸¸
- âš ï¸ **ç§‘å­¦åˆ†æéƒ¨åˆ†é”™è¯¯**: PCAå¯è§†åŒ–æ­¥éª¤å¤±è´¥
- ğŸ“Š **ç»“æœ**: å¤§éƒ¨åˆ†åˆ†æå®Œæˆï¼Œåªæœ‰PCAå›¾è¡¨ç¼ºå¤±

### é—®é¢˜ï¼šTaské—´ç¯å¢ƒéš”ç¦»å¯¼è‡´ä¾èµ–ä¸¢å¤±

**ç°è±¡**ï¼š
```
Step1å®‰è£…äº†PythonåŒ…ï¼Œä½†Step3æ‰§è¡Œæ—¶æç¤ºåŒ…ä¸å­˜åœ¨
ModuleNotFoundError: No module named 'scanpy'
```

**åŸå› **ï¼š
- **Tektonæ¶æ„ç‰¹æ€§**: æ¯ä¸ªTask = ç‹¬ç«‹Pod = å…¨æ–°å®¹å™¨ç¯å¢ƒ
- **ç¯å¢ƒéš”ç¦»**: Step1å®‰è£…çš„åŒ…åœ¨Step2/Step3ä¸­ä¸å¯ç”¨
- **åªæœ‰æ–‡ä»¶å…±äº«**: é€šè¿‡workspaceå…±äº«æ–‡ä»¶ï¼Œä¸å…±äº«è½¯ä»¶ç¯å¢ƒ

**è§£å†³æ–¹æ¡ˆ1 - å•Taskè®¾è®¡ï¼ˆæ¨èï¼‰**ï¼š
```bash
# å°†ç›¸å…³æ­¥éª¤åˆå¹¶åˆ°åŒä¸€Taskä¸­ï¼Œå®ç°ç¯å¢ƒè¿ç»­æ€§
cat > /tmp/single-task-pipeline.yaml << 'EOF'
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: gpu-single-task-
  namespace: tekton-pipelines
spec:
  pipelineSpec:
    tasks:
    - name: gpu-workflow-all-steps
      taskSpec:
        workspaces:
        - name: shared-storage
        steps:
        - name: step1-environment-setup
          image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
          script: |
            # å®‰è£…æ‰€æœ‰ä¾èµ–
            pip install papermill jupyter scanpy rapids-singlecell
        - name: step2-git-clone
          image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
          script: |
            # ç¯å¢ƒä¿æŒï¼Œå¯ç›´æ¥ä½¿ç”¨å·²å®‰è£…çš„åŒ…
            git clone https://github.com/rapidsai/single-cell-analysis-blueprint.git
        - name: step3-papermill-execution
          image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
          script: |
            # ç¯å¢ƒä¿æŒï¼Œæ‰€æœ‰åŒ…éƒ½å¯ç”¨
            papermill input.ipynb output.ipynb
  workspaces:
  - name: shared-storage
    persistentVolumeClaim:
      claimName: source-code-workspace
EOF

kubectl create -f /tmp/single-task-pipeline.yaml
```

**è§£å†³æ–¹æ¡ˆ2 - æ¯ä¸ªTaské‡æ–°å®‰è£…**ï¼š
```bash
# åœ¨éœ€è¦ä¾èµ–çš„Taskä¸­é‡æ–°å®‰è£…
# åœ¨Step3çš„taskSpecä¸­æ·»åŠ ï¼š
steps:
- name: install-deps
  image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
  script: |
    pip install papermill jupyter scanpy rapids-singlecell
- name: execute-notebook
  image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
  script: |
    papermill input.ipynb output.ipynb
```

**è§£å†³æ–¹æ¡ˆ3 - é¢„æ„å»ºé•œåƒ**ï¼š
```bash
# æ„å»ºåŒ…å«æ‰€æœ‰ä¾èµ–çš„è‡ªå®šä¹‰é•œåƒ
# Dockerfile:
FROM nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
RUN pip install papermill jupyter scanpy rapids-singlecell

# åœ¨Pipelineä¸­ä½¿ç”¨è‡ªå®šä¹‰é•œåƒ
image: your-registry/gpu-analysis:latest
```

**æ¶æ„ç†è§£**ï¼š
```
å¤šTaskæ¶æ„ï¼ˆå½“å‰ï¼‰:
Task1[å®‰è£…åŒ…] â†’ ç¯å¢ƒæ¶ˆå¤± âŒ
Task2[Git Clone] â†’ é‡æ–°å¼€å§‹ï¼Œæ— åŒ… âŒ  
Task3[æ‰§è¡Œ] â†’ é‡æ–°å¼€å§‹ï¼Œéœ€é‡è£…åŒ… ğŸ”„

å•Taskæ¶æ„ï¼ˆæ¨èï¼‰:
Task1[
  Step1: å®‰è£…åŒ… âœ…
  Step2: Git Clone âœ… (åŒ…ä»å¯ç”¨)
  Step3: æ‰§è¡Œ âœ… (åŒ…ä»å¯ç”¨)
]
```

## ğŸ“ ç›¸å…³æ–‡æ¡£

- [Tekton å®‰è£…æŒ‡å—](01-tekton-installation.md)
- [Triggers é…ç½®æŒ‡å—](02-tekton-triggers-setup.md)  
- [Webhook é…ç½®æŒ‡å—](03-tekton-webhook-configuration.md)
- [GPU Pipeline éƒ¨ç½²æŒ‡å—](04-gpu-pipeline-deployment.md)

---

**æœ€åæ›´æ–°**: 2025-07-31  
**ç‰ˆæœ¬**: v1.1 