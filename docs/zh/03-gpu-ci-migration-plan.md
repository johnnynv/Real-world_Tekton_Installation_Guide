# GPU ç§‘å­¦è®¡ç®— CI æµç¨‹è¿ç§»æ–¹æ¡ˆ

ä» GitHub Actions è¿ç§»åˆ° Tekton çš„ç”Ÿäº§çº§ GPU CI/CD è§£å†³æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•

1. [ç°çŠ¶åˆ†æ](#ç°çŠ¶åˆ†æ)
2. [ç›®æ ‡æ¶æ„](#ç›®æ ‡æ¶æ„)
3. [GPU èµ„æºç®¡ç†æ–¹æ¡ˆ](#gpu-èµ„æºç®¡ç†æ–¹æ¡ˆ)
4. [Pipeline è®¾è®¡](#pipeline-è®¾è®¡)
5. [å­˜å‚¨å’Œå·¥ä½œç©ºé—´](#å­˜å‚¨å’Œå·¥ä½œç©ºé—´)
6. [å®¹å™¨é•œåƒç­–ç•¥](#å®¹å™¨é•œåƒç­–ç•¥)
7. [ç›‘æ§å’ŒæŠ¥å‘Š](#ç›‘æ§å’ŒæŠ¥å‘Š)
8. [å®‰å…¨å’Œæƒé™](#å®‰å…¨å’Œæƒé™)
9. [éƒ¨ç½²å’Œè¿ç»´](#éƒ¨ç½²å’Œè¿ç»´)
10. [è¿ç§»è·¯çº¿å›¾](#è¿ç§»è·¯çº¿å›¾)

## ğŸ” ç°çŠ¶åˆ†æ

### å½“å‰ GitHub Actions æµç¨‹

```mermaid
graph TD
    A[GitHub Webhook] --> B[GitHub Actions Runner with GPU]
    B --> C[Docker Compose å¯åŠ¨å®¹å™¨]
    C --> D[Papermill æ‰§è¡Œ Notebook]
    D --> E[ç”Ÿæˆ .ipynb æ–‡ä»¶]
    E --> F[Jupyter nbconvert è½¬ HTML]
    F --> G[ä¸‹è½½æµ‹è¯• Repo]
    G --> H[æ‰§è¡Œ PyTest]
    H --> I[ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶]
    I --> J[ä¸Šä¼  Artifacts]
    J --> K[é‚®ä»¶é€šçŸ¥]
```

### å½“å‰æµç¨‹ç‰¹ç‚¹

- âœ… **èµ„æº**: GPU æ”¯æŒçš„ VM Runner
- âœ… **éš”ç¦»**: Docker å®¹å™¨éš”ç¦»
- âœ… **ä¾èµ–**: papermillã€jupyterã€poetry å·¥å…·é“¾
- âœ… **è¾“å‡º**: HTML æŠ¥å‘Šã€æµ‹è¯•ç»“æœã€è¦†ç›–ç‡æŠ¥å‘Š
- âš ï¸ **é™åˆ¶**: å•èŠ‚ç‚¹æ‰§è¡Œï¼Œèµ„æºåˆ©ç”¨ç‡ä½
- âš ï¸ **æ‰©å±•æ€§**: éš¾ä»¥å¤„ç†å¤šå¹¶å‘ä»»åŠ¡

## ğŸ¯ ç›®æ ‡æ¶æ„

### Tekton è¿ç§»åæ¶æ„

```mermaid
graph TD
    A[GitHub Webhook] --> B[Tekton EventListener]
    B --> C[GPU Pipeline è§¦å‘]
    C --> D[Task: ç¯å¢ƒå‡†å¤‡]
    D --> E[Task: Notebook æ‰§è¡Œ]
    E --> F[Task: ç»“æœè½¬æ¢]
    F --> G[Task: æµ‹è¯•æ‰§è¡Œ]
    G --> H[Task: æŠ¥å‘Šç”Ÿæˆ]
    H --> I[Task: ç»“æœå‘å¸ƒ]
    I --> J[é€šçŸ¥æœåŠ¡]
    
    subgraph "K8s GPU èŠ‚ç‚¹æ± "
        K[GPU Node 1]
        L[GPU Node 2]
        M[GPU Node N]
    end
    
    subgraph "å­˜å‚¨å±‚"
        N[æŒä¹…åŒ–å­˜å‚¨]
        O[å¯¹è±¡å­˜å‚¨]
        P[ç¼“å­˜å±‚]
    end
```

### æ¶æ„ä¼˜åŠ¿

- ğŸš€ **å¼¹æ€§æ‰©å±•**: æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒåº¦ GPU èµ„æº
- ğŸ”„ **å¹¶è¡Œå¤„ç†**: æ”¯æŒå¤šä¸ª Notebook å¹¶è¡Œæ‰§è¡Œ
- ğŸ“Š **èµ„æºæ•ˆç‡**: GPU èµ„æºæ± åŒ–ç®¡ç†
- ğŸ›¡ï¸ **æ•…éšœæ¢å¤**: è‡ªåŠ¨é‡è¯•å’Œæ•…éšœè½¬ç§»
- ğŸ“ˆ **å¯è§‚æµ‹æ€§**: å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ

## ğŸ® GPU èµ„æºç®¡ç†æ–¹æ¡ˆ

### 1. GPU èŠ‚ç‚¹é…ç½®

#### èŠ‚ç‚¹æ ‡ç­¾å’Œæ±¡ç‚¹ç­–ç•¥

```yaml
# GPU èŠ‚ç‚¹æ ‡ç­¾
metadata:
  labels:
    node-type: gpu-compute
    gpu-type: nvidia-tesla-v100  # æˆ–å…¶ä»– GPU å‹å·
    gpu-count: "4"
    workload: scientific-computing

# æ±¡ç‚¹é…ç½®ï¼ˆç¡®ä¿åªæœ‰ GPU å·¥ä½œè´Ÿè½½è°ƒåº¦åˆ° GPU èŠ‚ç‚¹ï¼‰
spec:
  taints:
  - key: nvidia.com/gpu
    value: "true"
    effect: NoSchedule
```

#### GPU Operator éƒ¨ç½²

```bash
# 1. å®‰è£… NVIDIA GPU Operator
helm repo add nvidia https://nvidia.github.io/k8s-device-plugin
helm repo update

# 2. éƒ¨ç½² GPU Operator
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator-resources \
  --create-namespace \
  --set operator.defaultRuntime=containerd
```

### 2. GPU èµ„æºè¯·æ±‚ç­–ç•¥

#### èµ„æºé…é¢ç®¡ç†

```yaml
# GPU èµ„æºé…é¢
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: tekton-gpu-pipelines
spec:
  hard:
    requests.nvidia.com/gpu: "8"  # æ€» GPU è¯·æ±‚é™åˆ¶
    limits.nvidia.com/gpu: "8"
    requests.memory: "64Gi"
    requests.cpu: "16"
```

#### GPU å…±äº«ç­–ç•¥

```yaml
# å¤šå®ä¾‹ GPU (MIG) é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-sharing-config
data:
  config.yaml: |
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 4  # æ¯ä¸ªç‰©ç† GPU è™šæ‹Ÿä¸º 4 ä¸ª
```

### 3. GPU ä»»åŠ¡è°ƒåº¦ä¼˜åŒ–

#### èŠ‚ç‚¹äº²å’Œæ€§é…ç½®

```yaml
# ç§‘å­¦è®¡ç®—å·¥ä½œè´Ÿè½½ä¼˜å…ˆè°ƒåº¦åˆ°é«˜æ€§èƒ½ GPU èŠ‚ç‚¹
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: gpu-type
            operator: In
            values: ["nvidia-tesla-v100", "nvidia-a100"]
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: gpu-count
            operator: Gt
            values: ["2"]
```

## ğŸ”§ Pipeline è®¾è®¡

### 1. æ ¸å¿ƒ Pipeline æ¶æ„

```yaml
# ç§‘å­¦è®¡ç®— CI Pipeline
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: scientific-computing-ci
  namespace: tekton-gpu-pipelines
spec:
  description: |
    GPU åŠ é€Ÿçš„ç§‘å­¦è®¡ç®— Notebook CI æµæ°´çº¿
  params:
  - name: git-repo-url
    description: æºä»£ç ä»“åº“ URL
  - name: git-revision
    description: Git æäº¤ ID
  - name: notebook-path
    description: Notebook æ–‡ä»¶è·¯å¾„
    default: "notebooks/01_scRNA_analysis_preprocessing.ipynb"
  - name: gpu-count
    description: æ‰€éœ€ GPU æ•°é‡
    default: "1"
  
  workspaces:
  - name: source-code
    description: æºä»£ç å·¥ä½œç©ºé—´
  - name: notebook-output
    description: Notebook è¾“å‡ºå·¥ä½œç©ºé—´
  - name: test-results
    description: æµ‹è¯•ç»“æœå·¥ä½œç©ºé—´
  - name: gpu-cache
    description: GPU è®¡ç®—ç¼“å­˜å·¥ä½œç©ºé—´
```

### 2. Task åˆ†è§£è®¾è®¡

#### Task 1: ç¯å¢ƒå‡†å¤‡å’Œä»£ç æ£€å‡º

```yaml
# èŒè´£ï¼š
# - Git ä»£ç æ£€å‡º
# - ä¾èµ–ç¯å¢ƒæ£€æŸ¥
# - GPU ç¯å¢ƒéªŒè¯
# - ç¼“å­˜é¢„çƒ­

# ç‰¹ç‚¹ï¼š
# - ä¸éœ€è¦ GPU èµ„æº
# - å¿«é€Ÿæ‰§è¡Œï¼ˆ< 2 åˆ†é’Ÿï¼‰
# - ä¸ºåç»­ GPU ä»»åŠ¡åšå‡†å¤‡
```

#### Task 2: Notebook æ‰§è¡Œï¼ˆGPU å¯†é›†å‹ï¼‰

```yaml
# èŒè´£ï¼š
# - ä½¿ç”¨ papermill æ‰§è¡Œ Notebook
# - GPU åŠ é€Ÿç§‘å­¦è®¡ç®—
# - ç”Ÿæˆä¸­é—´ç»“æœæ–‡ä»¶
# - æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—è®°å½•

# èµ„æºéœ€æ±‚ï¼š
# - GPU: 1-4 ä¸ªï¼ˆå¯é…ç½®ï¼‰
# - å†…å­˜: 16-32GB
# - CPU: 4-8 æ ¸
# - å­˜å‚¨: é«˜æ€§èƒ½ NVMe
```

#### Task 3: ç»“æœè½¬æ¢å’ŒéªŒè¯

```yaml
# èŒè´£ï¼š
# - Jupyter nbconvert HTML è½¬æ¢
# - ç»“æœæ–‡ä»¶éªŒè¯
# - æ ¼å¼æ ‡å‡†åŒ–
# - å…ƒæ•°æ®æå–

# ç‰¹ç‚¹ï¼š
# - ä¸éœ€è¦ GPU èµ„æº
# - I/O å¯†é›†å‹æ“ä½œ
# - å¯ä»¥åœ¨æ™®é€šèŠ‚ç‚¹æ‰§è¡Œ
```

#### Task 4: æµ‹è¯•æ‰§è¡Œ

```yaml
# èŒè´£ï¼š
# - ä¸‹è½½æµ‹è¯•æ¡†æ¶ repo
# - é…ç½®æµ‹è¯•ç¯å¢ƒ
# - æ‰§è¡Œ pytest æµ‹è¯•å¥—ä»¶
# - ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š

# èµ„æºéœ€æ±‚ï¼š
# - ä¸­ç­‰è®¡ç®—èµ„æº
# - ç½‘ç»œè®¿é—®æƒé™
# - ä¸´æ—¶å­˜å‚¨ç©ºé—´
```

#### Task 5: æŠ¥å‘Šèšåˆå’Œå‘å¸ƒ

```yaml
# èŒè´£ï¼š
# - èšåˆæ‰€æœ‰æµ‹è¯•ç»“æœ
# - ç”Ÿæˆç»¼åˆæŠ¥å‘Š
# - ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨
# - è§¦å‘é€šçŸ¥æœåŠ¡

# ç‰¹ç‚¹ï¼š
# - è½»é‡çº§ä»»åŠ¡
# - å¤–éƒ¨æœåŠ¡é›†æˆ
# - æœ€ç»ˆçŠ¶æ€ç¡®è®¤
```

### 3. å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

```yaml
# å¹¶è¡Œæ‰§è¡Œç­–ç•¥
tasks:
- name: prepare-environment
  taskRef:
    name: env-preparation
  
- name: execute-notebook-gpu
  taskRef:
    name: notebook-execution-gpu
  runAfter: ["prepare-environment"]
  
- name: convert-results
  taskRef:
    name: result-conversion
  runAfter: ["execute-notebook-gpu"]
  
- name: download-test-framework
  taskRef:
    name: test-framework-setup
  runAfter: ["prepare-environment"]  # ä¸ GPU ä»»åŠ¡å¹¶è¡Œ
  
- name: execute-tests
  taskRef:
    name: test-execution
  runAfter: ["convert-results", "download-test-framework"]
  
- name: publish-results
  taskRef:
    name: result-publishing
  runAfter: ["execute-tests"]
```

## ğŸ’¾ å­˜å‚¨å’Œå·¥ä½œç©ºé—´

### 1. å­˜å‚¨å±‚è®¾è®¡

#### é«˜æ€§èƒ½å­˜å‚¨ï¼ˆGPU è®¡ç®—ç”¨ï¼‰

```yaml
# NVMe SSD å­˜å‚¨ç±»
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gpu-nvme-ssd
provisioner: kubernetes.io/aws-ebs  # æˆ–å…¶ä»–äº‘æä¾›å•†
parameters:
  type: gp3
  iops: "10000"
  throughput: "1000"
  fsType: ext4
volumeBindingMode: WaitForFirstConsumer
```

#### æŒä¹…åŒ–å­˜å‚¨ï¼ˆç»“æœä¿å­˜ï¼‰

```yaml
# ç»“æœæŒä¹…åŒ–å­˜å‚¨
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gpu-results-storage
provisioner: kubernetes.io/aws-efs  # ç½‘ç»œæ–‡ä»¶ç³»ç»Ÿ
parameters:
  provisioningMode: efs-utils
  directoryPerms: "0755"
  uid: "1000"
  gid: "1000"
```

### 2. å·¥ä½œç©ºé—´é…ç½®

#### GPU ç¼“å­˜å·¥ä½œç©ºé—´

```yaml
# GPU è®¡ç®—ç¼“å­˜ï¼Œæé«˜å¤ç”¨æ€§
- name: gpu-cache
  persistentVolumeClaim:
    claimName: gpu-cache-pvc
    storageClass: gpu-nvme-ssd
    accessModes: ["ReadWriteOnce"]
    resources:
      requests:
        storage: 500Gi
```

#### å…±äº«ç»“æœå·¥ä½œç©ºé—´

```yaml
# è·¨ Task å…±äº«çš„ç»“æœå­˜å‚¨
- name: shared-results
  persistentVolumeClaim:
    claimName: shared-results-pvc
    storageClass: gpu-results-storage
    accessModes: ["ReadWriteMany"]
    resources:
      requests:
        storage: 100Gi
```

## ğŸ³ å®¹å™¨é•œåƒç­–ç•¥

### 1. é•œåƒå±‚çº§è®¾è®¡

#### åŸºç¡€é•œåƒå±‚

```dockerfile
# åŸºç¡€ GPU è¿è¡Œæ—¶é•œåƒ
FROM nvidia/cuda:11.8-devel-ubuntu22.04

# ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# GPU è¿è¡Œæ—¶ç¯å¢ƒ
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility
```

#### ç§‘å­¦è®¡ç®—é•œåƒå±‚

```dockerfile
# ç§‘å­¦è®¡ç®—å·¥å…·é“¾
FROM gpu-base:latest

# Python ç§‘å­¦è®¡ç®—ç¯å¢ƒ
RUN pip install --no-cache-dir \
    jupyter \
    papermill \
    numpy \
    pandas \
    scipy \
    scikit-learn \
    torch \
    tensorflow-gpu \
    cupy-cuda118

# Jupyter ä¼˜åŒ–é…ç½®
COPY jupyter_config.py /etc/jupyter/
```

#### åº”ç”¨é•œåƒå±‚

```dockerfile
# å…·ä½“åº”ç”¨é•œåƒ
FROM scientific-computing:latest

# åº”ç”¨ç‰¹å®šä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# åº”ç”¨ä»£ç å’Œé…ç½®
COPY scripts/ /app/scripts/
WORKDIR /app
```

### 2. é•œåƒä¼˜åŒ–ç­–ç•¥

#### å¤šé˜¶æ®µæ„å»º

```dockerfile
# æ„å»ºé˜¶æ®µ
FROM scientific-computing:latest AS builder
COPY . /build
WORKDIR /build
RUN python setup.py bdist_wheel

# è¿è¡Œæ—¶é•œåƒ
FROM gpu-base:latest AS runtime
COPY --from=builder /build/dist/*.whl /tmp/
RUN pip install /tmp/*.whl && rm /tmp/*.whl
```

#### é•œåƒç¼“å­˜ç­–ç•¥

```yaml
# é•œåƒé¢„æ‹‰å– DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: gpu-image-puller
spec:
  selector:
    matchLabels:
      name: gpu-image-puller
  template:
    spec:
      nodeSelector:
        node-type: gpu-compute
      containers:
      - name: image-puller
        image: alpine:latest
        command: ["/bin/sh", "-c"]
        args:
        - |
          docker pull scientific-computing:latest
          docker pull notebook-executor:latest
          sleep infinity
```

## ğŸ“Š ç›‘æ§å’ŒæŠ¥å‘Š

### 1. GPU èµ„æºç›‘æ§

#### Prometheus ç›‘æ§é…ç½®

```yaml
# GPU æŒ‡æ ‡é‡‡é›†
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-monitoring-config
data:
  prometheus.yml: |
    scrape_configs:
    - job_name: 'gpu-metrics'
      kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['tekton-gpu-pipelines']
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
```

#### å…³é”®ç›‘æ§æŒ‡æ ‡

```yaml
# GPU åˆ©ç”¨ç‡ç›‘æ§
- name: gpu_utilization
  query: nvidia_gpu_utilization_percent
  description: GPU åˆ©ç”¨ç‡ç™¾åˆ†æ¯”

# GPU å†…å­˜ä½¿ç”¨
- name: gpu_memory_usage
  query: nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes * 100
  description: GPU å†…å­˜ä½¿ç”¨ç‡

# Pipeline æ‰§è¡Œæ—¶é—´
- name: pipeline_duration
  query: tekton_pipelinerun_duration_seconds
  description: Pipeline æ‰§è¡Œæ—¶é—´

# ä»»åŠ¡æˆåŠŸç‡
- name: task_success_rate
  query: rate(tekton_taskrun_status{status="success"}[5m])
  description: ä»»åŠ¡æˆåŠŸç‡
```

### 2. æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ

#### æŠ¥å‘Šèšåˆ Task

```yaml
# ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: generate-comprehensive-report
spec:
  description: ç”ŸæˆåŒ…å«æ€§èƒ½ã€è´¨é‡ã€èµ„æºä½¿ç”¨çš„ç»¼åˆæŠ¥å‘Š
  params:
  - name: pipeline-run-name
  - name: execution-time
  - name: gpu-utilization
  steps:
  - name: generate-report
    image: report-generator:latest
    script: |
      #!/bin/bash
      python /scripts/generate_report.py \
        --pipeline-run $(params.pipeline-run-name) \
        --execution-time $(params.execution-time) \
        --gpu-utilization $(params.gpu-utilization) \
        --output-format html,pdf,json
```

#### é€šçŸ¥é›†æˆ

```yaml
# Slack/Email é€šçŸ¥é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: notification-config
data:
  config.yaml: |
    notifications:
      slack:
        webhook_url: "${SLACK_WEBHOOK_URL}"
        channels:
          success: "#ci-success"
          failure: "#ci-alerts"
      email:
        smtp_server: "${SMTP_SERVER}"
        recipients:
          - team@company.com
          - devops@company.com
```

## ğŸ”’ å®‰å…¨å’Œæƒé™

### 1. RBAC é…ç½®

#### ServiceAccount æƒé™

```yaml
# GPU Pipeline ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: gpu-pipeline-sa
  namespace: tekton-gpu-pipelines

---
# ClusterRole for GPU access
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: gpu-pipeline-role
rules:
- apiGroups: [""]
  resources: ["pods", "persistentvolumeclaims"]
  verbs: ["get", "list", "create", "delete"]
- apiGroups: [""]
  resources: ["secrets", "configmaps"]
  verbs: ["get", "list"]
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]
```

### 2. é•œåƒå®‰å…¨

#### é•œåƒç­¾åéªŒè¯

```yaml
# Cosign é•œåƒç­¾åéªŒè¯
apiVersion: v1
kind: ConfigMap
metadata:
  name: image-security-policy
data:
  policy.yaml: |
    apiVersion: v1alpha1
    kind: ClusterImagePolicy
    metadata:
      name: gpu-images-policy
    spec:
      images:
      - glob: "registry.company.com/gpu/*"
      authorities:
      - keyless:
          url: "https://fulcio.sigstore.dev"
          identities:
          - issuer: "https://accounts.google.com"
            subject: "ci-system@company.com"
```

### 3. ç½‘ç»œå®‰å…¨

#### ç½‘ç»œç­–ç•¥

```yaml
# GPU Pipeline ç½‘ç»œéš”ç¦»
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: gpu-pipeline-netpol
  namespace: tekton-gpu-pipelines
spec:
  podSelector:
    matchLabels:
      app: gpu-pipeline
  policyTypes:
  - Ingress
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
  - to: []
    ports:
    - protocol: TCP
      port: 443  # HTTPS å¤–éƒ¨è®¿é—®
    - protocol: TCP
      port: 80   # HTTP å¤–éƒ¨è®¿é—®
```

## ğŸš€ éƒ¨ç½²å’Œè¿ç»´

### 1. æ¸è¿›å¼éƒ¨ç½²ç­–ç•¥

#### é˜¶æ®µ 1: åŸºç¡€è®¾æ–½å‡†å¤‡

```yaml
# éƒ¨ç½²æ£€æŸ¥æ¸…å•
infrastructure_checklist:
  - âœ… GPU Operator å®‰è£…
  - âœ… å­˜å‚¨ç±»é…ç½®
  - âœ… ç½‘ç»œç­–ç•¥é…ç½®
  - âœ… ç›‘æ§ç³»ç»Ÿéƒ¨ç½²
  - âœ… é•œåƒä»“åº“å‡†å¤‡
```

#### é˜¶æ®µ 2: Pipeline éƒ¨ç½²

```yaml
# Pipeline éƒ¨ç½²é¡ºåº
deployment_order:
  1. åŸºç¡€ Tasks éƒ¨ç½²
  2. GPU Tasks éƒ¨ç½²å’Œæµ‹è¯•
  3. å®Œæ•´ Pipeline é›†æˆæµ‹è¯•
  4. ç”Ÿäº§ç¯å¢ƒéªŒè¯
```

#### é˜¶æ®µ 3: æµé‡åˆ‡æ¢

```yaml
# ç°åº¦å‘å¸ƒç­–ç•¥
traffic_splitting:
  phase1: 10%  # å°è§„æ¨¡éªŒè¯
  phase2: 50%  # å¹¶è¡Œè¿è¡Œå¯¹æ¯”
  phase3: 100% # å®Œå…¨åˆ‡æ¢
```

### 2. è¿ç»´è‡ªåŠ¨åŒ–

#### è‡ªåŠ¨æ‰©ç¼©å®¹

```yaml
# GPU èŠ‚ç‚¹è‡ªåŠ¨æ‰©ç¼©å®¹
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: gpu-pipeline-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gpu-worker-pool
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: nvidia.com/gpu
      target:
        type: Utilization
        averageUtilization: 70
```

#### è‡ªåŠ¨æ¸…ç†

```yaml
# å®šæœŸæ¸…ç† CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: pipeline-cleanup
spec:
  schedule: "0 2 * * *"  # æ¯å¤©å‡Œæ™¨ 2 ç‚¹
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: cleanup-tool:latest
            command:
            - /bin/sh
            - -c
            - |
              # æ¸…ç†å®Œæˆçš„ PipelineRuns (ä¿ç•™æœ€è¿‘ 7 å¤©)
              tkn pipelinerun delete --keep 7
              
              # æ¸…ç†æœªä½¿ç”¨çš„ PVCs
              kubectl delete pvc --field-selector=status.phase=Available
```

## ğŸ“‹ è¿ç§»è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¾æ–½å‡†å¤‡ï¼ˆ2-3 å‘¨ï¼‰

#### å‘¨ 1-2: GPU é›†ç¾¤å‡†å¤‡
- [ ] GPU Operator éƒ¨ç½²å’Œé…ç½®
- [ ] å­˜å‚¨ç±»å’ŒæŒä¹…åŒ–å·é…ç½®
- [ ] ç½‘ç»œç­–ç•¥å’Œå®‰å…¨é…ç½®
- [ ] ç›‘æ§ç³»ç»Ÿé›†æˆ

#### å‘¨ 3: é•œåƒå’Œå·¥å…·é“¾å‡†å¤‡
- [ ] åŸºç¡€é•œåƒæ„å»ºå’Œä¼˜åŒ–
- [ ] é•œåƒä»“åº“é…ç½®å’Œå®‰å…¨ç­–ç•¥
- [ ] é•œåƒé¢„æ‹‰å–å’Œç¼“å­˜ç­–ç•¥
- [ ] å·¥å…·é“¾éªŒè¯æµ‹è¯•

### ç¬¬äºŒé˜¶æ®µï¼šPipeline å¼€å‘ï¼ˆ3-4 å‘¨ï¼‰

#### å‘¨ 1-2: æ ¸å¿ƒ Tasks å¼€å‘
- [ ] ç¯å¢ƒå‡†å¤‡ Task å¼€å‘å’Œæµ‹è¯•
- [ ] GPU Notebook æ‰§è¡Œ Task å¼€å‘
- [ ] ç»“æœè½¬æ¢ Task å¼€å‘
- [ ] æµ‹è¯•æ‰§è¡Œ Task å¼€å‘
- [ ] æŠ¥å‘Šç”Ÿæˆ Task å¼€å‘

#### å‘¨ 3-4: Pipeline é›†æˆ
- [ ] å®Œæ•´ Pipeline ç»„è£…å’Œé…ç½®
- [ ] å·¥ä½œç©ºé—´å’Œå­˜å‚¨é›†æˆ
- [ ] å‚æ•°åŒ–å’Œé…ç½®ç®¡ç†
- [ ] é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### ç¬¬ä¸‰é˜¶æ®µï¼šéªŒè¯å’Œä¼˜åŒ–ï¼ˆ2-3 å‘¨ï¼‰

#### å‘¨ 1-2: åŠŸèƒ½éªŒè¯
- [ ] å• Task åŠŸèƒ½æµ‹è¯•
- [ ] ç«¯åˆ°ç«¯ Pipeline æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ•…éšœæ¢å¤æµ‹è¯•

#### å‘¨ 3: æ€§èƒ½ä¼˜åŒ–
- [ ] GPU èµ„æºåˆ©ç”¨ç‡ä¼˜åŒ–
- [ ] å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–
- [ ] å­˜å‚¨ I/O ä¼˜åŒ–
- [ ] ç½‘ç»œä¼ è¾“ä¼˜åŒ–

### ç¬¬å››é˜¶æ®µï¼šç”Ÿäº§éƒ¨ç½²ï¼ˆ2-3 å‘¨ï¼‰

#### å‘¨ 1: ç°åº¦éƒ¨ç½²
- [ ] 10% æµé‡åˆ‡æ¢
- [ ] æ€§èƒ½å’Œç¨³å®šæ€§ç›‘æ§
- [ ] é—®é¢˜ä¿®å¤å’Œè°ƒä¼˜

#### å‘¨ 2: æ‰©å¤§éƒ¨ç½²
- [ ] 50% æµé‡åˆ‡æ¢
- [ ] å¹¶è¡Œè¿è¡Œå¯¹æ¯”éªŒè¯
- [ ] ç”¨æˆ·ä½“éªŒæ”¶é›†

#### å‘¨ 3: å…¨é‡åˆ‡æ¢
- [ ] 100% æµé‡åˆ‡æ¢
- [ ] åŸç³»ç»Ÿå¤‡ç”¨ä¿ç•™
- [ ] è¿ç»´æ–‡æ¡£å’ŒåŸ¹è®­

## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

### 1. èµ„æºæ•ˆç‡æå‡

- **GPU åˆ©ç”¨ç‡**: ä» 30-40% æå‡åˆ° 70-80%
- **å¹¶å‘å¤„ç†**: æ”¯æŒ 3-5 å€å¹¶å‘ä»»åŠ¡é‡
- **æˆæœ¬é™ä½**: GPU èµ„æºæˆæœ¬é™ä½ 40-50%

### 2. å¯é æ€§æå‡

- **æ•…éšœæ¢å¤**: è‡ªåŠ¨é‡è¯•å’Œæ•…éšœè½¬ç§»
- **å¯è§‚æµ‹æ€§**: å®Œæ•´çš„ç›‘æ§å’Œå‘Šè­¦
- **å¯ç»´æŠ¤æ€§**: æ ‡å‡†åŒ–çš„è¿ç»´æµç¨‹

### 3. å¼€å‘æ•ˆç‡æå‡

- **å¼€å‘å‘¨æœŸ**: CI/CD å‘¨æœŸç¼©çŸ­ 30-40%
- **åé¦ˆé€Ÿåº¦**: æ›´å¿«çš„æµ‹è¯•åé¦ˆ
- **æ‰©å±•æ€§**: æ›´å®¹æ˜“æ·»åŠ æ–°çš„è®¡ç®—ä»»åŠ¡

## ğŸ¯ æˆåŠŸæŒ‡æ ‡

### æŠ€æœ¯æŒ‡æ ‡

- GPU å¹³å‡åˆ©ç”¨ç‡ > 70%
- Pipeline æˆåŠŸç‡ > 95%
- å¹³å‡æ‰§è¡Œæ—¶é—´ < 30 åˆ†é’Ÿ
- æ•…éšœæ¢å¤æ—¶é—´ < 5 åˆ†é’Ÿ

### ä¸šåŠ¡æŒ‡æ ‡

- å¼€å‘è€…æ»¡æ„åº¦ > 4.5/5
- éƒ¨ç½²é¢‘ç‡æå‡ 2x
- é—®é¢˜è§£å†³æ—¶é—´å‡å°‘ 50%
- æ€»ä½“è¿ç»´æˆæœ¬é™ä½ 30%

---

**æ³¨æ„**: æœ¬æ–¹æ¡ˆåŸºäºç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µè®¾è®¡ï¼Œè€ƒè™‘äº†å¯æ‰©å±•æ€§ã€å¯é æ€§å’Œå®‰å…¨æ€§ã€‚å»ºè®®æ ¹æ®å®é™…ç¯å¢ƒå’Œéœ€æ±‚è¿›è¡Œé€‚å½“è°ƒæ•´ã€‚ 