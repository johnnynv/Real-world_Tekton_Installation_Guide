# GPU ç§‘å­¦è®¡ç®— Pipeline éƒ¨ç½²æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨ Tekton ä¸Šéƒ¨ç½² GPU åŠ é€Ÿçš„ç§‘å­¦è®¡ç®— Pipelineï¼Œå®ç°ä» GitHub Actions åˆ° Tekton çš„å®Œæ•´è¿ç§»ã€‚

## ğŸ“‹ éƒ¨ç½²ç›®æ ‡

- âœ… éƒ¨ç½² GPU æ‰§è¡Œç¯å¢ƒ
- âœ… é…ç½®ç§‘å­¦è®¡ç®— Tasks
- âœ… åˆ›å»ºå®Œæ•´çš„ GPU Pipeline
- âœ… é›†æˆ GitHub Webhook è§¦å‘
- âœ… éªŒè¯ç«¯åˆ°ç«¯æµç¨‹

## ğŸ”§ å‰ææ¡ä»¶

### ç³»ç»Ÿè¦æ±‚
- âœ… å·²å®Œæˆ [Tekton Webhook é…ç½®](03-tekton-webhook-configuration.md)
- âœ… Kubernetes é›†ç¾¤æ”¯æŒ GPU
- âœ… NVIDIA GPU Operator å·²å®‰è£…
- âœ… åŒ…å« Jupyter Notebook çš„ GitHub ä»“åº“

### GPU ç¯å¢ƒéªŒè¯
```bash
# æ£€æŸ¥ GPU èŠ‚ç‚¹
kubectl get nodes -l accelerator=nvidia-tesla-gpu

# æ£€æŸ¥ GPU èµ„æº
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu

# å¦‚æœæ²¡æœ‰ GPU æ ‡ç­¾ï¼Œè¯·æ·»åŠ ï¼š
kubectl label nodes <node-name> accelerator=nvidia-tesla-gpu
```

## ğŸš€ æ­¥éª¤1ï¼šéƒ¨ç½² GPU ç§‘å­¦è®¡ç®— Tasks

### åˆ›å»ºç¯å¢ƒå‡†å¤‡ Task
```bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: gpu-env-preparation
  namespace: tekton-pipelines
spec:
  description: Environment preparation for GPU scientific computing workflow
  params:
  - name: git-repo-url
    description: Git repository URL to clone
    type: string
  - name: git-revision
    description: Git revision to checkout
    type: string
    default: "main"
  workspaces:
  - name: source-code
    description: Workspace for source code checkout
    mountPath: /workspace/source
  - name: shared-storage
    description: Shared storage for artifacts
    mountPath: /workspace/shared
  results:
  - name: commit-sha
    description: SHA of the checked out commit
  - name: repo-status
    description: Repository checkout status
  steps:
  - name: git-clone
    image: alpine/git:latest
    env:
    - name: WORKSPACE_SOURCE_PATH
      value: \$(workspaces.source-code.path)
    - name: WORKSPACE_SHARED_PATH
      value: \$(workspaces.shared-storage.path)
    script: |
      #!/bin/sh
      set -eu
      
      echo "Starting GPU environment preparation..."
      echo "Repository URL: \$(params.git-repo-url)"
      echo "Revision: \$(params.git-revision)"
      
      # Create necessary directories
      mkdir -p "\${WORKSPACE_SOURCE_PATH}/source"
      mkdir -p "\${WORKSPACE_SHARED_PATH}/artifacts"
      mkdir -p "\${WORKSPACE_SHARED_PATH}/notebooks"
      
      # Clone repository
      cd "\${WORKSPACE_SOURCE_PATH}"
      git clone "\$(params.git-repo-url)" source
      cd source
      
      # Checkout specific revision
      if [ "\$(params.git-revision)" != "main" ]; then
        git checkout "\$(params.git-revision)"
      fi
      
      # Get commit information
      COMMIT_SHA=\$(git rev-parse HEAD)
      echo -n "\${COMMIT_SHA}" > "\$(results.commit-sha.path)"
      
      # Validate repository structure
      if [ -d "notebooks" ]; then
        echo "Found notebooks/ directory"
        ls -la notebooks/
      else
        echo "Warning: notebooks/ directory not found"
      fi
      
      # Copy files to shared workspace
      cp -r . "\${WORKSPACE_SHARED_PATH}/"
      echo -n "success" > "\$(results.repo-status.path)"
      echo "Environment preparation completed"
EOF
```

### åˆ›å»º GPU Papermill æ‰§è¡Œ Task
```bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: gpu-papermill-execution
  namespace: tekton-pipelines
  labels:
    tekton.dev/gpu-required: "true"
spec:
  description: GPU-accelerated Papermill execution for scientific computing
  params:
  - name: notebook-path
    description: Path to the input notebook file
    type: string
    default: "notebooks/01_scRNA_analysis_preprocessing.ipynb"
  - name: output-notebook-name
    description: Name for the output executed notebook
    type: string
    default: "executed_notebook.ipynb"
  - name: container-image
    description: GPU-enabled container image
    type: string
    default: "nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12"
  workspaces:
  - name: shared-storage
    description: Shared storage for source code and artifacts
    mountPath: /workspace/shared
  - name: gpu-cache
    description: GPU computation cache workspace
    mountPath: /workspace/gpu-cache
  results:
  - name: execution-status
    description: Status of notebook execution
  - name: execution-time
    description: Total execution time in seconds
  steps:
  - name: gpu-papermill-execute
    image: \$(params.container-image)
    computeResources:
      requests:
        nvidia.com/gpu: "1"
        memory: "16Gi"
        cpu: "4"
      limits:
        nvidia.com/gpu: "1"
        memory: "32Gi"
        cpu: "8"
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility"
    - name: WORKSPACE_SHARED_PATH
      value: \$(workspaces.shared-storage.path)
    - name: GPU_CACHE_PATH
      value: \$(workspaces.gpu-cache.path)
    script: |
      #!/bin/bash
      set -eu
      
      echo "Starting GPU-accelerated Papermill execution..."
      cd "\${WORKSPACE_SHARED_PATH}"
      
      # Record start time
      START_TIME=\$(date +%s)
      
      # Verify GPU availability
      if command -v nvidia-smi &> /dev/null; then
        echo "GPU available:"
        nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
      fi
      
      # Verify notebook exists
      if [ ! -f "\$(params.notebook-path)" ]; then
        echo "Error: Notebook not found: \$(params.notebook-path)"
        exit 1
      fi
      
      # Create output directory
      mkdir -p "artifacts"
      mkdir -p "\${GPU_CACHE_PATH}/papermill"
      
      # Install papermill if needed
      pip install --quiet papermill ipykernel jupyter
      
      # Set up GPU environment
      export CUDA_VISIBLE_DEVICES=\${NVIDIA_VISIBLE_DEVICES}
      export CUPY_CACHE_DIR="\${GPU_CACHE_PATH}/cupy"
      export NUMBA_CACHE_DIR="\${GPU_CACHE_PATH}/numba"
      mkdir -p "\${CUPY_CACHE_DIR}" "\${NUMBA_CACHE_DIR}"
      
      # Execute notebook with papermill
      echo "Executing notebook with Papermill..."
      papermill "\$(params.notebook-path)" "artifacts/\$(params.output-notebook-name)" \\
        --log-output \\
        --log-level DEBUG \\
        --progress-bar \\
        --report-mode \\
        --kernel python3 \\
        2>&1 | tee "artifacts/papermill.log"
      
      PAPERMILL_EXIT_CODE=\${PIPESTATUS[0]}
      
      # Calculate execution time
      END_TIME=\$(date +%s)
      EXECUTION_TIME=\$((END_TIME - START_TIME))
      echo -n "\${EXECUTION_TIME}" > "\$(results.execution-time.path)"
      
      # Check results
      if [ \$PAPERMILL_EXIT_CODE -eq 0 ] && [ -f "artifacts/\$(params.output-notebook-name)" ]; then
        echo "Papermill execution completed successfully"
        echo -n "success" > "\$(results.execution-status.path)"
      else
        echo "Papermill execution failed"
        echo -n "failed" > "\$(results.execution-status.path)"
        exit 1
      fi
EOF
```

### åˆ›å»º Jupyter nbconvert Task
```bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: jupyter-nbconvert
  namespace: tekton-pipelines
spec:
  description: Convert executed notebooks to HTML format
  params:
  - name: input-notebook-name
    description: Name of the input notebook file
    type: string
    default: "executed_notebook.ipynb"
  - name: output-html-name
    description: Name for the output HTML file
    type: string
    default: "executed_notebook.html"
  workspaces:
  - name: shared-storage
    description: Shared storage for notebooks and HTML output
    mountPath: /workspace/shared
  results:
  - name: conversion-status
    description: Status of HTML conversion
  steps:
  - name: convert-to-html
    image: jupyter/minimal-notebook:latest
    env:
    - name: WORKSPACE_SHARED_PATH
      value: \$(workspaces.shared-storage.path)
    script: |
      #!/bin/bash
      set -eu
      
      echo "Starting Jupyter nbconvert HTML conversion..."
      cd "\${WORKSPACE_SHARED_PATH}"
      
      if [ ! -f "artifacts/\$(params.input-notebook-name)" ]; then
        echo "Error: Input notebook not found"
        exit 1
      fi
      
      # Install nbconvert
      pip install --quiet nbconvert
      
      # Convert to HTML
      jupyter nbconvert --to html \\
        "artifacts/\$(params.input-notebook-name)" \\
        --output "\$(params.output-html-name)" \\
        --output-dir "artifacts" \\
        --embed-images \\
        > "artifacts/jupyter_nbconvert.log" 2>&1
      
      if [ -f "artifacts/\$(params.output-html-name)" ]; then
        echo "HTML conversion completed successfully"
        echo -n "success" > "\$(results.conversion-status.path)"
      else
        echo "HTML conversion failed"
        echo -n "failed" > "\$(results.conversion-status.path)"
        exit 1
      fi
  
  - name: prepare-for-testing
    image: alpine:latest
    env:
    - name: WORKSPACE_SHARED_PATH
      value: \$(workspaces.shared-storage.path)
    script: |
      #!/bin/sh
      set -eu
      
      echo "Preparing HTML file for testing..."
      cd "\${WORKSPACE_SHARED_PATH}"
      
      mkdir -p "artifacts/staging"
      
      if [ -f "artifacts/\$(params.output-html-name)" ]; then
        cp "artifacts/\$(params.output-html-name)" "artifacts/staging/"
        echo "HTML file prepared for testing"
      else
        echo "Error: HTML file not found"
        exit 1
      fi
EOF
```

### åˆ›å»º PyTest æ‰§è¡Œ Task
```bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: pytest-execution
  namespace: tekton-pipelines
spec:
  description: Execute PyTest against generated HTML reports
  params:
  - name: test-repo-url
    description: URL of the test framework repository
    type: string
    default: "https://github.com/NVIDIA-AI-Blueprints/blueprint-github-test.git"
  - name: html-input-file
    description: Name of the HTML file to test
    type: string
    default: "executed_notebook.html"
  - name: pytest-markers
    description: PyTest markers to run
    type: string
    default: "single_cell"
  workspaces:
  - name: shared-storage
    description: Shared storage for HTML files and test results
    mountPath: /workspace/shared
  - name: test-workspace
    description: Workspace for test repository
    mountPath: /workspace/test
  results:
  - name: test-status
    description: Overall test execution status
  - name: coverage-xml-path
    description: Path to coverage XML report
  - name: pytest-xml-path
    description: Path to pytest XML report
  - name: report-html-path
    description: Path to pytest HTML report
  steps:
  - name: download-test-repo
    image: alpine/git:latest
    env:
    - name: WORKSPACE_TEST_PATH
      value: \$(workspaces.test-workspace.path)
    script: |
      #!/bin/sh
      set -eu
      
      echo "Downloading test framework repository..."
      mkdir -p "\${WORKSPACE_TEST_PATH}"
      cd "\${WORKSPACE_TEST_PATH}"
      
      git clone --depth 1 \$(params.test-repo-url) test-framework
      cd test-framework
      
      if [ -d "input" ]; then
        echo "Found input directory"
      else
        mkdir -p input
      fi
  
  - name: prepare-test-inputs
    image: alpine:latest
    env:
    - name: WORKSPACE_SHARED_PATH
      value: \$(workspaces.shared-storage.path)
    - name: WORKSPACE_TEST_PATH
      value: \$(workspaces.test-workspace.path)
    script: |
      #!/bin/sh
      set -eu
      
      echo "Preparing test inputs..."
      cd "\${WORKSPACE_TEST_PATH}/test-framework"
      
      # Clear input directory
      rm -rf input/*
      
      # Copy HTML file to input directory
      if [ -f "\${WORKSPACE_SHARED_PATH}/artifacts/staging/\$(params.html-input-file)" ]; then
        cp "\${WORKSPACE_SHARED_PATH}/artifacts/staging/\$(params.html-input-file)" input/
        echo "HTML file copied to input directory"
      else
        echo "Error: HTML file not found"
        exit 1
      fi
  
  - name: execute-tests
    image: python:3.12-slim
    env:
    - name: WORKSPACE_TEST_PATH
      value: \$(workspaces.test-workspace.path)
    - name: WORKSPACE_SHARED_PATH
      value: \$(workspaces.shared-storage.path)
    workingDir: \$(workspaces.test-workspace.path)/test-framework
    script: |
      #!/bin/bash
      set -eu
      
      echo "Setting up test environment..."
      apt-get update && apt-get install -y curl git
      
      # Install Poetry
      curl -sSL https://install.python-poetry.org | python3 -
      export PATH="/root/.local/bin:\$PATH"
      
      # Install dependencies
      poetry install --no-dev
      
      # Execute tests
      echo "Running PyTest..."
      poetry run pytest -m \$(params.pytest-markers) \\
        --cov=./ \\
        --cov-report=xml:"\${WORKSPACE_SHARED_PATH}/artifacts/coverage.xml" \\
        --junitxml="\${WORKSPACE_SHARED_PATH}/artifacts/pytest_results.xml" \\
        --html="\${WORKSPACE_SHARED_PATH}/artifacts/pytest_report.html" \\
        --self-contained-html \\
        -v \\
        2>&1 | tee "\${WORKSPACE_SHARED_PATH}/artifacts/pytest_execution.log" || true
      
      # Save result paths
      echo -n "\${WORKSPACE_SHARED_PATH}/artifacts/coverage.xml" > "\$(results.coverage-xml-path.path)"
      echo -n "\${WORKSPACE_SHARED_PATH}/artifacts/pytest_results.xml" > "\$(results.pytest-xml-path.path)"
      echo -n "\${WORKSPACE_SHARED_PATH}/artifacts/pytest_report.html" > "\$(results.report-html-path.path)"
      echo -n "completed" > "\$(results.test-status.path)"
      
      echo "Test execution completed"
EOF
```

## ğŸ”— æ­¥éª¤2ï¼šåˆ›å»ºå®Œæ•´çš„ GPU Pipeline

```bash
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: Pipeline
metadata:
  name: gpu-scientific-computing-pipeline
  namespace: tekton-pipelines
spec:
  description: GPU-accelerated scientific computing pipeline
  params:
  - name: git-repo-url
    description: Git repository URL
    type: string
  - name: git-revision
    description: Git revision
    type: string
    default: "main"
  - name: notebook-path
    description: Path to the notebook file
    type: string
    default: "notebooks/01_scRNA_analysis_preprocessing.ipynb"
  - name: test-repo-url
    description: URL of the test framework repository
    type: string
    default: "https://github.com/NVIDIA-AI-Blueprints/blueprint-github-test.git"
  
  workspaces:
  - name: source-code-workspace
    description: Workspace for source code
  - name: shared-artifacts-workspace
    description: Shared workspace for artifacts
  - name: gpu-cache-workspace
    description: GPU computation cache
  - name: test-execution-workspace
    description: Workspace for test execution
  
  results:
  - name: notebook-execution-time
    description: Time taken for notebook execution
    value: "\$(tasks.execute-notebook-gpu.results.execution-time)"
  - name: test-results-summary
    description: Summary of test execution results
    value: "\$(tasks.run-tests.results.test-status)"
  
  tasks:
  # Environment preparation
  - name: prepare-environment
    taskRef:
      name: gpu-env-preparation
    params:
    - name: git-repo-url
      value: \$(params.git-repo-url)
    - name: git-revision
      value: \$(params.git-revision)
    workspaces:
    - name: source-code
      workspace: source-code-workspace
    - name: shared-storage
      workspace: shared-artifacts-workspace
  
  # GPU notebook execution
  - name: execute-notebook-gpu
    taskRef:
      name: gpu-papermill-execution
    runAfter: ["prepare-environment"]
    params:
    - name: notebook-path
      value: \$(params.notebook-path)
    workspaces:
    - name: shared-storage
      workspace: shared-artifacts-workspace
    - name: gpu-cache
      workspace: gpu-cache-workspace
  
  # Convert to HTML
  - name: convert-to-html
    taskRef:
      name: jupyter-nbconvert
    runAfter: ["execute-notebook-gpu"]
    workspaces:
    - name: shared-storage
      workspace: shared-artifacts-workspace
  
  # Run tests
  - name: run-tests
    taskRef:
      name: pytest-execution
    runAfter: ["convert-to-html"]
    params:
    - name: test-repo-url
      value: \$(params.test-repo-url)
    workspaces:
    - name: shared-storage
      workspace: shared-artifacts-workspace
    - name: test-workspace
      workspace: test-execution-workspace
EOF
```

## ğŸ¯ æ­¥éª¤3ï¼šé›†æˆ GitHub Webhook è§¦å‘

### åˆ›å»º GPU Pipeline TriggerTemplate
```bash
cat <<EOF | kubectl apply -f -
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerTemplate
metadata:
  name: gpu-pipeline-trigger-template
  namespace: tekton-pipelines
spec:
  params:
  - name: git-repo-url
    description: Git repository URL
  - name: git-revision
    description: Git revision
  - name: git-repo-name
    description: Git repository name
  resourcetemplates:
  - apiVersion: tekton.dev/v1
    kind: PipelineRun
    metadata:
      generateName: gpu-pipeline-run-
      labels:
        tekton.dev/pipeline: gpu-scientific-computing-pipeline
        git.repository: \$(tt.params.git-repo-name)
    spec:
      pipelineRef:
        name: gpu-scientific-computing-pipeline
      params:
      - name: git-repo-url
        value: \$(tt.params.git-repo-url)
      - name: git-revision
        value: \$(tt.params.git-revision)
      workspaces:
      - name: source-code-workspace
        volumeClaimTemplate:
          spec:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 10Gi
      - name: shared-artifacts-workspace
        volumeClaimTemplate:
          spec:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 20Gi
      - name: gpu-cache-workspace
        volumeClaimTemplate:
          spec:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 5Gi
      - name: test-execution-workspace
        volumeClaimTemplate:
          spec:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 5Gi
EOF
```

### åˆ›å»º GPU EventListener
```bash
cat <<EOF | kubectl apply -f -
apiVersion: triggers.tekton.dev/v1beta1
kind: EventListener
metadata:
  name: gpu-scientific-computing-eventlistener
  namespace: tekton-pipelines
spec:
  serviceAccountName: tekton-triggers-sa
  triggers:
  - name: gpu-pipeline-trigger
    interceptors:
    # GitHub webhook éªŒè¯
    - ref:
        name: "github"
      params:
      - name: "secretRef"
        value:
          secretName: github-webhook-secret
          secretKey: webhook-secret
      - name: "eventTypes"
        value: ["push"]
    
    # æ¡ä»¶è¿‡æ»¤
    - ref:
        name: "cel"
      params:
      - name: "filter"
        value: >
          (body.ref == 'refs/heads/main' || 
           body.ref == 'refs/heads/develop') &&
          (body.head_commit.message.contains('[gpu]') ||
           body.head_commit.message.contains('[notebook]') ||
           body.head_commit.modified.exists(f, f.contains('notebooks/')) ||
           body.head_commit.added.exists(f, f.contains('notebooks/')))
    
    bindings:
    - ref: github-trigger-binding
    
    template:
      ref: gpu-pipeline-trigger-template
EOF
```

## âœ… éªŒè¯å®Œæ•´éƒ¨ç½²

### 1. æ£€æŸ¥æ‰€æœ‰ç»„ä»¶çŠ¶æ€
```bash
# æ£€æŸ¥ Tasks
kubectl get tasks -n tekton-pipelines | grep gpu

# æ£€æŸ¥ Pipeline
kubectl get pipeline gpu-scientific-computing-pipeline -n tekton-pipelines

# æ£€æŸ¥ EventListener
kubectl get eventlistener gpu-scientific-computing-eventlistener -n tekton-pipelines

# è·å– EventListener è®¿é—®åœ°å€
kubectl get svc -n tekton-pipelines | grep gpu-scientific-computing-eventlistener
```

### 2. æ‰‹åŠ¨æµ‹è¯• Pipeline
```bash
# åˆ›å»ºæµ‹è¯• PipelineRun
cat <<EOF | kubectl apply -f -
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: gpu-manual-test-
  namespace: tekton-pipelines
spec:
  pipelineRef:
    name: gpu-scientific-computing-pipeline
  params:
  - name: git-repo-url
    value: "https://github.com/your-username/your-repo.git"  # æ›¿æ¢ä¸ºæ‚¨çš„ä»“åº“
  - name: git-revision
    value: "main"
  workspaces:
  - name: source-code-workspace
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
  - name: shared-artifacts-workspace
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 20Gi
  - name: gpu-cache-workspace
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
  - name: test-execution-workspace
    volumeClaimTemplate:
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Gi
EOF
```

### 3. ç›‘æ§æ‰§è¡Œè¿›åº¦
```bash
# æŸ¥çœ‹ PipelineRun çŠ¶æ€
kubectl get pipelineruns -n tekton-pipelines -w

# æŸ¥çœ‹å…·ä½“ä»»åŠ¡çŠ¶æ€
kubectl get taskruns -n tekton-pipelines

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
kubectl logs -f <pod-name> -n tekton-pipelines
```

### 4. Dashboard éªŒè¯
åœ¨ Tekton Dashboard ä¸­éªŒè¯ï¼š
- âœ… Pipelines é¡µé¢æ˜¾ç¤º GPU Pipeline
- âœ… PipelineRuns é¡µé¢æ˜¾ç¤ºæ‰§è¡Œå†å²
- âœ… TaskRuns é¡µé¢æ˜¾ç¤ºå„ä¸ªä»»åŠ¡çŠ¶æ€
- âœ… å¯ä»¥æŸ¥çœ‹ GPU ä»»åŠ¡çš„å®æ—¶æ—¥å¿—

## ğŸ‰ ç«¯åˆ°ç«¯éªŒè¯

### è§¦å‘å®Œæ•´æµç¨‹
```bash
# åœ¨æ‚¨çš„é¡¹ç›®ä»“åº“ä¸­æäº¤ä»£ç 
echo "Test GPU pipeline integration" >> README.md
git add README.md
git commit -m "Test GPU scientific computing pipeline [gpu]"
git push origin main
```

### éªŒè¯ç”Ÿæˆçš„æ–‡ä»¶
Pipeline æ‰§è¡Œå®Œæˆåï¼Œåº”ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼š
- âœ… `executed_notebook.ipynb` - æ‰§è¡Œåçš„ Notebook
- âœ… `executed_notebook.html` - HTML æ ¼å¼æŠ¥å‘Š
- âœ… `coverage.xml` - ä»£ç è¦†ç›–ç‡æŠ¥å‘Š
- âœ… `pytest_results.xml` - JUnit æ ¼å¼æµ‹è¯•ç»“æœ
- âœ… `pytest_report.html` - HTML æ ¼å¼æµ‹è¯•æŠ¥å‘Š

## ğŸ”§ æ•…éšœæ’é™¤

### GPU è°ƒåº¦é—®é¢˜
```bash
# æ£€æŸ¥ GPU èŠ‚ç‚¹
kubectl get nodes -l accelerator=nvidia-tesla-gpu

# æ£€æŸ¥ GPU èµ„æº
kubectl describe node <gpu-node-name> | grep nvidia.com/gpu

# æŸ¥çœ‹ GPU Pod è°ƒåº¦
kubectl get pods -n tekton-pipelines -o wide | grep gpu
```

### Pipeline æ‰§è¡Œå¤±è´¥
```bash
# æŸ¥çœ‹ Pipeline çŠ¶æ€
kubectl describe pipelinerun <pipeline-run-name> -n tekton-pipelines

# æŸ¥çœ‹å¤±è´¥çš„ Task æ—¥å¿—
kubectl logs <failed-pod-name> -n tekton-pipelines

# æ£€æŸ¥å·¥ä½œç©ºé—´é…ç½®
kubectl get pvc -n tekton-pipelines
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### GPU ä½¿ç”¨ç‡ç›‘æ§
```bash
# åœ¨ GPU æ‰§è¡ŒæœŸé—´æŸ¥çœ‹ä½¿ç”¨ç‡
kubectl exec -it <gpu-pod-name> -n tekton-pipelines -- nvidia-smi

# æŸ¥çœ‹ GPU å†…å­˜ä½¿ç”¨
kubectl top pod <gpu-pod-name> -n tekton-pipelines --containers
```

## ğŸŠ éƒ¨ç½²å®Œæˆ

æ­å–œï¼æ‚¨å·²æˆåŠŸå®Œæˆä» GitHub Actions åˆ° Tekton GPU ç§‘å­¦è®¡ç®—å·¥ä½œæµçš„å®Œæ•´è¿ç§»ï¼š

1. âœ… **Tekton æ ¸å¿ƒç»„ä»¶** - å®‰è£…å’Œé…ç½®å®Œæˆ
2. âœ… **Tekton Triggers** - äº‹ä»¶é©±åŠ¨æœºåˆ¶å°±ç»ª
3. âœ… **GitHub Webhooks** - è‡ªåŠ¨è§¦å‘é…ç½®å®Œæˆ
4. âœ… **GPU Pipeline** - ç§‘å­¦è®¡ç®—å·¥ä½œæµéƒ¨ç½²æˆåŠŸ

ç°åœ¨æ¯æ¬¡æ¨é€ä»£ç åˆ° GitHubï¼Œéƒ½ä¼šè‡ªåŠ¨è§¦å‘ GPU åŠ é€Ÿçš„ç§‘å­¦è®¡ç®—æµç¨‹ï¼Œç”Ÿæˆä¸åŸ GitHub Actions ç›¸åŒçš„è¾“å‡ºæ–‡ä»¶ï¼ 