# Rocky Linux 10 å•èŠ‚ç‚¹ Kubernetes é›†ç¾¤å®‰è£…æŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•åœ¨ Rocky Linux 10 ç³»ç»Ÿä¸Šå®‰è£…å•èŠ‚ç‚¹ Kubernetes é›†ç¾¤ï¼Œæ”¯æŒ GPU å·¥ä½œè´Ÿè½½ã€‚

## ğŸ“‹ æŠ€æœ¯æ–¹æ¡ˆ

- **Kubernetes**: kubeadm v1.31.12
- **å®¹å™¨è¿è¡Œæ—¶**: containerd v1.7.27
- **ç½‘ç»œæ’ä»¶**: Calico v3.29.2
- **å­˜å‚¨**: Local Path Provisioner v0.0.30
- **ç›‘æ§**: Metrics Server + NVIDIA DCGM Exporter
- **ä»ªè¡¨æ¿**: Kubernetes Dashboard v2.7.0
- **GPUæ”¯æŒ**: NVIDIA GPU Operator
- **åŒ…ç®¡ç†**: Helm v3.18.5

## ğŸ æ­¥éª¤1ï¼šç³»ç»Ÿç¯å¢ƒæ£€æŸ¥

```bash
# æ£€æŸ¥æ“ä½œç³»ç»Ÿç‰ˆæœ¬
cat /etc/os-release
# é¢„æœŸè¾“å‡ºï¼šRocky Linux 10.0 (Red Quartz)

# æ£€æŸ¥ç¡¬ä»¶èµ„æº
free -h
nproc
df -h /

# æ£€æŸ¥ç½‘ç»œå’Œä¸»æœºå
hostname
hostname -I

# æ£€æŸ¥é˜²ç«å¢™çŠ¶æ€
systemctl status firewalld
# é¢„æœŸçŠ¶æ€ï¼šinactive (disabled)

# æ£€æŸ¥SELinuxçŠ¶æ€
getenforce
# é¢„æœŸè¾“å‡ºï¼šDisabled

# æ£€æŸ¥swapçŠ¶æ€
swapon --show
free -h
# é¢„æœŸï¼šSwapä¸º0B (å·²å…³é—­)
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… Rocky Linux 10.0 ç³»ç»Ÿ
- âœ… å†…å­˜ 502GBï¼ŒCPU 32æ ¸ï¼Œç£ç›˜ 3TB
- âœ… é˜²ç«å¢™å·²å…³é—­
- âœ… SELinuxå·²ç¦ç”¨
- âœ… Swapå·²å…³é—­

## ğŸ”§ æ­¥éª¤2ï¼šç³»ç»Ÿå‰ç½®é…ç½®

### é…ç½®å†…æ ¸æ¨¡å—

```bash
# é…ç½®éœ€è¦çš„å†…æ ¸æ¨¡å—
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

# åŠ è½½å†…æ ¸æ¨¡å—
sudo modprobe overlay
sudo modprobe br_netfilter

# éªŒè¯æ¨¡å—åŠ è½½
lsmod | grep -E "(overlay|br_netfilter)"
```

### é…ç½®å†…æ ¸å‚æ•°

```bash
# é…ç½®ç½‘ç»œå‚æ•°
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# åº”ç”¨é…ç½®
sudo sysctl --system

# éªŒè¯å…³é”®å‚æ•°
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… overlay å’Œ br_netfilter æ¨¡å—å·²åŠ è½½
- âœ… ç½‘ç»œè½¬å‘å‚æ•°å·²å¯ç”¨

## ğŸ³ æ­¥éª¤3ï¼šå®‰è£…containerdå®¹å™¨è¿è¡Œæ—¶

### æ·»åŠ Dockerä»“åº“

```bash
# æ·»åŠ Dockerä»“åº“ï¼ˆåŒ…å«containerdï¼‰
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

# æ›´æ–°ç¼“å­˜
sudo dnf makecache

# æœç´¢containerd
sudo dnf search containerd
```

### å®‰è£…containerd

```bash
# å®‰è£…containerd.io
sudo dnf install -y containerd.io

# ç”Ÿæˆé»˜è®¤é…ç½®
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

# é…ç½®systemd cgroupé©±åŠ¨
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# éªŒè¯é…ç½®
grep -n "SystemdCgroup" /etc/containerd/config.toml
```

### å¯åŠ¨containerdæœåŠ¡

```bash
# å¯åŠ¨å¹¶å¯ç”¨æœåŠ¡
sudo systemctl enable --now containerd

# éªŒè¯çŠ¶æ€
systemctl status containerd --no-pager
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… containerd v1.7.27 å®‰è£…æˆåŠŸ
- âœ… systemd cgroup é©±åŠ¨å·²é…ç½®
- âœ… æœåŠ¡è¿è¡Œæ­£å¸¸

## âš™ï¸ æ­¥éª¤4ï¼šå®‰è£…Kuberneteså·¥å…·

### æ·»åŠ Kubernetesä»“åº“

```bash
# æ·»åŠ Kuberneteså®˜æ–¹ä»“åº“
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.31/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.31/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF
```

### å®‰è£…Kubernetesç»„ä»¶

```bash
# æ›´æ–°ä»“åº“ç¼“å­˜
sudo dnf makecache

# å®‰è£…kubeadmã€kubeletã€kubectl
sudo dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

# å¯ç”¨kubeletæœåŠ¡
sudo systemctl enable kubelet

# éªŒè¯ç‰ˆæœ¬
kubeadm version
kubelet --version
kubectl version --client
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… kubeadm v1.31.12
- âœ… kubelet v1.31.12
- âœ… kubectl v1.31.12

## ğŸ¯ æ­¥éª¤5ï¼šåˆå§‹åŒ–Kubernetesé›†ç¾¤

### é›†ç¾¤åˆå§‹åŒ–

```bash
# åˆå§‹åŒ–é›†ç¾¤
sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/containerd/containerd.sock

# é…ç½®kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# ç§»é™¤masterèŠ‚ç‚¹æ±¡ç‚¹ï¼ˆå•èŠ‚ç‚¹é›†ç¾¤ï¼‰
kubectl taint nodes --all node-role.kubernetes.io/control-plane-

# éªŒè¯é›†ç¾¤çŠ¶æ€
kubectl get nodes
kubectl get pods -A
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… é›†ç¾¤åˆå§‹åŒ–æˆåŠŸ
- âœ… èŠ‚ç‚¹çŠ¶æ€ä¸º NotReadyï¼ˆéœ€è¦ç½‘ç»œæ’ä»¶ï¼‰
- âœ… æ§åˆ¶å¹³é¢ç»„ä»¶è¿è¡Œæ­£å¸¸

## ğŸŒ æ­¥éª¤6ï¼šå®‰è£…Calicoç½‘ç»œæ’ä»¶

### å®‰è£…Tigera Operator

```bash
# å®‰è£…Calico operator
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/tigera-operator.yaml
```

### é…ç½®Calicoå®‰è£…

```bash
# åˆ›å»ºCalicoå®‰è£…é…ç½®
cat <<EOF | kubectl create -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 192.168.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF

# ç­‰å¾…å®‰è£…å®Œæˆ
kubectl wait --for=condition=Ready --timeout=300s installation/default

# éªŒè¯å®‰è£…
kubectl get pods -n calico-system
kubectl get nodes
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… Calicoç½‘ç»œæ’ä»¶å®‰è£…æˆåŠŸ
- âœ… èŠ‚ç‚¹çŠ¶æ€å˜ä¸º Ready
- âœ… ç½‘ç»œè¿é€šæ€§æ­£å¸¸

## ğŸ’¾ æ­¥éª¤7ï¼šå®‰è£…Local Path Provisionerå­˜å‚¨

### å®‰è£…å­˜å‚¨provisioner

```bash
# å®‰è£…Local Path Provisioner
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml

# è®¾ç½®ä¸ºé»˜è®¤å­˜å‚¨ç±»
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# éªŒè¯å®‰è£…
kubectl get storageclass
kubectl get pods -n local-path-storage
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… local-path è®¾ä¸ºé»˜è®¤å­˜å‚¨ç±»
- âœ… provisioner è¿è¡Œæ­£å¸¸

## ğŸ“Š æ­¥éª¤8ï¼šå®‰è£…Metrics Serverç›‘æ§

### å®‰è£…Metrics Server

```bash
# å®‰è£…Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# ä¿®å¤å•èŠ‚ç‚¹é›†ç¾¤TLSé—®é¢˜
kubectl patch deployment metrics-server -n kube-system --patch='
spec:
  template:
    spec:
      containers:
      - name: metrics-server
        args:
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
'

# éªŒè¯åŠŸèƒ½
kubectl get pods -n kube-system -l k8s-app=metrics-server
kubectl top nodes
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… Metrics Serverè¿è¡Œæ­£å¸¸
- âœ… èƒ½å¤Ÿè·å–èŠ‚ç‚¹èµ„æºæŒ‡æ ‡

## ğŸ–¥ï¸ æ­¥éª¤9ï¼šå®‰è£…Kubernetes Dashboard

### å®‰è£…Dashboard

```bash
# å®‰è£…Dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

# éªŒè¯å®‰è£…
kubectl get pods -n kubernetes-dashboard
```

### è®¿é—®Dashboard

```bash
# è·å–è®¿é—®Token
kubectl -n kubernetes-dashboard create token admin-user

# å¯åŠ¨ç«¯å£è½¬å‘
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8443:443
```

**è®¿é—®ä¿¡æ¯**ï¼š
- **URL**: https://localhost:8443
- **Token**: é€šè¿‡ä¸Šè¿°å‘½ä»¤è·å–
- **ç”¨æˆ·**: admin-user

## ğŸ“¦ æ­¥éª¤10ï¼šå®‰è£…HelmåŒ…ç®¡ç†å™¨

```bash
# å®‰è£…Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# éªŒè¯ç‰ˆæœ¬
helm version
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… Helm v3.18.5 å®‰è£…æˆåŠŸ

## ğŸ® æ­¥éª¤11ï¼šå®‰è£…NVIDIA GPU Operator

### æ·»åŠ NVIDIA Helmä»“åº“

```bash
# æ·»åŠ NVIDIAä»“åº“
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update
```

### å®‰è£…GPU Operator

```bash
# å®‰è£…GPU Operatorï¼ˆç¦ç”¨é©±åŠ¨å®‰è£…ï¼‰
helm install --wait gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator \
  --create-namespace \
  --set driver.enabled=false

# éªŒè¯å®‰è£…
kubectl get pods -n gpu-operator
```

### éªŒè¯GPUèµ„æº

```bash
# æ£€æŸ¥GPUèµ„æº
kubectl describe node | grep -A 5 -B 5 nvidia.com/gpu

# æ£€æŸ¥GPUåˆ†é…
kubectl get nodes -o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu
```

**éªŒè¯ç»“æœ**ï¼š
- âœ… è¯†åˆ«åˆ°4ä¸ªGPUè®¾å¤‡
- âœ… GPU Operatoræ‰€æœ‰ç»„ä»¶è¿è¡Œæ­£å¸¸
- âœ… NVIDIA DCGM Exporterè‡ªåŠ¨å®‰è£…

## ğŸ“ˆ æ­¥éª¤12ï¼šéªŒè¯DCGMç›‘æ§æŒ‡æ ‡

### è®¿é—®GPUæŒ‡æ ‡

```bash
# ç«¯å£è½¬å‘DCGM Exporter
kubectl port-forward -n gpu-operator service/nvidia-dcgm-exporter 9400:9400 &

# æµ‹è¯•æŒ‡æ ‡è·å–
curl -s http://localhost:9400/metrics | grep -E "(DCGM_FI_DEV_GPU_UTIL|DCGM_FI_DEV_MEM_COPY_UTIL)" | head -5
```

**DCGMæŒ‡æ ‡è®¿é—®**ï¼š
- **URL**: http://localhost:9400/metrics
- **ä¸»è¦æŒ‡æ ‡**: GPUåˆ©ç”¨ç‡ã€å†…å­˜ä½¿ç”¨ã€æ¸©åº¦ç­‰

## âœ… æ­¥éª¤13ï¼šå®Œæ•´ç³»ç»ŸéªŒè¯

### é›†ç¾¤çŠ¶æ€éªŒè¯

```bash
echo "ğŸ” å®Œæ•´ç³»ç»ŸéªŒè¯"
echo "========================="

# 1. èŠ‚ç‚¹çŠ¶æ€
echo "1. èŠ‚ç‚¹çŠ¶æ€ï¼š"
kubectl get nodes

# 2. æ‰€æœ‰PodçŠ¶æ€
echo "2. å…³é”®PodçŠ¶æ€ï¼š"
kubectl get pods -A --field-selector=status.phase!=Succeeded | grep -v Completed

# 3. GPUèµ„æº
echo "3. GPUèµ„æºåˆ†é…ï¼š"
kubectl get nodes -o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu

# 4. å­˜å‚¨ç±»
echo "4. å­˜å‚¨ç±»ï¼š"
kubectl get storageclass

# 5. ç½‘ç»œæ’ä»¶
echo "5. ç½‘ç»œæ’ä»¶ï¼š"
kubectl get pods -n calico-system

echo "========================="
echo "ğŸ‰ ç³»ç»ŸéªŒè¯å®Œæˆï¼"
```

## ğŸš€ è®¿é—®ä¿¡æ¯æ±‡æ€»

### Dashboardå’Œç›‘æ§è®¿é—®

#### ğŸŒ **é€šè¿‡nip.ioåŸŸåè®¿é—®ï¼ˆæ¨èï¼‰**

##### Dashboardè®¿é—®
```bash
# åˆ›å»ºNodePortæœåŠ¡
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-dashboard-nodeport
  namespace: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
spec:
  type: NodePort
  ports:
  - port: 443
    targetPort: 8443
    nodePort: 30443
    protocol: TCP
    name: https
  selector:
    k8s-app: kubernetes-dashboard
EOF

# å®‰è£…nginxåå‘ä»£ç†
sudo dnf install -y nginx

# åˆ›å»ºSSLè¯ä¹¦
sudo mkdir -p /etc/nginx/ssl
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout /etc/nginx/ssl/dashboard.key \
  -out /etc/nginx/ssl/dashboard.crt \
  -subj "/C=CN/ST=State/L=City/O=Organization/CN=dashboard.$(hostname -I | awk '{print $1}').nip.io"

# é…ç½®nginx
sudo bash -c 'cat > /etc/nginx/conf.d/kubernetes-dashboard.conf << "EOF"
server {
    listen 443 ssl;
    server_name dashboard.$(hostname -I | awk "{print $1}").nip.io;

    ssl_certificate /etc/nginx/ssl/dashboard.crt;
    ssl_certificate_key /etc/nginx/ssl/dashboard.key;
    ssl_protocols TLSv1.2 TLSv1.3;

    location / {
        proxy_pass https://127.0.0.1:30443;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_ssl_verify off;
    }
}

server {
    listen 80;
    server_name dashboard.$(hostname -I | awk "{print $1}").nip.io;
    return 301 https://$server_name$request_uri;
}

# GPUç›‘æ§æœåŠ¡
server {
    listen 80;
    server_name gpu-metrics.$(hostname -I | awk "{print $1}").nip.io;

    location / {
        proxy_pass http://127.0.0.1:30400;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
EOF'

# å¯åŠ¨nginx
sudo systemctl enable --now nginx

# è·å–è®¿é—®Token
kubectl -n kubernetes-dashboard create token admin-user
```

##### GPUç›‘æ§è®¿é—®
```bash
# åˆ›å»ºDCGM Exporter NodePortæœåŠ¡
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: nvidia-dcgm-exporter-nodeport
  namespace: gpu-operator
  labels:
    app: nvidia-dcgm-exporter
spec:
  type: NodePort
  ports:
  - port: 9400
    targetPort: 9400
    nodePort: 30400
    protocol: TCP
    name: metrics
  selector:
    app: nvidia-dcgm-exporter
EOF
```

#### ğŸ“± **è®¿é—®é“¾æ¥**

- **Kubernetes Dashboard**: https://dashboard.10.78.14.61.nip.io
- **GPUç›‘æ§æŒ‡æ ‡**: http://gpu-metrics.10.78.14.61.nip.io/metrics  
- **Tokenè·å–**: `kubectl -n kubernetes-dashboard create token admin-user`

#### ğŸ”§ **æœ¬åœ°ç«¯å£è½¬å‘è®¿é—®ï¼ˆå¤‡é€‰ï¼‰**
```bash
# Dashboardç«¯å£è½¬å‘
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8443:443 &

# GPUç›‘æ§ç«¯å£è½¬å‘  
kubectl port-forward -n gpu-operator service/nvidia-dcgm-exporter 9400:9400 &
```

- **Dashboard URL**: https://localhost:8443
- **DCGMæŒ‡æ ‡URL**: http://localhost:9400/metrics

## ğŸ“‹ å®‰è£…ç»“æœæ‘˜è¦

### âœ… æˆåŠŸå®‰è£…çš„ç»„ä»¶
1. **Kubernetesé›†ç¾¤**: v1.31.12 å•èŠ‚ç‚¹é›†ç¾¤
2. **å®¹å™¨è¿è¡Œæ—¶**: containerd v1.7.27
3. **ç½‘ç»œæ’ä»¶**: Calico v3.29.2
4. **å­˜å‚¨**: Local Path Provisionerï¼ˆé»˜è®¤å­˜å‚¨ç±»ï¼‰
5. **ç›‘æ§**: Metrics Server + NVIDIA DCGM Exporter
6. **ä»ªè¡¨æ¿**: Kubernetes Dashboard v2.7.0
7. **GPUæ”¯æŒ**: NVIDIA GPU Operator
8. **åŒ…ç®¡ç†**: Helm v3.18.5

### ğŸ“Š é›†ç¾¤è§„æ ¼
- **èŠ‚ç‚¹æ•°é‡**: 1ä¸ªï¼ˆcontrol-plane + workerï¼‰
- **GPUè®¾å¤‡**: 4x NVIDIA Graphics Device
- **æ€»æ˜¾å­˜**: 716GBï¼ˆ4x179GBï¼‰
- **è®¡ç®—èƒ½åŠ›**: 10.0ï¼ˆæ”¯æŒæœ€æ–°CUDAç‰¹æ€§ï¼‰
- **ç½‘ç»œ**: Calico VXLANï¼ŒPod CIDR 192.168.0.0/16
- **å­˜å‚¨**: æœ¬åœ°è·¯å¾„å­˜å‚¨ï¼Œæ”¯æŒåŠ¨æ€åˆ†é…

### ğŸ¯ åç»­é›†æˆå‡†å¤‡
è¯¥é›†ç¾¤å·²ä¸ºä»¥ä¸‹å·¥ä½œè´Ÿè½½åšå¥½å‡†å¤‡ï¼š
- **Tekton Pipeline**: GPUåŠ é€Ÿçš„CI/CDå·¥ä½œæµ
- **ç§‘å­¦è®¡ç®—**: PyTorchã€TensorFlowã€RAPIDSç­‰
- **æœºå™¨å­¦ä¹ **: æ¨¡å‹è®­ç»ƒå’Œæ¨ç†å·¥ä½œè´Ÿè½½
- **å®¹å™¨åŒ–åº”ç”¨**: æ”¯æŒGPUçš„å®¹å™¨åº”ç”¨

## ğŸ› ï¸ ä¸€é”®å®‰è£…è„šæœ¬

### å¿«é€Ÿå®‰è£…
```bash
# ä¸‹è½½å¹¶è¿è¡Œå®‰è£…è„šæœ¬
curl -O https://raw.githubusercontent.com/your-repo/scripts/install-k8s-gpu.sh
chmod +x install-k8s-gpu.sh
./install-k8s-gpu.sh
```

### å®Œæ•´å¸è½½
```bash
# ä¸‹è½½å¹¶è¿è¡Œå¸è½½è„šæœ¬
curl -O https://raw.githubusercontent.com/your-repo/scripts/uninstall-k8s-gpu.sh
chmod +x uninstall-k8s-gpu.sh
./uninstall-k8s-gpu.sh
```

## ğŸ“š ä¸‹ä¸€æ­¥

é›†ç¾¤å®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥ç»§ç»­ï¼š
1. [Tekton æ ¸å¿ƒç»„ä»¶å®‰è£…](04-tekton-installation.md)
2. [Tekton Triggers é…ç½®](05-tekton-triggers-setup.md)
3. [GPU Pipeline éƒ¨ç½²](07-gpu-pipeline-deployment.md)

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **èŠ‚ç‚¹NotReady**: æ£€æŸ¥ç½‘ç»œæ’ä»¶å®‰è£…çŠ¶æ€
2. **Podæ‹‰å–é•œåƒå¤±è´¥**: æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒDNSé…ç½®
3. **GPUèµ„æºæœªè¯†åˆ«**: éªŒè¯GPU Operatorå’Œé©±åŠ¨çŠ¶æ€
4. **Dashboardæ— æ³•è®¿é—®**: æ£€æŸ¥ç«¯å£è½¬å‘å’ŒTokenæœ‰æ•ˆæ€§

### è¯Šæ–­å‘½ä»¤
```bash
# èŠ‚ç‚¹è¯Šæ–­
kubectl describe nodes

# Podè¯Šæ–­
kubectl get pods -A
kubectl describe pod <pod-name> -n <namespace>

# GPUè¯Šæ–­
kubectl get pods -n gpu-operator
kubectl logs -n gpu-operator <gpu-operator-pod>

# ç½‘ç»œè¯Šæ–­
kubectl get pods -n calico-system
kubectl logs -n calico-system <calico-pod>
```

## ğŸ“– å‚è€ƒèµ„æ–™

- [Kuberneteså®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/)
- [Calicoç½‘ç»œæ’ä»¶](https://docs.tigera.io/calico/)
- [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/)
- [Local Path Provisioner](https://github.com/rancher/local-path-provisioner)
- [Kubernetes Dashboard](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)

---

# Kubernetes v1.30 + Tekton v0.61.x ç”Ÿäº§ç¯å¢ƒå®‰è£…è®°å½•

æœ¬ç« èŠ‚è®°å½•åŸºäºç”Ÿäº§ç¯å¢ƒæ¨èçš„ç‰ˆæœ¬ç»„åˆè¿›è¡Œçš„å…¨æ–°å®‰è£…è¿‡ç¨‹ã€‚

## ğŸ¯ ç‰ˆæœ¬é€‰æ‹©ç­–ç•¥

### æ¨èç‰ˆæœ¬ç»„åˆ
- **Kubernetes**: v1.30.14 (æœ€æ–°ç¨³å®šç‰ˆ)
- **Tekton Pipelines**: v0.61.x (è®¡åˆ’å®‰è£…)
- **å®¹å™¨è¿è¡Œæ—¶**: containerd v1.7.27
- **ç½‘ç»œæ’ä»¶**: Calico v3.29.2

### ç‰ˆæœ¬é€‰æ‹©ç†ç”±
1. **K8s v1.30.14**: 
   - å½“å‰æœ€æ–°ç¨³å®šç‰ˆæœ¬ï¼ŒåŠŸèƒ½å®Œå–„ï¼Œå®‰å…¨æ€§å¥½
   - æ»¡è¶³Tekton v0.61.xçš„æœ€ä½è¦æ±‚(K8s 1.28+)
   - ç”Ÿäº§ç¯å¢ƒå¹¿æ³›éªŒè¯

2. **Tekton v0.61.x**:
   - æ”¯æŒæœ€æ–°çš„CI/CDåŠŸèƒ½å’Œå®‰å…¨ç‰¹æ€§
   - ä¸K8s v1.30å®Œå…¨å…¼å®¹
   - é€‚åˆä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒ

## ğŸ æ­¥éª¤1ï¼šç³»ç»Ÿç¯å¢ƒéªŒè¯

### æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯
```bash
# æ£€æŸ¥æ“ä½œç³»ç»Ÿç‰ˆæœ¬
cat /etc/os-release
```

**è¾“å‡ºç»“æœ**:
```
NAME="Rocky Linux"
VERSION="10.0 (Red Quartz)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="10.0"
PLATFORM_ID="platform:el10"
PRETTY_NAME="Rocky Linux 10.0 (Red Quartz)"
```

### æ£€æŸ¥ç¡¬ä»¶èµ„æº
```bash
# æ£€æŸ¥å†…å­˜çŠ¶æ€
free -h
# è·å–ä¸»æœºIP
hostname -I
```

**éªŒè¯ç»“æœ**:
- âœ… Rocky Linux 10.0 ç³»ç»Ÿ
- âœ… å†…å­˜ 502GBï¼Œè¶³å¤Ÿè¿è¡Œå¤§è§„æ¨¡å·¥ä½œè´Ÿè½½
- âœ… ä¸»æœºIP: 10.78.14.61

## ğŸ”§ æ­¥éª¤2ï¼šç³»ç»Ÿå‰ç½®é…ç½®

### é…ç½®å†…æ ¸æ¨¡å—
```bash
# é…ç½®K8séœ€è¦çš„å†…æ ¸æ¨¡å—
sudo bash -c 'cat <<EOF > /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF'

# åŠ è½½å†…æ ¸æ¨¡å—
sudo modprobe overlay && sudo modprobe br_netfilter

# éªŒè¯æ¨¡å—åŠ è½½çŠ¶æ€
lsmod | grep -E "(overlay|br_netfilter)"
```

**éªŒè¯ç»“æœ**:
```
br_netfilter           36864  0
bridge                417792  1 br_netfilter
overlay               245760  0
```

### é…ç½®å†…æ ¸å‚æ•°
```bash
# é…ç½®ç½‘ç»œå‚æ•°
sudo bash -c 'cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF'

# åº”ç”¨å†…æ ¸å‚æ•°é…ç½®
sudo sysctl --system
```

**éªŒè¯ç»“æœ**:
- âœ… ç½‘ç»œè½¬å‘å‚æ•°å·²å¯ç”¨
- âœ… iptablesæ¡¥æ¥é…ç½®æ­£ç¡®

## ğŸ³ æ­¥éª¤3ï¼šå®‰è£…containerd v1.7.27

### æ·»åŠ Dockerä»“åº“
```bash
# æ·»åŠ Dockerä»“åº“ç”¨äºå®‰è£…containerd
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

# æ›´æ–°ä»“åº“ç¼“å­˜
sudo dnf makecache
```

### å®‰è£…containerd
```bash
# å®‰è£…containerdå®¹å™¨è¿è¡Œæ—¶
sudo dnf install -y containerd.io
```

**å®‰è£…ç»“æœ**:
```
Installed:
  containerd.io-1.7.27-3.1.el10.x86_64
```

### é…ç½®containerd
```bash
# ç”Ÿæˆé»˜è®¤é…ç½®æ–‡ä»¶
sudo mkdir -p /etc/containerd && sudo containerd config default | sudo tee /etc/containerd/config.toml

# å¯ç”¨systemd cgroupé©±åŠ¨
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# å¯åŠ¨å¹¶å¯ç”¨containerdæœåŠ¡
sudo systemctl enable --now containerd
```

**éªŒè¯ç»“æœ**:
- âœ… containerd v1.7.27 å®‰è£…æˆåŠŸ
- âœ… systemd cgroupé©±åŠ¨å·²é…ç½®
- âœ… æœåŠ¡è¿è¡Œæ­£å¸¸

## âš™ï¸ æ­¥éª¤4ï¼šå®‰è£…Kubernetes v1.30.14å·¥å…·

### æ·»åŠ Kubernetes v1.30ä»“åº“
```bash
# æ·»åŠ Kubernetes v1.30ä»“åº“
sudo bash -c 'cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.30/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF'

# æ›´æ–°ä»“åº“ç¼“å­˜
sudo dnf makecache
```

### å®‰è£…Kubernetesç»„ä»¶
```bash
# å®‰è£…Kubernetes v1.30ç»„ä»¶
sudo dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

# å¯ç”¨kubeletæœåŠ¡
sudo systemctl enable kubelet
```

**å®‰è£…ç»“æœ**:
```
Installed:
  kubeadm-1.30.14-150500.1.1.x86_64            
  kubectl-1.30.14-150500.1.1.x86_64       
  kubelet-1.30.14-150500.1.1.x86_64      
  kubernetes-cni-1.4.0-150500.1.1.x86_64
```

### éªŒè¯ç‰ˆæœ¬
```bash
# éªŒè¯K8så·¥å…·ç‰ˆæœ¬
kubeadm version && kubectl version --client
```

**ç‰ˆæœ¬ä¿¡æ¯**:
```
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.14"}
Client Version: v1.30.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

**éªŒè¯ç»“æœ**:
- âœ… kubeadm v1.30.14
- âœ… kubelet v1.30.14  
- âœ… kubectl v1.30.14

## ğŸ¯ æ­¥éª¤5ï¼šåˆå§‹åŒ–Kubernetes v1.30é›†ç¾¤

### é›†ç¾¤åˆå§‹åŒ–
```bash
# åˆå§‹åŒ–Kubernetes v1.30é›†ç¾¤
sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --cri-socket=unix:///var/run/containerd/containerd.sock --kubernetes-version=v1.30.14
```

**åˆå§‹åŒ–æˆåŠŸè¾“å‡º**:
```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### é…ç½®kubectlå®¢æˆ·ç«¯
```bash
# é…ç½®kubectlå®¢æˆ·ç«¯
mkdir -p $HOME/.kube && sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config && sudo chown $(id -u):$(id -g) $HOME/.kube/config

# ç§»é™¤masterèŠ‚ç‚¹æ±¡ç‚¹ä»¥æ”¯æŒå•èŠ‚ç‚¹é›†ç¾¤
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
```

**é…ç½®ç»“æœ**:
```
node/4u4g-gen-0071.ipp4a1.colossus.nvidia.com untainted
```

### éªŒè¯é›†ç¾¤çŠ¶æ€
```bash
# éªŒè¯èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes
```

**èŠ‚ç‚¹çŠ¶æ€**:
```
NAME                                       STATUS     ROLES           AGE   VERSION
4u4g-gen-0071.ipp4a1.colossus.nvidia.com   NotReady   control-plane   82s   v1.30.14
```

**éªŒè¯ç»“æœ**:
- âœ… é›†ç¾¤åˆå§‹åŒ–æˆåŠŸ
- âœ… èŠ‚ç‚¹çŠ¶æ€ä¸º NotReadyï¼ˆæ­£å¸¸ï¼Œéœ€è¦ç½‘ç»œæ’ä»¶ï¼‰
- âœ… ç‰ˆæœ¬æ˜¾ç¤ºä¸º v1.30.14

## ğŸŒ æ­¥éª¤6ï¼šå®‰è£…Calico v3.29.2ç½‘ç»œæ’ä»¶

### å®‰è£…Tigera Operator
```bash
# å®‰è£…Calicoç½‘ç»œæ’ä»¶çš„Tigera Operator
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/tigera-operator.yaml
```

**å®‰è£…ç»“æœ**:
```
namespace/tigera-operator created
customresourcedefinition.apiextensions.k8s.io/installations.operator.tigera.io created
deployment.apps/tigera-operator created
```

### é…ç½®Calicoå®‰è£…
```bash
# é…ç½®Calicoç½‘ç»œå®‰è£…å‚æ•°
cat <<EOF | kubectl create -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  calicoNetwork:
    ipPools:
    - blockSize: 26
      cidr: 192.168.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
EOF
```

**é…ç½®ç»“æœ**:
```
installation.operator.tigera.io/default created
```

### ç­‰å¾…å®‰è£…å®Œæˆ
```bash
# ç­‰å¾…Calicoå®‰è£…å®Œæˆ
kubectl wait --for=condition=Ready --timeout=300s installation/default

# æ£€æŸ¥Calicoç»„ä»¶çŠ¶æ€
kubectl get pods -n calico-system

# éªŒè¯èŠ‚ç‚¹æ˜¯å¦å˜ä¸ºReadyçŠ¶æ€
kubectl get nodes
```

**éªŒè¯ç»“æœ**:
```
# Calicoç»„ä»¶çŠ¶æ€
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-6cd8c77467-b99tn   1/1     Running   0          82s
calico-node-ndglc                          1/1     Running   0          82s
calico-typha-7c8758d78d-xrnsl              1/1     Running   0          83s
csi-node-driver-p4f2m                      2/2     Running   0          82s

# èŠ‚ç‚¹çŠ¶æ€
NAME                                       STATUS   ROLES           AGE     VERSION
4u4g-gen-0071.ipp4a1.colossus.nvidia.com   Ready    control-plane   3m11s   v1.30.14
```

**ç½‘ç»œéªŒè¯ç»“æœ**:
- âœ… Calicoç½‘ç»œæ’ä»¶å®‰è£…æˆåŠŸ
- âœ… èŠ‚ç‚¹çŠ¶æ€å˜ä¸º Ready
- âœ… ç½‘ç»œè¿é€šæ€§æ­£å¸¸
- âœ… Pod CIDR: 192.168.0.0/16

## ğŸ’¾ æ­¥éª¤7ï¼šå®‰è£…Local Path Provisionerå­˜å‚¨

### å®‰è£…å­˜å‚¨provisioner
```bash
# å®‰è£…Local Path Provisionerå­˜å‚¨
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml

# è®¾ç½®local-pathä¸ºé»˜è®¤å­˜å‚¨ç±»
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# éªŒè¯å­˜å‚¨ç±»
kubectl get storageclass
kubectl get pods -n local-path-storage
```

**å®‰è£…ç»“æœ**:
```
namespace/local-path-storage created
serviceaccount/local-path-provisioner-service-account created
deployment.apps/local-path-provisioner created
storageclass.storage.k8s.io/local-path created
storageclass.storage.k8s.io/local-path patched
```

**éªŒè¯ç»“æœ**:
- âœ… local-path è®¾ä¸ºé»˜è®¤å­˜å‚¨ç±»
- âœ… provisioner è¿è¡Œæ­£å¸¸

## ğŸ“Š æ­¥éª¤8ï¼šå®‰è£…Metrics Serverç›‘æ§

### å®‰è£…Metrics Server
```bash
# å®‰è£…Metrics Server
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

**å®‰è£…ç»“æœ**:
```
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
```

### ä¿®å¤TLSé…ç½®
```bash
# ä¿®å¤Metrics Serverçš„TLSé…ç½®
kubectl patch deployment metrics-server -n kube-system --patch='
spec:
  template:
    spec:
      containers:
      - name: metrics-server
        args:
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
'

# éªŒè¯åŠŸèƒ½
kubectl get pods -n kube-system -l k8s-app=metrics-server
kubectl top nodes
```

**éªŒè¯ç»“æœ**:
```
# Metrics ServerçŠ¶æ€
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-7fbfbcc44c-z42tg   1/1     Running   0          22s

# èŠ‚ç‚¹èµ„æºä½¿ç”¨æƒ…å†µ
NAME                                       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
4u4g-gen-0071.ipp4a1.colossus.nvidia.com   403m         1%     2531Mi          0%
```

**ç›‘æ§éªŒè¯ç»“æœ**:
- âœ… Metrics Serverè¿è¡Œæ­£å¸¸
- âœ… èƒ½å¤Ÿè·å–èŠ‚ç‚¹èµ„æºæŒ‡æ ‡
- âœ… TLSé…ç½®å·²ä¿®å¤

## ğŸ“¦ æ­¥éª¤9ï¼šå®‰è£…HelmåŒ…ç®¡ç†å™¨

### å®‰è£…Helm
```bash
# å®‰è£…HelmåŒ…ç®¡ç†å™¨
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# éªŒè¯ç‰ˆæœ¬
helm version
```

**éªŒè¯ç»“æœ**:
```
version.BuildInfo{Version:"v3.18.5", GitCommit:"b78692c18f0fb38fe5ba4571a674de067a4c53a5", GitTreeState:"clean", GoVersion:"go1.24.5"}
```

**HelméªŒè¯ç»“æœ**:
- âœ… Helm v3.18.5 å®‰è£…æˆåŠŸ

## âœ… æ­¥éª¤10ï¼šå®Œæ•´ç³»ç»ŸéªŒè¯

### é›†ç¾¤çŠ¶æ€éªŒè¯
```bash
echo "ğŸ” å®Œæ•´ç³»ç»ŸéªŒè¯ - Kubernetes v1.30.14"
echo "=================================="

# 1. èŠ‚ç‚¹çŠ¶æ€
echo "1. èŠ‚ç‚¹çŠ¶æ€ï¼š"
kubectl get nodes

# 2. æ‰€æœ‰PodçŠ¶æ€
echo "2. å…³é”®PodçŠ¶æ€ï¼š"
kubectl get pods -A --field-selector=status.phase!=Succeeded | grep -v Completed

# 3. å­˜å‚¨ç±»
echo "3. å­˜å‚¨ç±»ï¼š"
kubectl get storageclass

# 4. ç½‘ç»œæ’ä»¶
echo "4. ç½‘ç»œæ’ä»¶ï¼š"
kubectl get pods -n calico-system

echo "=================================="
echo "ğŸ‰ Kubernetes v1.30.14 ç³»ç»ŸéªŒè¯å®Œæˆï¼"
```

**éªŒè¯ç»“æœæ±‡æ€»**:

#### èŠ‚ç‚¹çŠ¶æ€
```
NAME                                       STATUS   ROLES           AGE     VERSION
4u4g-gen-0071.ipp4a1.colossus.nvidia.com   Ready    control-plane   4m32s   v1.30.14
```

#### å…³é”®PodçŠ¶æ€
```
NAMESPACE            NAME                                                               READY   STATUS    RESTARTS   AGE
calico-system        calico-kube-controllers-6cd8c77467-b99tn                           1/1     Running   0          3m9s
calico-system        calico-node-ndglc                                                  1/1     Running   0          3m9s
calico-system        calico-typha-7c8758d78d-xrnsl                                      1/1     Running   0          3m10s
calico-system        csi-node-driver-p4f2m                                              2/2     Running   0          3m9s
kube-system          coredns-55cb58b774-fnv2n                                           1/1     Running   0          4m32s
kube-system          coredns-55cb58b774-ssf97                                           1/1     Running   0          4m32s
kube-system          etcd-4u4g-gen-0071.ipp4a1.colossus.nvidia.com                      1/1     Running   0          4m46s
kube-system          kube-apiserver-4u4g-gen-0071.ipp4a1.colossus.nvidia.com            1/1     Running   0          4m48s
kube-system          kube-controller-manager-4u4g-gen-0071.ipp4a1.colossus.nvidia.com   1/1     Running   0          4m46s
kube-system          kube-proxy-vcq87                                                   1/1     Running   0          4m33s
kube-system          kube-scheduler-4u4g-gen-0071.ipp4a1.colossus.nvidia.com            1/1     Running   0          4m46s
kube-system          metrics-server-7fbfbcc44c-z42tg                                    1/1     Running   0          36s
local-path-storage   local-path-provisioner-79874bcbd9-268sm                            1/1     Running   0          81s
tigera-operator      tigera-operator-6479d6dc54-dfdbl                                   1/1     Running   0          3m20s
```

#### å­˜å‚¨ç±»çŠ¶æ€
```
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  61s
```

## ğŸ“‹ å®‰è£…ç»“æœæ‘˜è¦

### âœ… æˆåŠŸå®‰è£…çš„ç»„ä»¶
1. **Kubernetesé›†ç¾¤**: v1.30.14 å•èŠ‚ç‚¹é›†ç¾¤
2. **å®¹å™¨è¿è¡Œæ—¶**: containerd v1.7.27
3. **ç½‘ç»œæ’ä»¶**: Calico v3.29.2
4. **å­˜å‚¨**: Local Path Provisionerï¼ˆé»˜è®¤å­˜å‚¨ç±»ï¼‰
5. **ç›‘æ§**: Metrics Server
6. **åŒ…ç®¡ç†**: Helm v3.18.5

### ğŸ“Š é›†ç¾¤è§„æ ¼
- **èŠ‚ç‚¹æ•°é‡**: 1ä¸ªï¼ˆcontrol-plane + workerï¼‰
- **Kubernetesç‰ˆæœ¬**: v1.30.14
- **ç½‘ç»œ**: Calico VXLANï¼ŒPod CIDR 192.168.0.0/16
- **å­˜å‚¨**: æœ¬åœ°è·¯å¾„å­˜å‚¨ï¼Œæ”¯æŒåŠ¨æ€åˆ†é…
- **ç›‘æ§**: æ”¯æŒèŠ‚ç‚¹å’ŒPodèµ„æºç›‘æ§

### ğŸ¯ ä¸ºTekton v0.61.xåšå¥½å‡†å¤‡
è¯¥K8s v1.30.14é›†ç¾¤å·²ä¸ºä»¥ä¸‹å·¥ä½œè´Ÿè½½åšå¥½å‡†å¤‡ï¼š
- **Tekton Pipeline v0.61.x**: æ»¡è¶³æœ€ä½K8sç‰ˆæœ¬è¦æ±‚(1.28+)
- **CI/CDå·¥ä½œæµ**: å®Œæ•´çš„æŒç»­é›†æˆå’Œéƒ¨ç½²å¹³å°
- **å®¹å™¨åŒ–åº”ç”¨**: æ”¯æŒç°ä»£å®¹å™¨åº”ç”¨éƒ¨ç½²
- **GPUå·¥ä½œè´Ÿè½½**: å‡†å¤‡æ”¯æŒNVIDIA GPU Operator

### ğŸ”„ ç‰ˆæœ¬å…¼å®¹æ€§
- âœ… K8s v1.30.14 â† â†’ Tekton v0.61.x (å…¼å®¹)
- âœ… containerd v1.7.27 â† â†’ K8s v1.30.14 (å…¼å®¹)
- âœ… Calico v3.29.2 â† â†’ K8s v1.30.14 (å…¼å®¹)

## ğŸš€ ä¸‹ä¸€æ­¥

é›†ç¾¤å®‰è£…å®Œæˆåï¼Œæ‚¨å¯ä»¥ç»§ç»­ï¼š
1. [Tekton v0.61.x æ ¸å¿ƒç»„ä»¶å®‰è£…](04-tekton-installation.md)
2. [GPU Pipeline éƒ¨ç½²](07-gpu-pipeline-deployment.md)
3. [ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®](troubleshooting.md)

## ğŸ‰ æ€»ç»“

æˆåŠŸå®Œæˆäº†Kubernetes v1.30.14çš„å®‰è£…ï¼è¿™ä¸ªç‰ˆæœ¬ä¸ºä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒæä¾›äº†ï¼š
- **ç¨³å®šæ€§**: ç»è¿‡å……åˆ†æµ‹è¯•çš„ç¨³å®šç‰ˆæœ¬
- **å…¼å®¹æ€§**: ä¸Tekton v0.61.xå®Œç¾å…¼å®¹
- **åŠŸèƒ½æ€§**: æ”¯æŒæœ€æ–°çš„K8sç‰¹æ€§å’ŒAPI
- **å®‰å…¨æ€§**: åŒ…å«æœ€æ–°çš„å®‰å…¨è¡¥ä¸

ç°åœ¨å¯ä»¥å¼€å§‹å®‰è£…Tekton v0.61.xäº†ï¼
