apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: gpu-papermill-execution-fixed
  namespace: tekton-pipelines
  labels:
    app.kubernetes.io/name: gpu-papermill-execution-fixed
    app.kubernetes.io/component: tekton-task
    app.kubernetes.io/version: "1.0.0"
spec:
  description: |
    GPU-accelerated Papermill execution with Docker Compose compatible memory management.
    Fixed version with proper permissions and security context for conda/pip access.
  params:
  - name: notebook-path
    description: Path to the notebook to execute
    type: string
    default: "notebooks/01_scRNA_analysis_preprocessing.ipynb"
  - name: output-notebook-name
    description: Name for the output notebook
    type: string
    default: "01_scRNA_analysis_preprocessing_output.ipynb"
  - name: container-image
    description: Container image to use for execution
    type: string
    default: "nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12"
  - name: gpu-count
    description: Number of GPUs to allocate
    type: string
    default: "1"
  - name: memory-limit
    description: Memory limit for the container
    type: string
    default: "16Gi"
  - name: cpu-limit
    description: CPU limit for the container
    type: string
    default: "4"
  workspaces:
  - name: shared-storage
    description: Shared workspace for input/output files
    mountPath: /workspace/shared
  steps:
  - name: gpu-papermill-execute
    image: $(params.container-image)
    computeResources:
      requests:
        nvidia.com/gpu: 1
        memory: 16Gi
        cpu: 4
      limits:
        nvidia.com/gpu: 1
        memory: 32Gi
        cpu: 8
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
        add: ["IPC_LOCK", "SYS_RESOURCE"]
      runAsNonRoot: false
      runAsUser: 0
      runAsGroup: 0
      seccompProfile:
        type: RuntimeDefault
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility"
    - name: WORKSPACE_SHARED_PATH
      value: $(workspaces.shared-storage.path)
    - name: GPU_CACHE_PATH
      value: $(workspaces.shared-storage.path)/gpu-cache
    - name: DOCKER_WRITEABLE_DIR
      value: "/workspace/shared/artifacts"
    - name: OUTPUT_NOTEBOOK
      value: $(params.output-notebook-name)
    - name: EXTRA_PIP_PACKAGES
      value: "anndata==0.11.4 array-api-compat==1.12.0 contourpy==1.3.2 cycler==0.12.1 fonttools==4.58.0 h5py==3.13.0 imageio==2.37.0 joblib==1.5.1 kiwisolver==1.4.8 lazy-loader==0.4 legacy-api-wrap==1.4.1 llvmlite==0.44.0 matplotlib==3.10.3 natsort==8.4.0 networkx==3.4.2 numba==0.61.2 numpy==2.2.6 pandas==2.2.3 patsy==1.0.1 pillow==11.2.1 pynndescent==0.5.13 rapids-singlecell==0.12.6 scanpy==1.11.2 scikit-image==0.25.2 scikit-misc==0.5.1 scipy==1.15.3 seaborn==0.13.2 session-info2==0.1.2 statsmodels==0.14.4 threadpoolctl==3.6.0 tifffile==2025.5.10 tqdm==4.67.1 tzdata==2025.2 umap-learn==0.5.7 wget==3.2 deprecated==1.2.18 numcodecs==0.15.1 wrapt==1.17.2 zarr==2.18.7"
    - name: HOME
      value: "/root"
    - name: PATH
      value: "/opt/conda/envs/rapids/bin:/opt/conda/bin:/usr/local/bin:/usr/bin:/bin"
    - name: CONDA_DEFAULT_ENV
      value: "rapids"
    volumeMounts:
    - name: dshm
      mountPath: /dev/shm
    script: |
      #!/bin/bash
      set -eu
      
      echo "Starting GPU Papermill execution (Docker Compose Compatible Mode)..."
      echo "Target notebook: $(params.notebook-path)"
      echo "Shared workspace: ${WORKSPACE_SHARED_PATH}"
      echo "GPU cache: ${GPU_CACHE_PATH}"
      
      # Apply Docker Compose compatible memory settings
      echo "Applying Docker Compose compatible memory settings..."
      
      # Set memory lock limits (simulate Docker Compose ulimits)
      ulimit -l unlimited 2>/dev/null || echo "WARNING: Cannot set unlimited memlock"
      ulimit -s 67108864 2>/dev/null || echo "WARNING: Cannot set stack size"
      
      # Ensure shared memory is available
      if [ -d "/dev/shm" ]; then
        echo "Shared memory available: $(df -h /dev/shm | tail -1)"
      fi
      
      # Comprehensive permission fixes (critical)
      echo "Fixing all permission issues..."
      
      # Fix conda directories - full access
      chown -R root:root /opt/conda 2>/dev/null || echo "WARNING: Cannot change conda ownership"
      chmod -R 777 /opt/conda 2>/dev/null || echo "WARNING: Cannot change conda permissions"
      
      # Fix entire workspace permissions
      chown -R root:root "${WORKSPACE_SHARED_PATH}" 2>/dev/null || echo "WARNING: Cannot change workspace ownership"
      chmod -R 777 "${WORKSPACE_SHARED_PATH}" 2>/dev/null || echo "WARNING: Cannot change workspace permissions"
      
      # Setup environment as root user
      export HOME="/root"
      export USER="root"
      export PATH="/opt/conda/envs/rapids/bin:/opt/conda/bin:$PATH"
      
      cd "${WORKSPACE_SHARED_PATH}"
      
      # Verify GPU availability
      echo "Checking GPU availability..."
      nvidia-smi --query-gpu=name,memory.total --format=csv,noheader || echo "WARNING: nvidia-smi failed"
      
      # Verify notebook exists
      if [ ! -f "$(params.notebook-path)" ]; then
        echo "ERROR: Notebook not found: $(params.notebook-path)"
        find . -name "*.ipynb" -type f | head -5
        exit 1
      fi
      
      # Create output directories and fix permissions
      mkdir -p "${DOCKER_WRITEABLE_DIR}"
      mkdir -p "${GPU_CACHE_PATH}"
      
      # Fix output directory permissions with full access
      echo "Fixing output directory permissions..."
      chown -R root:root "${DOCKER_WRITEABLE_DIR}" 2>/dev/null || echo "WARNING: Cannot change output dir ownership"
      chmod -R 777 "${DOCKER_WRITEABLE_DIR}" 2>/dev/null || echo "WARNING: Cannot change output dir permissions"
      chown -R root:root "${GPU_CACHE_PATH}" 2>/dev/null || echo "WARNING: Cannot change cache dir ownership"
      chmod -R 777 "${GPU_CACHE_PATH}" 2>/dev/null || echo "WARNING: Cannot change cache dir permissions"
      
      echo "Notebook found: $(params.notebook-path)"
      
      # Setup Python environment with proper paths
      echo "Setting up Python environment..."
      
      # Try rapids environment first, fallback to base
      PYTHON_BIN=""
      PIP_BIN=""
      CONDA_BIN="/opt/conda/bin/conda"
      
      if [ -x "/opt/conda/envs/rapids/bin/python" ]; then
        PYTHON_BIN="/opt/conda/envs/rapids/bin/python"
        PIP_BIN="/opt/conda/envs/rapids/bin/pip"
        echo "Using rapids environment"
      elif [ -x "/opt/conda/bin/python" ]; then
        PYTHON_BIN="/opt/conda/bin/python"
        PIP_BIN="/opt/conda/bin/pip"
        echo "Using base conda environment"
      else
        echo "ERROR: No Python found in expected locations"
        exit 1
      fi
      
      # Verify Python environment
      echo "Python environment verification:"
      $PYTHON_BIN --version && echo "Python OK" || (echo "ERROR: Python failed" && exit 1)
      $PIP_BIN --version && echo "pip OK" || (echo "ERROR: pip failed" && exit 1)
      $CONDA_BIN --version && echo "conda OK" || (echo "ERROR: conda failed" && exit 1)
      
      # Install dependencies
      echo "Installing dependencies..."
      $PIP_BIN install --quiet --no-cache-dir papermill ipykernel jupyter || {
        echo "ERROR: Failed to install papermill"
        exit 1
      }
      
      # Install additional packages
      if [ -n "${EXTRA_PIP_PACKAGES:-}" ]; then
        echo "Installing scanpy with conda..."
        $CONDA_BIN install -y -c conda-forge -c bioconda scanpy || echo "WARNING: Conda scanpy failed"
        
                 echo "Installing other packages with pip..."
         $PIP_BIN install --quiet --no-cache-dir --force-reinstall ${EXTRA_PIP_PACKAGES} || echo "WARNING: Some pip packages failed"
        
        # Verify key packages
        $PYTHON_BIN -c "import scanpy as sc; print('scanpy version:', sc.__version__)" || echo "WARNING: scanpy verification failed"
        $PYTHON_BIN -c "import cupy as cp; print('cupy GPU count:', cp.cuda.runtime.getDeviceCount())" || echo "WARNING: cupy verification failed"
      fi
      
      # Setup GPU environment
      export CUDA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-"all"}
      export CUPY_CACHE_DIR="${GPU_CACHE_PATH}/cupy"
      export NUMBA_CACHE_DIR="${GPU_CACHE_PATH}/numba"
      mkdir -p "${CUPY_CACHE_DIR}" "${NUMBA_CACHE_DIR}"
      
      # Execute notebook
      echo "Executing notebook with Papermill..."
      PAPERMILL_OUTPUT_PATH="${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}"
      
      # Find papermill executable
      PAPERMILL_BIN=""
      if [ -x "/opt/conda/envs/rapids/bin/papermill" ]; then
        PAPERMILL_BIN="/opt/conda/envs/rapids/bin/papermill"
      elif [ -x "/opt/conda/bin/papermill" ]; then
        PAPERMILL_BIN="/opt/conda/bin/papermill"
      else
        echo "ERROR: papermill not found after installation"
        exit 1
      fi
      
      if $PAPERMILL_BIN "$(params.notebook-path)" "${PAPERMILL_OUTPUT_PATH}" --log-output --progress-bar; then
        echo "SUCCESS: Papermill execution completed!"
      else
        echo "ERROR: Papermill execution failed"
        ls -la "${DOCKER_WRITEABLE_DIR}/" || true
        exit 1
      fi
      
      # Verify output
      if [ -f "${PAPERMILL_OUTPUT_PATH}" ]; then
        OUTPUT_SIZE=$(du -h "${PAPERMILL_OUTPUT_PATH}" | cut -f1)
        echo "SUCCESS: Output notebook created: ${OUTPUT_SIZE}"
      else
        echo "ERROR: Output notebook not found"
        exit 1
      fi
      
      echo "Docker Compose compatible execution completed successfully!"
  volumes:
  - name: dshm
    emptyDir:
      medium: Memory
      sizeLimit: 1Gi 