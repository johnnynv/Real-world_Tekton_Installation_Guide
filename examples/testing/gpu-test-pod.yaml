apiVersion: v1
kind: Pod
metadata:
  name: gpu-test-pod
  namespace: tekton-pipelines
spec:
  restartPolicy: Never
  nodeSelector:
    accelerator: nvidia-tesla-gpu
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule
  containers:
  - name: gpu-test
    image: nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12
    command: ["/bin/bash"]
    args:
    - -c
    - |
      echo "ğŸ” Checking GPU access in container..."
      echo "ğŸ“ Checking /dev/nvidia* devices:"
      ls -la /dev/nvidia* || echo "âŒ No nvidia devices found"
      echo ""
      echo "ğŸ”§ Testing nvidia-smi:"
      nvidia-smi || echo "âŒ nvidia-smi failed"
      echo ""
      echo "ğŸ Testing Python CUDA access:"
      python3 -c "import cupy as cp; print('âœ… CuPy version:', cp.__version__); print('âœ… CUDA devices:', cp.cuda.runtime.getDeviceCount())" || echo "âŒ Python CUDA test failed"
      echo ""
      echo "ğŸ’¤ Sleeping for 300 seconds for debugging..."
      sleep 300
    resources:
      limits:
        nvidia.com/gpu: 1
      requests:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: "all"
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: "compute,utility" 